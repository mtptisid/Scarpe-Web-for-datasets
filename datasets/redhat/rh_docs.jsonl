{"title": "Building your RHEL AI environment", "content": "Building your RHEL AI environment Red Hat Enterprise Linux AI 1.1 Creating accounts, initalizing RHEL AI, downloading models, and serving/chat customizations Red Hat RHEL AI Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/building_your_rhel_ai_environment/index"}
{"title": "Chapter 1. About", "content": "Chapter 1. About 1.1. About OpenShift Virtualization Learn about OpenShift Virtualization’s capabilities and support scope. 1.1.1. What you can do with OpenShift Virtualization OpenShift Virtualization provides the scalable, enterprise-grade virtualization functionality in Red Hat OpenShift. You can use it to manage virtual machines (VMs) exclusively or alongside container workloads. Note If you have a Red Hat OpenShift Virtualization Engine subscription, you can run unlimited VMs on subscribed hosts, but you cannot run application instances in containers. For more information, see the subscription guide section about Red Hat OpenShift Virtualization Engine and related products . OpenShift Virtualization adds new objects into your OpenShift Container Platform cluster by using Kubernetes custom resources to enable virtualization tasks. These tasks include: Creating and managing Linux and Windows VMs Running pod and VM workloads alongside each other in a cluster Connecting to VMs through a variety of consoles and CLI tools Importing and cloning existing VMs Managing network interface controllers and storage disks attached to VMs Live migrating VMs between nodes You can manage your cluster and virtualization resources by using the Virtualization perspective of the OpenShift Container Platform web console, and by using the OpenShift CLI ( oc ). OpenShift Virtualization is designed and tested to work well with Red Hat OpenShift Data Foundation features. Important When you deploy OpenShift Virtualization with OpenShift Data Foundation, you must create a dedicated storage class for Windows virtual machine disks. See Optimizing ODF PersistentVolumes for Windows VMs for details. You can use OpenShift Virtualization with OVN-Kubernetes or one of the other certified network plugins listed in Certified OpenShift CNI Plug-ins . You can check your OpenShift Virtualization cluster for compliance issues by installing the Compliance Operator and running a scan with the ocp4-moderate and ocp4-moderate-node profiles . The Compliance Operator uses OpenSCAP, a NIST-certified tool , to scan and enforce security policies. 1.1.2. Comparing OpenShift Virtualization to VMware vSphere If you are familiar with VMware vSphere, the following table lists OpenShift Virtualization components that you can use to accomplish similar tasks. However, because OpenShift Virtualization is conceptually different from vSphere, and much of its functionality comes from the underlying OpenShift Container Platform, OpenShift Virtualization does not have direct alternatives for all vSphere concepts or components. Table 1.1. Mapping of vSphere concepts to their closest OpenShift Virtualization counterparts vSphere concept OpenShift Virtualization Explanation Datastore Persistent volume (PV) + Persistent volume claim (PVC) Stores VM disks. A PV represents existing storage and is attached to a VM through a PVC. When created with the ReadWriteMany (RWX) access mode, PVCs can be mounted by multiple VMs simultaneously. Dynamic Resource Scheduling (DRS) Pod eviction policy + Descheduler Provides active resource balancing. A combination of pod eviction policies and a descheduler allows VMs to be live migrated to more appropriate nodes to keep node resource utilization manageable. NSX Multus + OVN-Kubernetes + Third-party container network interface (CNI) plug-ins Provides an overlay network configuration. There is no direct equivalent for NSX in OpenShift Virtualization, but you can use the OVN-Kubernetes network provider or install certified third-party CNI plug-ins. Storage Policy Based Management (SPBM) Storage class Provides policy-based storage selection. Storage classes represent various storage types and describe storage capabilities, such as quality of service, backup policy, reclaim policy, and whether volume expansion is allowed. A PVC can request a specific storage class to satisfy application requirements. vCenter vRealize Operations OpenShift Metrics and Monitoring Provides host and VM metrics. You can view metrics and monitor the overall health of the cluster and VMs by using the OpenShift Container Platform web console. vMotion Live migration Moves a running VM to another node without interruption. For live migration to be available, the PVC attached to the VM must have the ReadWriteMany (RWX) access mode. vSwitch DvSwitch NMState Operator + Multus Provides a physical network configuration. You can use the NMState Operator to apply state-driven network configuration and manage various network interface types, including Linux bridges and network bonds. With Multus, you can attach multiple network interfaces and connect VMs to external networks. 1.1.3. Supported cluster versions for OpenShift Virtualization OpenShift Virtualization 4.17 is supported for use on  clusters. To use the latest z-stream release of OpenShift Virtualization, you must first upgrade to the latest version of OpenShift Container Platform. 1.1.4. About volume and access modes for virtual machine disks If you use the storage API with known storage providers, the volume and access modes are selected automatically. However, if you use a storage class that does not have a storage profile, you must configure the volume and access mode. For best results, use the ReadWriteMany (RWX) access mode and the Block volume mode. This is important for the following reasons: ReadWriteMany (RWX) access mode is required for live migration. The Block volume mode performs significantly better than the Filesystem volume mode. This is because the Filesystem volume mode uses more storage layers, including a file system layer and a disk image file. These layers are not necessary for VM disk storage. For example, if you use Red Hat OpenShift Data Foundation, Ceph RBD volumes are preferable to CephFS volumes. Important You cannot live migrate virtual machines with the following configurations: Storage volume with ReadWriteOnce (RWO) access mode Passthrough features such as GPUs Set the evictionStrategy field to None for these virtual machines. The None strategy powers down VMs during node reboots. 1.1.5. Single-node OpenShift differences You can install OpenShift Virtualization on single-node OpenShift. However, you should be aware that Single-node OpenShift does not support the following features: High availability Pod disruption Live migration Virtual machines or templates that have an eviction strategy configured 1.1.6. Additional resources Glossary of common terms for OpenShift Container Platform storage About single-node OpenShift Assisted installer Pod disruption budgets About live migration Eviction strategies Tuning & Scaling Guide Supported limits for OpenShift Virtualization 4.x 1.2. Security policies Learn about OpenShift Virtualization security and authorization. Key points OpenShift Virtualization adheres to the restricted Kubernetes pod security standards profile, which aims to enforce the current best practices for pod security. Virtual machine (VM) workloads run as unprivileged pods. Security context constraints (SCCs) are defined for the kubevirt-controller service account. TLS certificates for OpenShift Virtualization components are renewed and rotated automatically. 1.2.1. About workload security By default, virtual machine (VM) workloads do not run with root privileges in OpenShift Virtualization, and there are no supported OpenShift Virtualization features that require root privileges. For each VM, a virt-launcher pod runs an instance of libvirt in session mode to manage the VM process. In session mode, the libvirt daemon runs as a non-root user account and only permits connections from clients that are running under the same user identifier (UID). Therefore, VMs run as unprivileged pods, adhering to the security principle of least privilege. 1.2.2. TLS certificates TLS certificates for OpenShift Virtualization components are renewed and rotated automatically. You are not required to refresh them manually. Automatic renewal schedules TLS certificates are automatically deleted and replaced according to the following schedule: KubeVirt certificates are renewed daily. Containerized Data Importer controller (CDI) certificates are renewed every 15 days. MAC pool certificates are renewed every year. Automatic TLS certificate rotation does not disrupt any operations. For example, the following operations continue to function without any disruption: Migrations Image uploads VNC and console connections 1.2.3. Authorization OpenShift Virtualization uses role-based access control (RBAC) to define permissions for human users and service accounts. The permissions defined for service accounts control the actions that OpenShift Virtualization components can perform. You can also use RBAC roles to manage user access to virtualization features. For example, an administrator can create an RBAC role that provides the permissions required to launch a virtual machine. The administrator can then restrict access by binding the role to specific users. 1.2.3.1. Default cluster roles for OpenShift Virtualization By using cluster role aggregation, OpenShift Virtualization extends the default OpenShift Container Platform cluster roles to include permissions for accessing virtualization objects. Table 1.2. OpenShift Virtualization cluster roles Default cluster role OpenShift Virtualization cluster role OpenShift Virtualization cluster role description view kubevirt.io:view A user that can view all OpenShift Virtualization resources in the cluster but cannot create, delete, modify, or access them. For example, the user can see that a virtual machine (VM) is running but cannot shut it down or gain access to its console. edit kubevirt.io:edit A user that can modify all OpenShift Virtualization resources in the cluster. For example, the user can create VMs, access VM consoles, and delete VMs. admin kubevirt.io:admin A user that has full permissions to all OpenShift Virtualization resources, including the ability to delete collections of resources. The user can also view and modify the OpenShift Virtualization runtime configuration, which is located in the HyperConverged custom resource in the openshift-cnv namespace. 1.2.3.2. RBAC roles for storage features in OpenShift Virtualization The following permissions are granted to the Containerized Data Importer (CDI), including the cdi-operator and cdi-controller service accounts. 1.2.3.2.1. Cluster-wide RBAC roles Table 1.3. Aggregated cluster roles for the cdi.kubevirt.io API group CDI cluster role Resources Verbs cdi.kubevirt.io:admin datavolumes , uploadtokenrequests * (all) datavolumes/source create cdi.kubevirt.io:edit datavolumes , uploadtokenrequests * datavolumes/source create cdi.kubevirt.io:view cdiconfigs , dataimportcrons , datasources , datavolumes , objecttransfers , storageprofiles , volumeimportsources , volumeuploadsources , volumeclonesources get , list , watch datavolumes/source create cdi.kubevirt.io:config-reader cdiconfigs , storageprofiles get , list , watch Table 1.4. Cluster-wide roles for the cdi-operator service account API group Resources Verbs rbac.authorization.k8s.io clusterrolebindings , clusterroles get , list , watch , create , update , delete security.openshift.io securitycontextconstraints get , list , watch , update , create apiextensions.k8s.io customresourcedefinitions , customresourcedefinitions/status get , list , watch , create , update , delete cdi.kubevirt.io * * upload.cdi.kubevirt.io * * admissionregistration.k8s.io validatingwebhookconfigurations , mutatingwebhookconfigurations create , list , watch admissionregistration.k8s.io validatingwebhookconfigurations Allow list: cdi-api-dataimportcron-validate, cdi-api-populator-validate, cdi-api-datavolume-validate, cdi-api-validate, objecttransfer-api-validate get , update , delete admissionregistration.k8s.io mutatingwebhookconfigurations Allow list: cdi-api-datavolume-mutate get , update , delete apiregistration.k8s.io apiservices get , list , watch , create , update , delete Table 1.5. Cluster-wide roles for the cdi-controller service account API group Resources Verbs \"\" (core) events create , patch \"\" (core) persistentvolumeclaims get , list , watch , create , update , delete , deletecollection , patch \"\" (core) persistentvolumes get , list , watch , update \"\" (core) persistentvolumeclaims/finalizers , pods/finalizers update \"\" (core) pods , services get , list , watch , create , delete \"\" (core) configmaps get , create storage.k8s.io storageclasses , csidrivers get , list , watch config.openshift.io proxies get , list , watch cdi.kubevirt.io * * snapshot.storage.k8s.io volumesnapshots , volumesnapshotclasses , volumesnapshotcontents get , list , watch , create , delete snapshot.storage.k8s.io volumesnapshots update , deletecollection apiextensions.k8s.io customresourcedefinitions get , list , watch scheduling.k8s.io priorityclasses get , list , watch image.openshift.io imagestreams get , list , watch \"\" (core) secrets create kubevirt.io virtualmachines/finalizers update 1.2.3.2.2. Namespaced RBAC roles Table 1.6. Namespaced roles for the cdi-operator service account API group Resources Verbs rbac.authorization.k8s.io rolebindings , roles get , list , watch , create , update , delete \"\" (core) serviceaccounts , configmaps , events , secrets , services get , list , watch , create , update , patch , delete apps deployments , deployments/finalizers get , list , watch , create , update , delete route.openshift.io routes , routes/custom-host get , list , watch , create , update config.openshift.io proxies get , list , watch monitoring.coreos.com servicemonitors , prometheusrules get , list , watch , create , delete , update , patch coordination.k8s.io leases get , create , update Table 1.7. Namespaced roles for the cdi-controller service account API group Resources Verbs \"\" (core) configmaps get , list , watch , create , update , delete \"\" (core) secrets get , list , watch batch cronjobs get , list , watch , create , update , delete batch jobs create , delete , list , watch coordination.k8s.io leases get , create , update networking.k8s.io ingresses get , list , watch route.openshift.io routes get , list , watch 1.2.3.3. Additional SCCs and permissions for the kubevirt-controller service account Security context constraints (SCCs) control permissions for pods. These permissions include actions that a pod, a collection of containers, can perform and what resources it can access. You can use SCCs to define a set of conditions that a pod must run with to be accepted into the system. The virt-controller is a cluster controller that creates the virt-launcher pods for virtual machines in the cluster. These pods are granted permissions by the kubevirt-controller service account. The kubevirt-controller service account is granted additional SCCs and Linux capabilities so that it can create virt-launcher pods with the appropriate permissions. These extended permissions allow virtual machines to use OpenShift Virtualization features that are beyond the scope of typical pods. The kubevirt-controller service account is granted the following SCCs: scc.AllowHostDirVolumePlugin = true This allows virtual machines to use the hostpath volume plugin. scc.AllowPrivilegedContainer = false This ensures the virt-launcher pod is not run as a privileged container. scc.AllowedCapabilities = []corev1.Capability{\"SYS_NICE\", \"NET_BIND_SERVICE\"} SYS_NICE allows setting the CPU affinity. NET_BIND_SERVICE allows DHCP and Slirp operations. Viewing the SCC and RBAC definitions for the kubevirt-controller You can view the SecurityContextConstraints definition for the kubevirt-controller by using the oc tool: $ oc get scc kubevirt-controller -o yaml You can view the RBAC definition for the kubevirt-controller clusterrole by using the oc tool: $ oc get clusterrole kubevirt-controller -o yaml 1.2.4. Additional resources Managing security context constraints Using RBAC to define and apply permissions Creating a cluster role Cluster role binding commands Enabling user permissions to clone data volumes across namespaces 1.3. OpenShift Virtualization Architecture The Operator Lifecycle Manager (OLM) deploys operator pods for each component of OpenShift Virtualization: Compute: virt-operator Storage: cdi-operator Network: cluster-network-addons-operator Scaling: ssp-operator OLM also deploys the hyperconverged-cluster-operator pod, which is responsible for the deployment, configuration, and life cycle of other components, and several helper pods: hco-webhook , and hyperconverged-cluster-cli-download . After all operator pods are successfully deployed, you should create the HyperConverged custom resource (CR). The configurations set in the HyperConverged CR serve as the single source of truth and the entrypoint for OpenShift Virtualization, and guide the behavior of the CRs. The HyperConverged CR creates corresponding CRs for the operators of all other components within its reconciliation loop. Each operator then creates resources such as daemon sets, config maps, and additional components for the OpenShift Virtualization control plane. For example, when the HyperConverged Operator (HCO) creates the KubeVirt CR, the OpenShift Virtualization Operator reconciles it and creates additional resources such as virt-controller , virt-handler , and virt-api . The OLM deploys the Hostpath Provisioner (HPP) Operator, but it is not functional until you create a hostpath-provisioner CR. Virtctl client commands 1.3.1. About the HyperConverged Operator (HCO) The HCO, hco-operator , provides a single entry point for deploying and managing OpenShift Virtualization and several helper operators with opinionated defaults. It also creates custom resources (CRs) for those operators. Table 1.8. HyperConverged Operator components Component Description deployment/hco-webhook Validates the HyperConverged custom resource contents. deployment/hyperconverged-cluster-cli-download Provides the virtctl tool binaries to the cluster so that you can download them directly from the cluster. KubeVirt/kubevirt-kubevirt-hyperconverged Contains all operators, CRs, and objects needed by OpenShift Virtualization. SSP/ssp-kubevirt-hyperconverged A Scheduling, Scale, and Performance (SSP) CR. This is automatically created by the HCO. CDI/cdi-kubevirt-hyperconverged A Containerized Data Importer (CDI) CR. This is automatically created by the HCO. NetworkAddonsConfig/cluster A CR that instructs and is managed by the cluster-network-addons-operator . 1.3.2. About the Containerized Data Importer (CDI) Operator The CDI Operator, cdi-operator , manages CDI and its related resources, which imports a virtual machine (VM) image into a persistent volume claim (PVC) by using a data volume. Table 1.9. CDI Operator components Component Description deployment/cdi-apiserver Manages the authorization to upload VM disks into PVCs by issuing secure upload tokens. deployment/cdi-uploadproxy Directs external disk upload traffic to the appropriate upload server pod so that it can be written to the correct PVC. Requires a valid upload token. pod/cdi-importer Helper pod that imports a virtual machine image into a PVC when creating a data volume. 1.3.3. About the Cluster Network Addons Operator The Cluster Network Addons Operator, cluster-network-addons-operator , deploys networking components on a cluster and manages the related resources for extended network functionality. Table 1.10. Cluster Network Addons Operator components Component Description deployment/kubemacpool-cert-manager Manages TLS certificates of Kubemacpool’s webhooks. deployment/kubemacpool-mac-controller-manager Provides a MAC address pooling service for virtual machine (VM) network interface cards (NICs). daemonset/bridge-marker Marks network bridges available on nodes as node resources. daemonset/kube-cni-linux-bridge-plugin Installs Container Network Interface (CNI) plugins on cluster nodes, enabling the attachment of VMs to Linux bridges through network attachment definitions. 1.3.4. About the Hostpath Provisioner (HPP) Operator The HPP Operator, hostpath-provisioner-operator , deploys and manages the multi-node HPP and related resources. Table 1.11. HPP Operator components Component Description deployment/hpp-pool-hpp-csi-pvc-block-<worker_node_name> Provides a worker for each node where the HPP is designated to run. The pods mount the specified backing storage on the node. daemonset/hostpath-provisioner-csi Implements the Container Storage Interface (CSI) driver interface of the HPP. daemonset/hostpath-provisioner Implements the legacy driver interface of the HPP. 1.3.5. About the Scheduling, Scale, and Performance (SSP) Operator The SSP Operator, ssp-operator , deploys the common templates, the related default boot sources, the pipeline tasks, and the template validator. 1.3.6. About the OpenShift Virtualization Operator The OpenShift Virtualization Operator, virt-operator , deploys, upgrades, and manages OpenShift Virtualization without disrupting current virtual machine (VM) workloads. In addition, the OpenShift Virtualization Operator deploys the common instance types and common preferences. Table 1.12. virt-operator components Component Description deployment/virt-api HTTP API server that serves as the entry point for all virtualization-related flows. deployment/virt-controller Observes the creation of a new VM instance object and creates a corresponding pod. When the pod is scheduled on a node, virt-controller updates the VM with the node name. daemonset/virt-handler Monitors any changes to a VM and instructs virt-launcher to perform the required operations. This component is node-specific. pod/virt-launcher Contains the VM that was created by the user as implemented by libvirt and qemu . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/virtualization/about"}
{"title": "Getting started with Ansible Automation Platform", "content": "Getting started with Ansible Automation Platform Red Hat Ansible Automation Platform 2.5 Get started with Ansible Automation Platform Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/getting_started_with_ansible_automation_platform/index"}
{"title": "Planning your deployment", "content": "Planning your deployment Red Hat OpenStack Services on  Planning a Red Hat OpenStack Services on OpenShift environment on a Red Hat OpenShift Container Platform cluster OpenStack Documentation Team rhos-docs@redhat.com", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/planning_your_deployment/index"}
{"title": "Install ROSA with HCP clusters", "content": "Install ROSA with HCP clusters Red Hat OpenShift Service on AWS 4 Installing, accessing, and deleting Red Hat OpenShift Service on AWS (ROSA) clusters. Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html-single/install_rosa_with_hcp_clusters/index"}
{"title": "Using image mode for RHEL to build, deploy, and manage operating systems", "content": "Using image mode for RHEL to build, deploy, and manage operating systems Red Hat Enterprise Linux 9 Using RHEL bootc images on Red Hat Enterprise Linux 9 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/index"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9"}
{"title": "Getting Started with Red Hat Insights", "content": "Getting Started with Red Hat Insights Red Hat Insights 1-latest How to start using Red Hat Insights Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_insights/1-latest/html/getting_started_with_red_hat_insights/index"}
{"title": "Chapter 1. Configuring accounts for RHEL AI", "content": "Chapter 1. Configuring accounts for RHEL AI There are a few accounts you need to set up before interacting with RHEL AI. Creating a Red Hat account You can create a Red Hat account by registering on the Red Hat website. You can follow the procedure in Register for a Red Hat account . Creating a Red Hat registry account Before you can download models from the Red Hat registry, you need to create a registry account and login using the CLI. You can view your account username and password by selecting the Regenerate Token button on the webpage. You can create a Red Hat registry account by selecting the New Service Account button on the Registry Service Accounts page. There are several ways you can log into your registry account via the CLI. Follow the procedure in Red Hat Container Registry authentication to login on your machine. Optional: Configuring Red Hat Insights for hybrid cloud deployments Red Hat Insights is an offering that gives you visibility to the environments you are deploying. This platform can also help identify operational and vulnerability risks in your system. For more information about Red Hat Insights, see Red Hat Insights data and application security . You can create a Red Hat Insights account using an activation key and organization parameters by following the procedure in Viewing an activation key . You can then configure your account on your machine by running the following command: $ rhc connect --organization <org id> --activation-key <created key> To run RHEL AI in a disconnected environment, or opt out of Red Hat Insights, run the following commands: $ sudo mkdir -p /etc/ilab $ sudo touch /etc/ilab/insights-opt-out Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/building_your_rhel_ai_environment/setting_up_accounts"}
{"title": "Chapter 4. Serving and chatting with the models", "content": "Chapter 4. Serving and chatting with the models To interact with various models on Red Hat Enterprise Linux AI you must serve the model, which hosts it on a server, then you can chat with the models. 4.1. Serving the model To interact with the models, you must first activate the model in a machine through serving. The ilab model serve commands starts a vLLM server that allows you to chat with the model. Prerequisites You installed RHEL AI with the bootable container image. You initialized InstructLab. You installed your preferred Granite LLMs. You have root user access on your machine. Procedure If you do not specify a model, you can serve the default model, granite-7b-redhat-lab , by running the following command: $ ilab model serve To serve a specific model, run the following command $ ilab model serve --model-path <model-path> Example command $ ilab model serve --model-path ~/.cache/instructlab/models/granite-7b-code-instruct Example output of when the model is served and ready INFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/granite-7b-code-instruct' with -1 gpu-layers and 4096 max context size. Starting server process After application startup complete see http://127.0.0.1:8000/docs for API. Press CTRL+C to shut down the server. 4.1.1. Optional: Running ilab model serve as a service You can set up a systemd service so that the ilab model serve command runs as a running service. The systemd service runs the ilab model serve command in the background and restarts if it crashes or fails. You can configure the service to start upon system boot. Prerequisites You installed the Red Hat Enterprise Linux AI image on bare metal. You initialized InstructLab You downloaded your preferred Granite LLMs. You have root user access on your machine. Procedure. Create a directory for your systemd user service by running the following command: $ mkdir -p $HOME/.config/systemd/user Create your systemd service file with the following example configurations: $ cat << EOF > $HOME/.config/systemd/user/ilab-serve.service [Unit] Description=ilab model serve service [Install] WantedBy=multi-user.target default.target 1 [Service] ExecStart=ilab model serve --model-family granite Restart=always EOF 1 Specifies to start by default on boot. Reload the systemd manager configuration by running the following command: $ systemctl --user daemon-reload Start the ilab model serve systemd service by running the following command: $ systemctl --user start ilab-serve.service You can check that the service is running with the following command: $ systemctl --user status ilab-serve.service You can check the service logs by running the following command: $ journalctl --user-unit ilab-serve.service To allow the service to start on boot, run the following command: $ sudo loginctl enable-linger Optional: There are a few optional commands you can run for maintaining your systemd service. You can stop the ilab-serve system service by running the following command: $ systemctl --user stop ilab-serve.service You can prevent the service from starting on boot by removing the \"WantedBy=multi-user.target default.target\" from the $HOME/.config/systemd/user/ilab-serve.service file. 4.2. Chatting with the model Once you serve your model, you can now chat with the model. Important The model you are chatting with must match the model you are serving. With the default config.yaml file, the granite-7b-redhat-lab model is the default for serving and chatting. Prerequisites You installed RHEL AI with the bootable container image. You initialized InstructLab. You downloaded your preferred Granite LLMs. You are serving a model. You have root user access on your machine. Procedure Since you are serving the model in one terminal window, you must open another terminal to chat with the model. To chat with the default model, run the following command: $ ilab model chat To chat with a specific model run the following command: $ ilab model chat --model <model-path> Example command $ ilab model chat --model ~/.cache/instructlab/models/granite-7b-code-instruct Example output of the chatbot $ ilab model chat ╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮ │ Welcome to InstructLab Chat w/ GRANITE-7B-CODE-INSTRUCT (type /h for help) │ ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ >>> [S][default] + Type exit to leave the chatbot. 4.2.1. Optional: Creating an API key for model chatting By default, the ilab CLI does not use authentication. If you want to expose your server to the internet, you can create a API key that connects to your server with the following procedures. Prerequisites You installed the Red Hat Enterprise Linux AI image on bare metal. You initialized InstructLab You downloaded your preferred Granite LLMs. You have root user access on your machine. Procedure Create a API key that is held in $VLLM_API_KEY parameter by running the following command: $ export VLLM_API_KEY=$(python -c 'import secrets; print(secrets.token_urlsafe())') You can view the API key by running the following command: $ echo $VLLM_API_KEY Update the config.yaml by running the following command: $ ilab config edit Add the following parameters to the vllm_args section of your config.yaml file. serve: vllm: vllm_args: - --api-key - <api-key-string> where <api-key-string> Specify your API key string. You can verify that the server is using API key authentication by running the following command: $ ilab model chat Then, seeing the following error that shows an unauthorized user. openai.AuthenticationError: Error code: 401 - {'error': 'Unauthorized'} Verify that your API key is working by running the following command: $ ilab chat -m granite-7b-redhat-lab --endpoint-url https://inference.rhelai.com/v1 --api-key $VLLM_API_KEY Example output $ ilab model chat ╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮ │ Welcome to InstructLab Chat w/ GRANITE-7B-LAB (type /h for help) │ ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ >>> [S][default] 4.2.2. Optional: Allowing chat access to a model from a secure endpoint You can serve an inference endpoint and allow others to interact with models provided with Red Hat Enterprise Linux AI on secure connections by creating a systemd service and setting up a nginx reverse proxy that exposes a secure endpoint. This allows you to share the secure endpoint with others so they can chat with the model over a network. The following procedure uses self-signed certifications, but it is recommended to use certificates issued by a trusted Certificate Authority (CA). Note The following procedure is supported only on bare metal platforms. Prerequisites You installed the Red Hat Enterprise Linux AI image on bare-metal. You initialized InstructLab You downloaded your preferred Granite LLMs. You have root user access on your machine. Procedure Create a directory for your certificate file and key by running the following command: $ mkdir -p `pwd`/nginx/ssl/ Create an OpenSSL configuration file with the proper configurations by running the following command: $ cat > openssl.cnf <<EOL [ req ] default_bits = 2048 distinguished_name = <req-distinguished-name> 1 x509_extensions = v3_req prompt = no [ req_distinguished_name ] C = US ST = California L = San Francisco O = My Company OU = My Division CN = rhelai.redhat.com [ v3_req ] subjectAltName = <alt-names> 2 basicConstraints = critical, CA:true subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer [ alt_names ] DNS.1 = rhelai.redhat.com 3 DNS.2 = www.rhelai.redhat.com 4 1 Specify the distinguished name for your requirements. 2 Specify the alternate name for your requirements. 3 4 Specify the server common name for RHEL AI. In the example, the server name is rhelai.redhat.com . Generate a self signed certificate with a Subject Alternative Name (SAN) enabled with the following commands: $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout `pwd`/nginx/ssl/rhelai.redhat.com.key -out `pwd`/nginx/ssl/rhelai.redhat.com.crt -config openssl.cnf $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout Create the Nginx Configuration file and add it to the `pwd /nginx/conf.d` by running the following command: mkdir -p `pwd`/nginx/conf.d echo 'server { listen 8443 ssl; server_name <rhelai.redhat.com> 1 ssl_certificate /etc/nginx/ssl/rhelai.redhat.com.crt; ssl_certificate_key /etc/nginx/ssl/rhelai.redhat.com.key; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } ' > `pwd`/nginx/conf.d/rhelai.redhat.com.conf 1 Specify the name of your server. In the example, the server name is rhelai.redhat.com Run the Nginx container with the new configurations by running the following command: $ podman run --net host -v `pwd`/nginx/conf.d:/etc/nginx/conf.d:ro,Z -v `pwd`/nginx/ssl:/etc/nginx/ssl:ro,Z nginx If you want to use port 443, you must run the podman run command as a root user.. You can now connect to a serving ilab machine using a secure endpoint URL. Example command: $ ilab chat -m /instructlab/instructlab/granite-7b-redhat-lab --endpoint-url https://rhelai.redhat.com:8443/v1 Optional: You can also get the server certificate and append it to the Certifi CA Bundle Get the server certificate by running the following command: $ openssl s_client -connect rhelai.redhat.com:8443 </dev/null 2>/dev/null | openssl x509 -outform PEM > server.crt Copy the certificate to you system’s trusted CA storage directory and update the CA trust store with the following commands: $ sudo cp server.crt /etc/pki/ca-trust/source/anchors/ $ sudo update-ca-trust You can append your certificate to the Certifi CA bundle by running the following command: $ cat server.crt >> $(python -m certifi) You can now run ilab model chat with a self-signed certificate. Example command: $ ilab chat -m /instructlab/instructlab/granite-7b-redhat-lab --endpoint-url https://rhelai.redhat.com:8443/v1 Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/building_your_rhel_ai_environment/serving_and_chatting"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.4"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4"}
{"title": "Configuring security services", "content": "Configuring security services Red Hat OpenStack Services on  Configuring the security features for Red Hat OpenStack Services on OpenShift OpenStack Documentation Team rhos-docs@redhat.com", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_security_services/index"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/building_your_rhel_ai_environment/legal-notice"}
{"title": "Chapter 2. Initializing InstructLab", "content": "Chapter 2. Initializing InstructLab You must initialize the InstructLab environments to begin working with the Red Hat Enterprise Linux AI models. 2.1. Creating your RHEL AI environment You can start interacting with LLM models and the RHEL AI tooling by initializing the InstructLab environment. Prerequisites You installed RHEL AI with the bootable container image. You have root user access on your machine. Procedure Optional: To set up training profiles, you need to know the GPU accelerators in your machine. You can view your system information by running the following command: $ ilab system info Initialize InstructLab by running the following command: $ ilab config init The CLI prompts you to setup your config.yaml . Example output Welcome to InstructLab CLI. This guide will help you to setup your environment. Please provide the following values to initiate the environment [press Enter for defaults]: Generating `/home/<example-user>/.config/instructlab/config.yaml` and `/home/<example-user>/.local/share/instructlab/internal/train_configuration/profiles`... Follow the CLI prompts to set up your training hardware configurations. This updates your config.yaml file and adds the proper train configurations for training an LLM model. Type the number of the YAML file that matches your hardware specifications. Important These profiles only add the necessary configurations to the train section of your config.yaml file, therefore any profile can be selected for inference serving a model. Example output of selecting training profiles Please choose a train profile to use: [0] No profile (CPU-only) [1] A100_H100_x2.yaml [2] A100_H100_x4.yaml [3] A100_H100_x8.yaml [4] L40_x4.yaml [5] L40_x8.yaml [6] L4_x8.yaml Enter the number of your choice [hit enter for the default CPU-only profile] [0]: Example output of a completed ilab config init run. You selected: A100_H100_x8.yaml Initialization completed successfully, you're ready to start using `ilab`. Enjoy! Configuring your system’s GPU for inference serving: This step is only required if you are using Red Hat Enterprise Linux AI exclusively for inference serving. Edit your config.yaml file by running the following command: $ ilab config edit In the evaluate section of the configurations file, edit the gpus: parameter and add the number of accelerators on your machine. evaluate: base_branch: null base_model: ~/.cache/instructlab/models/granite-7b-starter branch: null gpus: <num-gpus> In the vllm section of the serve field in the configuration file, edit the gpus: and vllm_args: [\"--tensor-parallel-size\"] parameters and add the number of accelerators on your machine. serve: backend: vllm chat_template: auto host_port: 127.0.0.1:8000 llama_cpp: gpu_layers: -1 llm_family: '' max_ctx_size: 4096 model_path: ~/.cache/instructlab/models/granite-7b-redhat-lab vllm: llm_family: '' vllm_args: [\"--tensor-parallel-size\", \"<num-gpus>\"] gpus: <num-gpus> If you want to use the skeleton taxonomy tree, which includes two skills and one knowledge qna.yaml file, you can clone the skeleton repository and place it in the taxonomy directory by running the following command: rm -rf ~/.local/share/instructlab/taxonomy/ ; git clone https://github.com/RedHatOfficial/rhelai-sample-taxonomy.git ~/.local/share/instructlab/taxonomy/ Directory structure of the InstructLab environment ├─ ~/.cache/instructlab/models/ 1 ├─ ~/.local/share/instructlab/datasets 2 ├─ ~/.local/share/instructlab/taxonomy 3 ├─ ~/.local/share/instructlab/phased/<phase1-or-phase2>/checkpoints/ 4 1 ~/.cache/instructlab/models/ : Contains all downloaded large language models, including the saved output of ones you generate with RHEL AI. 2 ~/.local/share/instructlab/datasets/ : Contains data output from the SDG phase, built on modifications to the taxonomy repository. 3 ~/.local/share/instructlab/taxonomy/ : Contains the skill and knowledge data. 4 ~/.local/share/instructlab/phased/<phase1-or-phase2>/checkpoints/ : Contains the output of the multi-phase training process Verification You can view the full config.yaml file by running the following command $ ilab config show You can also manually edit the config.yaml file by running the following command: $ ilab config edit Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/building_your_rhel_ai_environment/initializing_instructlab"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_insights/1-latest/html/getting_started_with_red_hat_insights/legal-notice"}
{"title": "Authentication and authorization", "content": "Authentication and authorization  Configuring user authentication and access controls for users and services Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/authentication_and_authorization/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_security_services/legal-notice"}
{"title": "Providing feedback on Red Hat documentation", "content": "Providing feedback on Red Hat documentation We appreciate and prioritize your feedback regarding our documentation. Provide as much detail as possible, so that your request can be quickly addressed. Prerequisites You are logged in to the Red Hat Customer Portal. Procedure To provide feedback, perform the following steps: Click the following link: Create Issue Describe the issue or enhancement in the Summary text box. Provide details about the issue or requested enhancement in the Description text box. Type your name in the Reporter text box. Click the Create button. This action creates a documentation ticket and routes it to the appropriate documentation team. Thank you for taking the time to provide feedback. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_insights/1-latest/html/getting_started_with_red_hat_insights/proc-providing-feedback-on-redhat-documentation"}
{"title": "Chapter 10. ServiceMonitor [monitoring.coreos.com/v1]", "content": "Chapter 10. ServiceMonitor [monitoring.coreos.com/v1] Description ServiceMonitor defines monitoring for a set of services. Type object Required spec 10.1. Specification Property Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta Standard object’s metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata spec object Specification of desired Service selection for target discovery by Prometheus. 10.1.1. .spec Description Specification of desired Service selection for target discovery by Prometheus. Type object Required selector Property Type Description attachMetadata object attachMetadata defines additional metadata which is added to the discovered targets. It requires Prometheus >= v2.37.0. bodySizeLimit string When defined, bodySizeLimit specifies a job level limit on the size of uncompressed response body that will be accepted by Prometheus. It requires Prometheus >= v2.28.0. endpoints array List of endpoints part of this ServiceMonitor. endpoints[] object Endpoint defines an endpoint serving Prometheus metrics to be scraped by Prometheus. jobLabel string jobLabel selects the label from the associated Kubernetes Service object which will be used as the job label for all metrics. For example if jobLabel is set to foo and the Kubernetes Service object is labeled with foo: bar , then Prometheus adds the job=\"bar\" label to all ingested metrics. If the value of this field is empty or if the label doesn’t exist for the given Service, the job label of the metrics defaults to the name of the associated Kubernetes Service . keepDroppedTargets integer Per-scrape limit on the number of targets dropped by relabeling that will be kept in memory. 0 means no limit. It requires Prometheus >= v2.47.0. labelLimit integer Per-scrape limit on number of labels that will be accepted for a sample. It requires Prometheus >= v2.27.0. labelNameLengthLimit integer Per-scrape limit on length of labels name that will be accepted for a sample. It requires Prometheus >= v2.27.0. labelValueLengthLimit integer Per-scrape limit on length of labels value that will be accepted for a sample. It requires Prometheus >= v2.27.0. namespaceSelector object Selector to select which namespaces the Kubernetes Endpoints objects are discovered from. podTargetLabels array (string) podTargetLabels defines the labels which are transferred from the associated Kubernetes Pod object onto the ingested metrics. sampleLimit integer sampleLimit defines a per-scrape limit on the number of scraped samples that will be accepted. scrapeClass string The scrape class to apply. scrapeProtocols array (string) scrapeProtocols defines the protocols to negotiate during a scrape. It tells clients the protocols supported by Prometheus in order of preference (from most to least preferred). If unset, Prometheus uses its default value. It requires Prometheus >= v2.49.0. selector object Label selector to select the Kubernetes Endpoints objects. targetLabels array (string) targetLabels defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics. targetLimit integer targetLimit defines a limit on the number of scraped targets that will be accepted. 10.1.2. .spec.attachMetadata Description attachMetadata defines additional metadata which is added to the discovered targets. It requires Prometheus >= v2.37.0. Type object Property Type Description node boolean When set to true, Prometheus must have the get permission on the Nodes objects. 10.1.3. .spec.endpoints Description List of endpoints part of this ServiceMonitor. Type array 10.1.4. .spec.endpoints[] Description Endpoint defines an endpoint serving Prometheus metrics to be scraped by Prometheus. Type object Property Type Description authorization object authorization configures the Authorization header credentials to use when scraping the target. Cannot be set at the same time as basicAuth , or oauth2 . basicAuth object basicAuth configures the Basic Authentication credentials to use when scraping the target. Cannot be set at the same time as authorization , or oauth2 . bearerTokenFile string File to read bearer token for scraping the target. Deprecated: use authorization instead. bearerTokenSecret object bearerTokenSecret specifies a key of a Secret containing the bearer token for scraping targets. The secret needs to be in the same namespace as the ServiceMonitor object and readable by the Prometheus Operator. Deprecated: use authorization instead. enableHttp2 boolean enableHttp2 can be used to disable HTTP2 when scraping the target. filterRunning boolean When true, the pods which are not running (e.g. either in Failed or Succeeded state) are dropped during the target discovery. If unset, the filtering is enabled. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase followRedirects boolean followRedirects defines whether the scrape requests should follow HTTP 3xx redirects. honorLabels boolean When true, honorLabels preserves the metric’s labels when they collide with the target’s labels. honorTimestamps boolean honorTimestamps controls whether Prometheus preserves the timestamps when exposed by the target. interval string Interval at which Prometheus scrapes the metrics from the target. If empty, Prometheus uses the global scrape interval. metricRelabelings array metricRelabelings configures the relabeling rules to apply to the samples before ingestion. metricRelabelings[] object RelabelConfig allows dynamic rewriting of the label set for targets, alerts, scraped samples and remote write samples. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config oauth2 object oauth2 configures the OAuth2 settings to use when scraping the target. It requires Prometheus >= 2.27.0. Cannot be set at the same time as authorization , or basicAuth . params object params define optional HTTP URL parameters. params{} array (string) path string HTTP path from which to scrape for metrics. If empty, Prometheus uses the default value (e.g. /metrics ). port string Name of the Service port which this endpoint refers to. It takes precedence over targetPort . proxyUrl string proxyURL configures the HTTP Proxy URL (e.g. \"http://proxyserver:2195\") to go through when scraping the target. relabelings array relabelings configures the relabeling rules to apply the target’s metadata labels. The Operator automatically adds relabelings for a few standard Kubernetes fields. The original scrape job’s name is available via the \\__tmp_prometheus_job_name label. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config relabelings[] object RelabelConfig allows dynamic rewriting of the label set for targets, alerts, scraped samples and remote write samples. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config scheme string HTTP scheme to use for scraping. http and https are the expected values unless you rewrite the scheme label via relabeling. If empty, Prometheus uses the default value http . scrapeTimeout string Timeout after which Prometheus considers the scrape to be failed. If empty, Prometheus uses the global scrape timeout unless it is less than the target’s scrape interval value in which the latter is used. targetPort integer-or-string Name or number of the target port of the Pod object behind the Service. The port must be specified with the container’s port property. tlsConfig object TLS configuration to use when scraping the target. trackTimestampsStaleness boolean trackTimestampsStaleness defines whether Prometheus tracks staleness of the metrics that have an explicit timestamp present in scraped data. Has no effect if honorTimestamps is false. It requires Prometheus >= v2.48.0. 10.1.5. .spec.endpoints[].authorization Description authorization configures the Authorization header credentials to use when scraping the target. Cannot be set at the same time as basicAuth , or oauth2 . Type object Property Type Description credentials object Selects a key of a Secret in the namespace that contains the credentials for authentication. type string Defines the authentication type. The value is case-insensitive. \"Basic\" is not a supported value. Default: \"Bearer\" 10.1.6. .spec.endpoints[].authorization.credentials Description Selects a key of a Secret in the namespace that contains the credentials for authentication. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.7. .spec.endpoints[].basicAuth Description basicAuth configures the Basic Authentication credentials to use when scraping the target. Cannot be set at the same time as authorization , or oauth2 . Type object Property Type Description password object password specifies a key of a Secret containing the password for authentication. username object username specifies a key of a Secret containing the username for authentication. 10.1.8. .spec.endpoints[].basicAuth.password Description password specifies a key of a Secret containing the password for authentication. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.9. .spec.endpoints[].basicAuth.username Description username specifies a key of a Secret containing the username for authentication. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.10. .spec.endpoints[].bearerTokenSecret Description bearerTokenSecret specifies a key of a Secret containing the bearer token for scraping targets. The secret needs to be in the same namespace as the ServiceMonitor object and readable by the Prometheus Operator. Deprecated: use authorization instead. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.11. .spec.endpoints[].metricRelabelings Description metricRelabelings configures the relabeling rules to apply to the samples before ingestion. Type array 10.1.12. .spec.endpoints[].metricRelabelings[] Description RelabelConfig allows dynamic rewriting of the label set for targets, alerts, scraped samples and remote write samples. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config Type object Property Type Description action string Action to perform based on the regex matching. Uppercase and Lowercase actions require Prometheus >= v2.36.0. DropEqual and KeepEqual actions require Prometheus >= v2.41.0. Default: \"Replace\" modulus integer Modulus to take of the hash of the source label values. Only applicable when the action is HashMod . regex string Regular expression against which the extracted value is matched. replacement string Replacement value against which a Replace action is performed if the regular expression matches. Regex capture groups are available. separator string Separator is the string between concatenated SourceLabels. sourceLabels array (string) The source labels select values from existing labels. Their content is concatenated using the configured Separator and matched against the configured regular expression. targetLabel string Label to which the resulting string is written in a replacement. It is mandatory for Replace , HashMod , Lowercase , Uppercase , KeepEqual and DropEqual actions. Regex capture groups are available. 10.1.13. .spec.endpoints[].oauth2 Description oauth2 configures the OAuth2 settings to use when scraping the target. It requires Prometheus >= 2.27.0. Cannot be set at the same time as authorization , or basicAuth . Type object Required clientId clientSecret tokenUrl Property Type Description clientId object clientId specifies a key of a Secret or ConfigMap containing the OAuth2 client’s ID. clientSecret object clientSecret specifies a key of a Secret containing the OAuth2 client’s secret. endpointParams object (string) endpointParams configures the HTTP parameters to append to the token URL. scopes array (string) scopes defines the OAuth2 scopes used for the token request. tokenUrl string tokenURL configures the URL to fetch the token from. 10.1.14. .spec.endpoints[].oauth2.clientId Description clientId specifies a key of a Secret or ConfigMap containing the OAuth2 client’s ID. Type object Property Type Description configMap object ConfigMap containing data to use for the targets. secret object Secret containing data to use for the targets. 10.1.15. .spec.endpoints[].oauth2.clientId.configMap Description ConfigMap containing data to use for the targets. Type object Required key Property Type Description key string The key to select. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the ConfigMap or its key must be defined 10.1.16. .spec.endpoints[].oauth2.clientId.secret Description Secret containing data to use for the targets. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.17. .spec.endpoints[].oauth2.clientSecret Description clientSecret specifies a key of a Secret containing the OAuth2 client’s secret. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.18. .spec.endpoints[].params Description params define optional HTTP URL parameters. Type object 10.1.19. .spec.endpoints[].relabelings Description relabelings configures the relabeling rules to apply the target’s metadata labels. The Operator automatically adds relabelings for a few standard Kubernetes fields. The original scrape job’s name is available via the \\__tmp_prometheus_job_name label. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config Type array 10.1.20. .spec.endpoints[].relabelings[] Description RelabelConfig allows dynamic rewriting of the label set for targets, alerts, scraped samples and remote write samples. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config Type object Property Type Description action string Action to perform based on the regex matching. Uppercase and Lowercase actions require Prometheus >= v2.36.0. DropEqual and KeepEqual actions require Prometheus >= v2.41.0. Default: \"Replace\" modulus integer Modulus to take of the hash of the source label values. Only applicable when the action is HashMod . regex string Regular expression against which the extracted value is matched. replacement string Replacement value against which a Replace action is performed if the regular expression matches. Regex capture groups are available. separator string Separator is the string between concatenated SourceLabels. sourceLabels array (string) The source labels select values from existing labels. Their content is concatenated using the configured Separator and matched against the configured regular expression. targetLabel string Label to which the resulting string is written in a replacement. It is mandatory for Replace , HashMod , Lowercase , Uppercase , KeepEqual and DropEqual actions. Regex capture groups are available. 10.1.21. .spec.endpoints[].tlsConfig Description TLS configuration to use when scraping the target. Type object Property Type Description ca object Certificate authority used when verifying server certificates. caFile string Path to the CA cert in the Prometheus container to use for the targets. cert object Client certificate to present when doing client-authentication. certFile string Path to the client cert file in the Prometheus container for the targets. insecureSkipVerify boolean Disable target certificate validation. keyFile string Path to the client key file in the Prometheus container for the targets. keySecret object Secret containing the client key file for the targets. serverName string Used to verify the hostname for the targets. 10.1.22. .spec.endpoints[].tlsConfig.ca Description Certificate authority used when verifying server certificates. Type object Property Type Description configMap object ConfigMap containing data to use for the targets. secret object Secret containing data to use for the targets. 10.1.23. .spec.endpoints[].tlsConfig.ca.configMap Description ConfigMap containing data to use for the targets. Type object Required key Property Type Description key string The key to select. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the ConfigMap or its key must be defined 10.1.24. .spec.endpoints[].tlsConfig.ca.secret Description Secret containing data to use for the targets. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.25. .spec.endpoints[].tlsConfig.cert Description Client certificate to present when doing client-authentication. Type object Property Type Description configMap object ConfigMap containing data to use for the targets. secret object Secret containing data to use for the targets. 10.1.26. .spec.endpoints[].tlsConfig.cert.configMap Description ConfigMap containing data to use for the targets. Type object Required key Property Type Description key string The key to select. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the ConfigMap or its key must be defined 10.1.27. .spec.endpoints[].tlsConfig.cert.secret Description Secret containing data to use for the targets. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.28. .spec.endpoints[].tlsConfig.keySecret Description Secret containing the client key file for the targets. Type object Required key Property Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. TODO: Add other useful fields. apiVersion, kind, uid? More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Drop kubebuilder:default when controller-gen doesn’t need it https://github.com/kubernetes-sigs/kubebuilder/issues/3896 . optional boolean Specify whether the Secret or its key must be defined 10.1.29. .spec.namespaceSelector Description Selector to select which namespaces the Kubernetes Endpoints objects are discovered from. Type object Property Type Description any boolean Boolean describing whether all namespaces are selected in contrast to a list restricting them. matchNames array (string) List of namespace names to select from. 10.1.30. .spec.selector Description Label selector to select the Kubernetes Endpoints objects. Type object Property Type Description matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. matchExpressions[] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. matchLabels object (string) matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. 10.1.31. .spec.selector.matchExpressions Description matchExpressions is a list of label selector requirements. The requirements are ANDed. Type array 10.1.32. .spec.selector.matchExpressions[] Description A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Type object Required key operator Property Type Description key string key is the label key that the selector applies to. operator string operator represents a key’s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. values array (string) values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. 10.2. API endpoints The following API endpoints are available: /apis/monitoring.coreos.com/v1/servicemonitors GET : list objects of kind ServiceMonitor /apis/monitoring.coreos.com/v1/namespaces/{namespace}/servicemonitors DELETE : delete collection of ServiceMonitor GET : list objects of kind ServiceMonitor POST : create a ServiceMonitor /apis/monitoring.coreos.com/v1/namespaces/{namespace}/servicemonitors/{name} DELETE : delete a ServiceMonitor GET : read the specified ServiceMonitor PATCH : partially update the specified ServiceMonitor PUT : replace the specified ServiceMonitor 10.2.1. /apis/monitoring.coreos.com/v1/servicemonitors HTTP method GET Description list objects of kind ServiceMonitor Table 10.1. HTTP responses HTTP code Reponse body 200 - OK ServiceMonitorList schema 401 - Unauthorized Empty 10.2.2. /apis/monitoring.coreos.com/v1/namespaces/{namespace}/servicemonitors HTTP method DELETE Description delete collection of ServiceMonitor Table 10.2. HTTP responses HTTP code Reponse body 200 - OK Status schema 401 - Unauthorized Empty HTTP method GET Description list objects of kind ServiceMonitor Table 10.3. HTTP responses HTTP code Reponse body 200 - OK ServiceMonitorList schema 401 - Unauthorized Empty HTTP method POST Description create a ServiceMonitor Table 10.4. Query parameters Parameter Type Description dryRun string When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed fieldValidation string fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered. Table 10.5. Body parameters Parameter Type Description body ServiceMonitor schema Table 10.6. HTTP responses HTTP code Reponse body 200 - OK ServiceMonitor schema 201 - Created ServiceMonitor schema 202 - Accepted ServiceMonitor schema 401 - Unauthorized Empty 10.2.3. /apis/monitoring.coreos.com/v1/namespaces/{namespace}/servicemonitors/{name} Table 10.7. Global path parameters Parameter Type Description name string name of the ServiceMonitor HTTP method DELETE Description delete a ServiceMonitor Table 10.8. Query parameters Parameter Type Description dryRun string When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed Table 10.9. HTTP responses HTTP code Reponse body 200 - OK Status schema 202 - Accepted Status schema 401 - Unauthorized Empty HTTP method GET Description read the specified ServiceMonitor Table 10.10. HTTP responses HTTP code Reponse body 200 - OK ServiceMonitor schema 401 - Unauthorized Empty HTTP method PATCH Description partially update the specified ServiceMonitor Table 10.11. Query parameters Parameter Type Description dryRun string When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed fieldValidation string fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered. Table 10.12. HTTP responses HTTP code Reponse body 200 - OK ServiceMonitor schema 401 - Unauthorized Empty HTTP method PUT Description replace the specified ServiceMonitor Table 10.13. Query parameters Parameter Type Description dryRun string When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed fieldValidation string fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered. Table 10.14. Body parameters Parameter Type Description body ServiceMonitor schema Table 10.15. HTTP responses HTTP code Reponse body 200 - OK ServiceMonitor schema 201 - Created ServiceMonitor schema 401 - Unauthorized Empty Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring_apis/servicemonitor-monitoring-coreos-com-v1"}
{"title": "CLI reference", "content": "CLI reference Red Hat Enterprise Linux AI 1.4 RHEL AI command line interface (CLI) reference Red Hat RHEL AI Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.4/html/cli_reference/index"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant/2.x_latest"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18"}
{"title": "Configuring HA clusters to manage SAP NetWeaver or SAP S/4HANA Application server instances using the RHEL HA Add-On", "content": "Configuring HA clusters to manage SAP NetWeaver or SAP S/4HANA Application server instances using the RHEL HA Add-On Red Hat Enterprise Linux for SAP Solutions 9 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_ha_clusters_to_manage_sap_netweaver_or_sap_s4hana_application_server_instances_using_the_rhel_ha_add-on/index"}
{"title": "Dynamic plugins reference", "content": "Dynamic plugins reference Red Hat Developer Hub 1.4 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_developer_hub/1.4/html/dynamic_plugins_reference/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.4/html/cli_reference/legal-notice"}
{"title": "Chapter 1. Creating and managing activation keys", "content": "Chapter 1. Creating and managing activation keys An activation key is a preshared authentication token that enables authorized users to register and auto-configure systems. Running a registration command with an activation key and organization ID combination, instead of a username and password combination, increases security and facilitates automation. Administrative users in your organization can create and manage activation keys on the Red Hat Hybrid Cloud Console. When an authorized user enters a preconfigured activation key to register a system on the command line, all of the system-level settings configured on the activation key are automatically applied to the system during the registration process. You can also use an activation key in a Kickstart file to bulk-provision the registration of multiple Red Hat Enterprise Linux (RHEL) instances without exposing personal username and password values. Your organization’s activation keys and organization ID are displayed on the Activation Keys page in the Hybrid Cloud Console. Each user’s access to your organization’s activation keys is managed through a role-based access control (RBAC) system for the Hybrid Cloud Console. For example: Only users with the RHC user role can view the activation keys on the Activation Keys page. Only users with the RHC administrator role can create, configure, edit, and delete activation keys. Only users with root privileges or their equivalent can enter an activation key and organization ID with a registration command to connect and automatically configure systems from the command line. Users in the Organization Administrator group for your organization use the RBAC system to assign roles to other users in your organization. An Organization Administrator has the RHC administrator role by default. If you have questions about your access permissions, ask an Organization Administrator in your organization. 1.1. Viewing an activation key With the RHC user role, you can view your organization’s numeric identifier (organization ID) and available activation keys on the Activation Keys page in the Hybrid Cloud Console. You can also view additional details, such as the Workload setting and additional enabled repositories for each activation key in your organization. The Key Name column shows the unique name of the activation key. The Role column shows the role value for the system purpose attribute set on the key. A potential role value is Red Hat Enterprise Linux Server . The SLA column shows the service level agreement value for the system purpose attribute set on the key. A potential service level agreement value is Premium . The Usage column shows the usage value for the system purpose attribute that is set on the key. A potential usage value is Production . If the RHC administrator sets no system purpose attributes on the activation key, then the Role, SLA, and Usage columns show no values. Prerequisites You are logged in to the Red Hat Hybrid Cloud Console. You have the RHC user or RHC administrator role in the role-based access control (RBAC) system for the Red Hat Hybrid Cloud Console. Procedure To view the activation keys for your organization in the Hybrid Cloud Console, complete the following step: From the Hybrid Cloud Console home page, click Services > System Configuration > Activation Keys . Note Activation keys are listed alphabetically by default. 1.2. Creating an activation key With the RHC administrator role, you can use the Hybrid Cloud Console interface to create activation keys that root users in your organization can use as an authentication token. During the creation process, you can configure the activation key to set system-level features, such as system purpose attributes, to host systems. When an authorized user uses the preconfigured activation key to register a system to Red Hat, the selected attributes are automatically applied to the system during the registration process. The activation key creation wizard guides you through the following fields: Name The activation key name uniquely identifies it. You can use this name to locate the key in the table on the Activation Keys page or to specify the key in a CLI command or automation script. Workload The workload associates the appropriate selection of repositories to the activation key. You can edit these repositories on the activation key details page after the key is created. When creating an activation key, you can select either Latest release or Extended support for the workload. Latest release is selected by default. If your account has subscriptions that are eligible for extended update support (EUS), then you can select Extended support and then select an EUS product and version available to your account. If your account does not have any subscriptions that are eligible for EUS, then the Extended supported option is disabled. System purpose The subscriptions service uses system purpose values to filter and identify hosts. You can set values for the Role , Service Level Agreement (SLA) , and Usage attributes to ensure that subscriptions are accurately reported for the system. You can select the system purpose values that are available to your account from the drop-down menus. Review You can review your entries and selections for each field before creating the activation key. If you do not select a value for an optional field, then the default value is Not defined . Prerequisites You are logged in to the Red Hat Hybrid Cloud Console. You have the RHC administrator role in the role-based access control (RBAC) system for the Red Hat Hybrid Cloud Console. Procedure To create an activation key in the Hybrid Cloud Console, complete the following steps: From the Hybrid Cloud Console home page, click Services > System Configuration > Activation Keys . From the Activation Keys page, click Create activation key . In the Name field, enter a unique name for the activation key. Click Next . Note If you enter a name that already exists for an activation key in your organization, then you will receive an error message and the key will not be created. Select a workload option to associate the appropriate selection of repositories with the activation key. Click Next . Optional: In the Role , Service Level Agreement (SLA) , and Usage fields, select the system purpose attribute value that you want to set on the activation key. Click Next . Note Only the system purpose attributes that are available to your organization’s account are selectable. Review the information that you entered into each field. If the information is correct, click Create . To make changes to the activation key settings or to enable additional repositories, click View activation key . The activation key details page opens. Verification The new activation key is listed on the Activation Keys page of the Hybrid Cloud Console. 1.3. Enabling additional repositories on an activation key By default, your system has access to content repositories that contain software packages that you can use to set up and manage your system. However, enabling additional repositories gives your system access to features and capabilities beyond the default repositories. It is no longer necessary to use command line tools to manually enable your system to access additional content repositories or to use automation scripts after the system is registered. With the RHC administrator role, you can use the Red Hat Hybrid Cloud Console interface to add selected repositories to an exisiting activation key. When a root user uses a preconfigured activation key to register a system from the command line, any content repositories that have been added to the activation key are automatically enabled for system access during the registration process. Using an activation key to automate the repository enablement process allows you to configure multiple system settings in one place for simplified system management. You can also use activation keys to apply system settings to multiple instances of Red Hat Enterprise Linux (RHEL) for automated bulk provisioning. Users with the RHC user role can see all the repositories that are associated with each activation key, but only users with the RHC administrator role can perform management functions, such as adding or deleting repositories on an activation key. If you have questions about your access permissions, contact a user in your organization with the Organization Administrator role in the Hybrid Cloud Console role-based access control (RBAC) system. Prerequisites You are logged in to the Red Hat Hybrid Cloud Console. You have the RHC administrator role in the role-based access control (RBAC) system for the Red Hat Hybrid Cloud Console. Procedure To enable additional repositories on an activation key in the Hybrid Cloud Console, complete the following steps: From the Hybrid Cloud Console home page, click Services > System Configuration > Activation Keys . From the Activation Keys table, click the name of the activation key that you want to configure with additional repositories. Note Only users with the RHC user role can click an activation key to view its details. If you have questions about your access permissions, contact a user in your organization with the Organization Administrator RBAC role. From the Additional repositories table, click Add repositories . Note Only users with the RHC administrator role can add or delete enabled repositories on an activation key. If you do not have sufficient access permissions to complete this step, Add repositories is disabled. If you have questions about your access permissions, contact a user in your organization with the Organization Administrator RBAC role. Select each additional repository that you want to enable with the activation key. Click Save Changes . Result If a RHC administrator has enabled additional repositories on an activation key, then those repositories are listed in the Additional repositories table for the selected activation key. 1.4. Editing an activation key With the RHC administrator role, you can use the Hybrid Cloud Console interface to edit the activation keys on the Activation Keys page. Specifically, you can add, change, or remove the following configurations on an existing activation key: System purpose attributes Workload, such as the release version of your system Additional enabled repositories Note You cannot edit the name of the activation key after it has been created. 1.4.1. Editing system purpose settings on an activation key You can change the system purpose configuration on an existing activation key by selecting a different system purpose attribute value from the Role , Service Level Agreement (SLA) , or Usage drop-down list. Possible selections for each attribute include the following values: Role Red Hat Enterprise Linux Server Red Hat Enterprise Linux Workstation Red Hat Enterprise Linux Compute Node Not defined Service Level Agreement (SLA) Premium Standard Self-Support Not defined Usage Production Development/Test Not defined Prerequisites You are logged in to the Red Hat Hybrid Cloud Console. You have the RHC administrator role in the role-based access control (RBAC) system for the Red Hat Hybrid Cloud Console. Procedure To edit the system purpose attributes on an activation key, complete the following steps: From the Hybrid Cloud Console home page, click Services > System Configuration > Activation Keys . From the Activation Keys table, click the name of the activation key that you want to edit. From the System Purpose section of the activation key details page, click Edit . Select the value from the Role, SLA, or Usage drop-down list that you want to set on the activation key. Click Save changes . 1.4.2. Editing workload settings on an activation key You can change the workload configuration on an existing activation key by selecting a different value from the Release version drop-down list. Possible selections for the workload include the following RHEL release versions: 8.1 8.2 8.4 8.6 8.8 9.0 9.2 Not defined Prerequisites You are logged in to the Red Hat Hybrid Cloud Console. You have the RHC administrator role in the role-based access control (RBAC) system for the Red Hat Hybrid Cloud Console. Procedure To edit the workload setting on an activation key, complete the following steps: From the Hybrid Cloud Console home page, click Services > System Configuration > Activation Keys . From the Activation Keys table, click the name of the activation key that you want to edit. From the Workload section of the activation key details page, click Edit . Select the value from the Release version drop-down list that you want to set on the activation key. Click Save changes . 1.5. Deleting an activation key With the RHC administrator role, you can use the Hybrid Cloud Console interface to delete an activation key from the table on the Activation Keys page. You might want to delete an unwanted or compromised activation key for security or maintenance purposes. However, deleting an activation key that is referenced in an automation script will impact the ability of that automation to function. To avoid any negative impacts to your automated processes, complete one of the following actions: Remove the unwanted activation key from the automation script. Retire the automation script prior to deleting the key. Prerequisites You are logged in to the Red Hat Hybrid Cloud Console. You have the RHC administrator role in the role-based access control (RBAC) system for the Red Hat Hybrid Cloud Console. Procedure To delete an activation key in the Hybrid Cloud Console, complete the following steps: From the Hybrid Cloud Console home page, click Services > System Configuration > Activation Keys . From the Activation Keys page, locate the row containing the activation key that you want to delete. Click the Delete icon. In the Delete Activation Key window, review the information about deleting activation keys. If you want to continue with the deletion, click Delete . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/subscription_central/1-latest/html/getting_started_with_activation_keys_on_the_hybrid_cloud_console/assembly-creating-managing-activation-keys"}
{"title": "Support", "content": "Support Red Hat OpenShift Service on AWS 4 Red Hat OpenShift Service on AWS Support. Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html-single/support/index"}
{"title": "Chapter 1. Red Hat Enterprise Linux AI command line interface reference", "content": "Chapter 1. Red Hat Enterprise Linux AI command line interface reference This reference provides descriptions and examples for the Red Hat Enterprise Linux AI CLI ( ilab ) commands. 1.1. Red Hat Enterprise Linux AI CLI commands 1.1.1. ilab config Command Group for Interacting with the configuration of InstructLab Example usage # Prints the usable commands in the config group ilab config 1.1.1.1. ilab config init Initializes environment for InstructLab Example usage # Set up the InstructLab environment ilab config init 1.1.1.2. ilab config show Displays current state of the config file stored at ~/.config/instructlab/config.yaml Example usage # Shows the `config.yaml` file on your system ilab config show 1.1.1.3. ilab config edit Allows you to edit the config stored at ~/.config/config.yaml Example usage # Opens a vim shell where you can edit your config file ilab config edit 1.1.2. ilab data Command Group for Interacting with the data generated by InstructLab Example usage # Prints the usable commands in the data group ilab data 1.1.2.1. ilab data generate Runs the synthetic data generation (SDG) process for InstructLab Example usage # Runs the SDG process on the default model, the default model is specified in the `~/.config/config.yaml` ilab data generate # Runs the SDG process on a selected model ilab data generate --model <model-name> # Runs the SDG process on the customized taxonomy path ilab data generate --taxonomy-path <path-to-taxonomy> # Edits the `config.yaml` to use a specified number of GPUs in SDG ilab data generate --gpus <num-gpus> 1.1.2.2. ilab data list Displays every dataset in the datasets directory, ` ~/.local/instructlab/datasets` , on your machine Example usage # List every dataset in the datasets directory ilab data list 1.1.3. ilab model Command Group for Interacting with the models in InstructLab Example usage # Prints the usable commands in the model group ilab model 1.1.3.1. ilab model chat Run a chat using the modified model Example usage # Creates a virtual environment to chat with the model ilab model chat # Creates a virtual environment to chat with a specified model ilab model chat --model <model-name> 1.1.3.2. ilab model download Downloads the model(s) Example usage # Downloads the default models ilab model download # Downloads the models from a specific repository ilab model download --repository <name-of-repository> 1.1.3.3. ilab model evaluate Runs the evaluation process on the model Example usage # Runs the evaluation process on the MMLU benchmark ilab model evaluate --benchmark mmlu # Runs the evaluation process on the MT_BENCH benchmark ilab model evaluate --benchmark mt_bench # Runs the evaluation process on the MMLU_BRANCH benchmark ilab model evaluate --benchmark mmlu_branch # Runs the evaluation process on the MT_BENCH_BRANCH benchmark ilab model evaluate --benchmark mt_bench_branch 1.1.3.4. ilab model list Lists all the models installed on your system Example usage * List all the installed models ilab model list 1.1.3.5. ilab model train Runs the training process on the model Example usage # Runs the training process on the default model from the config.yaml ilab model train # Runs the training process on a specified model ilab model train --model-name <name-of-model> 1.1.3.6. ilab model serve Serves the model on an endpoint Example usage # Serves the default model to the server ilab model serve # Serves the specified model to the server ilab model serve --model-path <path-to-model> # Serves the default model using a specified number of GPUs ilab model serve --gpus <num-gpus> 1.1.4. ilab system Command group for all system-related commands Example usage # Prints the usable commands in the system group ilab system 1.1.4.1. ilab system info Displays the hardware specifications of your system Example usage #Prints the hardware specifications of your machine ilab system info 1.1.5. ilab taxonomy Command Group for Interacting with the taxonomy path of InstructLab Example usage # Prints the usable commands in the taxonomy group ilab taxonomy 1.1.5.1. ilab taxonomy diff Lists taxonomy files that you changed and verifies that the taxonomy is valid Example usage # Prints the taxonomy files you changed and verifies that the taxonomy is valid ilab taxonomy diff # Prints the taxonomy files in a specified path and verifies that the taxonomy is valid ilab taxonomy diff --taxonomy-path <path-to-taxonomy> Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.4/html/cli_reference/cli_reference"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_serverless/1.35"}
{"title": "Using Ansible plug-ins for Red Hat Developer Hub", "content": "Using Ansible plug-ins for Red Hat Developer Hub Red Hat Ansible Automation Platform 2.5 Use Ansible plug-ins for Red Hat Developer Hub Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_ansible_plug-ins_for_red_hat_developer_hub/index"}
{"title": "Updating clusters", "content": "Updating clusters  Updating OpenShift Container Platform clusters Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/updating_clusters/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_ha_clusters_to_manage_sap_netweaver_or_sap_s4hana_application_server_instances_using_the_rhel_ha_add-on/legal-notice"}
{"title": "Chapter 7. References", "content": "Chapter 7. References 7.1. Red Hat Configuring and managing high availability clusters on RHEL 9 Support Policies for RHEL High Availability Clusters Support Policies for RHEL High Availability Clusters - Fencing/STONITH Support Policies for RHEL High Availability Clusters - Management of SAP S/4HANA Support Policies for RHEL High Availability Clusters - Management of SAP NetWeaver in a Cluster Red Hat HA Solutions for SAP HANA, S/4HANA and NetWeaver based SAP Applications How to enable the SAP HA Interface for SAP ABAP application server instances managed by the RHEL HA Add-On? How to manage standalone SAP Web Dispatcher instances using the RHEL HA Add-On The Systemd-Based SAP Startup Framework 7.2. SAP SAP Note 1552925 - Linux: High Availability Cluster Solutions SAP Note 1693245 - SAP HA Script Connector Library SAP Note 1908655 - Support details for Red Hat Enterprise Linux HA Add-On SAP Note 2630416 - Support for Standalone Enqueue Server 2 SAP Note 2641322 - Installation of ENSA2 and update from ENSA1 to ENSA2 when using the Red Hat HA solutions for SAP SAP Note 3108316 - Red Hat Enterprise Linux 9.x: Installation and Configuration Standalone Enqueue Server | SAP Help Portal Setting up Enqueue Replication Server Fail over | SAP Blogs High Availability with the Standalone Enqueue Server Evolution of ENSA2 and ERS2… | SAP Blogs Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_ha_clusters_to_manage_sap_netweaver_or_sap_s4hana_application_server_instances_using_the_rhel_ha_add-on/asmb_ref_configuring-clusters-to-manage"}
{"title": "Configuring a Cost-Optimized SAP S/4HANA HA cluster (HANA System Replication + ENSA2) using the RHEL HA Add-On", "content": "Configuring a Cost-Optimized SAP S/4HANA HA cluster (HANA System Replication + ENSA2) using the RHEL HA Add-On Red Hat Enterprise Linux for SAP Solutions 9 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_a_cost-optimized_sap_s4hana_ha_cluster_hana_system_replication_ensa2_using_the_rhel_ha_add-on/index"}
{"title": "Configuring SAP HANA Scale-Up Multitarget System Replication for disaster recovery", "content": "Configuring SAP HANA Scale-Up Multitarget System Replication for disaster recovery Red Hat Enterprise Linux for SAP Solutions 9 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_sap_hana_scale-up_multitarget_system_replication_for_disaster_recovery/index"}
{"title": "CLI reference", "content": "CLI reference Red Hat Enterprise Linux AI 1.1 RHEL AI command line interface (CLI) reference Red Hat RHEL AI Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/cli_reference/index"}
{"title": "Using content navigator", "content": "Using content navigator Red Hat Ansible Automation Platform 2.5 Develop content that is compatible with Ansible Automation Platform Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_content_navigator/index"}
{"title": "Installing Ansible plug-ins for Red Hat Developer Hub", "content": "Installing Ansible plug-ins for Red Hat Developer Hub Red Hat Ansible Automation Platform 2.5 Install and configure Ansible plug-ins for Red Hat Developer Hub Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/installing_ansible_plug-ins_for_red_hat_developer_hub/index"}
{"title": "Storage APIs", "content": "Storage APIs  Reference guide for storage APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/storage_apis/index"}
{"title": "Networking", "content": "Networking  Configuring and managing cluster networking Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/networking/index"}
{"title": "Template APIs", "content": "Template APIs  Reference guide for template APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/template_apis/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_ansible_plug-ins_for_red_hat_developer_hub/legal-notice"}
{"title": "User and group APIs", "content": "User and group APIs  Reference guide for user and group APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/user_and_group_apis/index"}
{"title": "Validation and troubleshooting", "content": "Validation and troubleshooting  Validating and troubleshooting an OpenShift Container Platform installation Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/validation_and_troubleshooting/index"}
{"title": "Chapter 2. Providing feedback in the Ansible plug-ins", "content": "Chapter 2. Providing feedback in the Ansible plug-ins The Ansible plug-ins provide a feedback form where you can suggest new features and content, as well as general feedback. Click the Ansible A icon in the Red Hat Developer Hub navigation panel. Click the Feedback icon to display the feedback form. Enter the feedback you want to provide. Tick the I understand that feedback is shared with Red Hat checkbox. Click Submit . Note To ensure that Red Hat receives your feedback, exclude your Red Hat Developer Hub URL in any browser ad blockers or privacy tools. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_ansible_plug-ins_for_red_hat_developer_hub/rhdh-feedback_aap-plugin-rhdh-using"}
{"title": "Security APIs", "content": "Security APIs  Reference guide for security APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/security_apis/index"}
{"title": "Chapter 1. Using the Ansible plug-ins", "content": "Chapter 1. Using the Ansible plug-ins You can use Ansible plug-ins for Red Hat Developer Hub (RHDH) to learn about Ansible, create automation projects, and access opinionated workflows and tools to develop and test your automation code. From the Red Hat Developer Hub UI, you can navigate to your Ansible Automation Platform instance, where you can configure and run automation jobs. This document describes how to use the Ansible plug-ins for Red Hat Developer Hub. It presents a worked example of developing a playbook project for automating updates to your firewall configuration on RHEL systems. 1.1. Optional requirement The Ansible plug-ins for Red Hat Developer Hub link to Learning Paths on the Red Hat developer portal, developers.redhat.com/learn . To access the Learning Paths, you must have a Red Hat account and you must be able to log in to developers.redhat.com . 1.2. Dashboard navigation When you log in to Red Hat Developer Hub (RHDH), the main RHDH menu and dashboard are displayed. To view the dashboard for Ansible plug-ins for Red Hat Developer Hub, click Ansible in the Red Hat Developer Hub navigation panel. The plug-in dashboard illustrates the steps you need to take from learning about Ansible to deploying automation jobs from Ansible Automation Platform: Overview displays the main dashboard page. Learn provides links to resources curated by Red Hat that introduce you to Ansible and provide step-by-step examples to get you started. For more information, see Learning about Ansible . Discover existing collections links to private automation hub, if configured in the plug-ins, or to automation hub hosted on the Red Hat Hybrid Cloud Console. Automation hub stores existing collections and execution environments that you can use in your projects. For more information, see Discovering existing collections . Create creates new projects in your configured Source Control Management platforms such as GitHub. For more information, see Creating a project . Develop links you to OpenShift Dev Spaces, if configured in the Ansible plug-ins installation. OpenShift Dev Spaces provides on-demand, web-based Integrated Development Environments (IDEs), where you can develop automation content. For more information, see Developing projects . Operate connects you to Ansible Automation Platform, where you can create and run automation jobs that use the projects you have developed. For more information, see Setting up a controller project to run your playbook project . 1.3. Learning about Ansible To learn more about getting started with automation, click Learn from the Overview page of the plug-in dashboard. The Learn page provides the following options for learning: Learning Paths lists a curated selection of learning tools hosted on developers.redhat.com that guide you through the foundations of working with Ansible, the Ansible VS Code extension, and using YAML. You can select other Ansible learning paths from the Useful links section. Labs are self-led labs that are designed to give you hands-on experience in writing Ansible content and using Ansible development tools. 1.4. Discovering existing collections From the Overview page in the Ansible plug-ins dashboard on Red Hat Developer Hub, click Discover Existing Collections . The links in this pane provide access to the source of reusable automation content collections that you configured during plug-in installation. If you configured private automation hub when installing the plug-in, you can click Go to Automation Hub to view the collections and execution environments that your enterprise has curated. If you did not configure a private automation hub URL when installing the plug-in, the Discover existing collection pane provides a link to Red Hat automation hub on console.redhat.com. You can explore certified and validated Ansible content collections on this site. 1.5. Creating a project Prerequisite Ensure you have the correct access (RBAC) to view the templates in Red Hat Developer Hub. Ask your administrator to assign access to you if necessary. Procedure: Log in to your Red Hat Developer Hub UI. Click the Ansible A icon in the Red Hat Developer Hub navigation panel. Navigate to the Overview page. Click Create . Click Create Ansible Git Project . The Available Templates page opens. Click Create Ansible Playbook project . In the Create Ansible Playbook Project page, enter information for your new project in the form. You can see sample values for this form in the Example project. Field Description Source code repository organization name or username The name of your source code repository username or organization name Playbook repository name The name of your new Git repository Playbook description (Optional) A description of the new playbook project Playbook project’s collection namespace The new playbook Git project creates an example collection folder for you. Enter a value for the collection namespace. Playbook project’s collection name The name of the collection Catalog Owner Name The name of the Developer Hub catalog item owner. This is a Red Hat Developer Hub field. Source code repository organization name or username The name of your source code repository username or organization name Playbook repository name The name of your new Git repository Playbook description (Optional) A description of the new playbook project System (Optional) This is a Red Hat Developer Hub field Note Collection namespaces must follow Python module naming conventions. Collections must have short, all lowercase names. You can use underscores in the collection name if it improves readability. For more information, see the Ansible Collection naming conventions documentation . Click Review . 1.6. Viewing your projects To view the projects that you have created in the plug-in, navigate to the Overview page for the Ansible plug-in and click My Items . 1.7. Developing projects 1.7.1. Developing projects on Dev Spaces OpenShift Dev Spaces is not included with your Ansible Automation Platform subscription or the Ansible plug-ins for Red Hat Developer Hub. The plug-ins provide context-aware links to edit your project in Dev Spaces. The Dev Spaces instance provides a default configuration that installs the Ansible VS Code extension and provides the Ansible command line tools. You can activate Ansible Lightspeed in the Ansible VS Code extension. For more information, refer to the Red Hat Ansible Lightspeed with IBM watsonx Code Assistant User Guide . 1.7.2. Executing automation tasks in Dev Spaces The default configuration for Dev Spaces provides access to the Ansible command line tools. To execute an automation task in Dev Spaces from the VSCode user interface, right-click a playbook name in the list of files and select Run Ansible Playbook via ansible-navigator run or Run playbook via ansible-playbook . 1.8. Setting up a controller project to run your playbook project Procedure The Ansible plug-ins provide a link to Ansible Automation Platform. Log in to your Red Hat Developer Hub UI. Click the Ansible A icon in the Red Hat Developer Hub navigation panel. Click Operate to display a link to your Ansible Automation Platform instance. If automation controller was not included in your plug-in installation, a link to the product feature page is displayed. Click Go to Ansible Automation Platform to open your platform instance in a new browser tab. Alternatively, if your platform instance was not configured during the Ansible plug-in installation, navigate to your automation controller instance in a browser and log in. Log in to automation controller. Create a project in Ansible Automation Platform for the GitHub repository where you stored your playbook project. Refer to the Projects chapter of TitleControllerUserGuide . Create a job template that uses a playbook from the project that you created. Refer to the Workflow job templates chapter of TitleControllerUserGuide . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_ansible_plug-ins_for_red_hat_developer_hub/rhdh-using_aap-plugin-rhdh-using"}
{"title": "Chapter 1. Overview", "content": "Chapter 1. Overview 1.1. Introduction Cost-Optimized deployments of SAP S/4HANA systems play an important role in S/4HANA migration scenarios, especially in saving costs related to additional nodes. It is also critical if such systems are to be made highly available, in which case, the constraints need to be correctly configured. 1.2. Audience This document is intended for SAP and Red Hat certified or trained administrators and consultants who already have experience setting up highly available solutions using the Red Hat Enterprise Linux (RHEL) HA add-on or other clustering solutions. Access to both SAP Service Marketplace and Red Hat Customer Portal is required to be able to download software and additional documentation. Red Hat Professional Services is highly recommended to set up the cluster and customize the solution to meet the customer’s data center requirements, which may be different than the solution presented in this document. 1.3. Concepts This document describes how to set up a Cost-Optimized, two-node cluster solution that conforms to the high availability guidelines established by SAP and Red Hat. It is based on Standalone Enqueue Server 2 (ENSA2), now the default installation in SAP S/4HANA 1809 or newer, on top of RHEL 8 for SAP Solutions or above, and highlights a scale-up SAP HANA instance that supports fully automated failover using SAP HANA System Replication. According to SAP, ENSA2 is the successor to Standalone Enqueue Server 1 (ENSA1). It is a component of the SAP lock concept and manages the lock table. This principle ensures the consistency of data in an ABAP system. During a failover with ENSA1, the ASCS instance is required to \"follow\" the Enqueue Replication Server (ERS). That is, the HA software had to start the ASCS instance on the host where the ERS instance is currently running. In contrast to ENSA1, the newer ENSA2 model and Enqueue Replicator 2 no longer have these restrictions. For more information on ENSA2, please refer to SAP OSS Note 2630416 - Support for Standalone Enqueue Server 2 . Additionally, the document will also highlight the SAP HANA Scale-Up instance, with fully automated failover using SAP HANA System Replication, where the SAP HANA promotable clone resources will run on each node as per set constraints. This article does NOT cover preparation of the RHEL system for SAP HANA installation, nor the SAP HANA installation procedure. For fast and error-free preparation of the systems for SAP S/4HANA and SAP HANA, we recommend using RHEL System Roles for SAP . The setup described in this documentation shows a configuration of both the above combined, that is, an ENSA 2 model with SAP HANA Scale-up with System Replication managed by a single 2-node pacemaker cluster. This is considered a Cost-Optimized SAP S/4HANA with an Automated SAP HANA Scale-Up System Replication HA Environment. 1.4. Support Policies Please refer to Support Policies for RHEL High Availability Clusters - Management of SAP S/4HANA and Support Policies for RHEL High Availability Clusters - Management of SAP HANA in a Cluster for more details. This solution is supported subject to fulfilling the above policies. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_a_cost-optimized_sap_s4hana_ha_cluster_hana_system_replication_ensa2_using_the_rhel_ha_add-on/asmb_cco_overview_configuring-cost-optimized-sap-v9"}
{"title": "Web console", "content": "Web console Red Hat OpenShift Service on AWS 4 Getting started with web console in Red Hat OpenShift Service on AWS Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html-single/web_console/index"}
{"title": "Schedule and quota APIs", "content": "Schedule and quota APIs  Reference guide for schedule and quota APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/schedule_and_quota_apis/index"}
{"title": "Registry", "content": "Registry  Configuring registries for OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/registry/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_content_navigator/legal-notice"}
{"title": "Chapter 3. Example: Automate Red Hat Enterprise Linux firewall configuration", "content": "Chapter 3. Example: Automate Red Hat Enterprise Linux firewall configuration This example demonstrates how the Ansible plug-ins can help Ansible users of all skill levels create quality Ansible content. As an infrastructure engineer new to Ansible, you have been tasked to create a playbook to configure a Red Hat Enterprise Linux (RHEL) host firewall. The following procedures show you how to use the Ansible plug-ins and Dev Spaces to develop a playbook. 3.1. Learning more about playbooks The first step is to learn more about Ansible playbooks using the available learning paths. Click the Ansible A icon in the Red Hat Developer Hub navigation panel. Click Learn and select the Getting Started with Ansible Playbooks learning path. This redirects you to the Red Hat Developer website. If you are prompted to log in, create a Red Hat Developer account, or enter your details. Complete the learning path. 3.2. Discovering existing Ansible content for RHEL system roles Red Hat recommends that you use trusted automation content that has been tested and approved by Red Hat or your organization. Automation hub is a central repository for discovering, downloading, and managing trusted content collections from Red Hat and its partners. Private automation hub provides an on-premise solution for managing content collections. Click on the Ansible A icon in the Red Hat Developer Hub navigation panel. Click Discover existing collections . Click Go to Automation Hub . If private automation hub has been configured in the Ansible plug-ins, you are redirected to your PrivateHubName instance. If private automation hub has not been configured in the Ansible plug-ins installation configuration, you will be redirected to the Red Hat Hybrid Console (RHCC) automation hub. In this example, you are redirected to the RHCC automation hub. If you are prompted to log in, provide your Red Hat Customer Portal credentials. Filter the collections with the rhel firewall keywords. The search returns the rhel_system_roles collection. The RHEL System Roles collection contains certified Ansible content that you can reuse to configure your firewall. 3.3. Create a new playbook project to configure a firewall Use the Ansible plug-ins to create a new Ansible Playbook project. Click the Ansible A icon in the Red Hat Developer Hub navigation panel. From the Create dropdown menu on the landing page, select Create Ansible Git Project . Click Choose in the Create Ansible Playbook Project software template. Fill in the following information in the Create Ansible Playbook Project page: Field Required Description Example value Source code repository organization name or username Yes The name of your source code repository username or organization name. my_github_username Playbook repository name Yes The name of your new Git repository. rhel_firewall_config Playbook description No A description of the new playbook project. This playbook configures firewalls on Red Hat Enterprise Linux systems Playbook project’s collection namespace Yes The new playbook Git project creates an example collection folder for you. Enter a value for the collection namespace. my_galaxy_username Playbook project’s collection name Yes This is the name of the example collection. rhel_firewall_config Catalog Owner Name Yes The name of the Developer Hub catalog item owner. It is a Red Hat Developer Hub field. my_rhdh_username System No This is a Red Hat Developer Hub field. my_rhdh_linux_system Click Review . Click Create to provision your new playbook project. Click Open in catalog to view your project. 3.4. Creating a new playbook to automate the firewall configuration Create a new playbook and use the RHEL System Role collection to automate your Red Hat Enterprise Linux firewall configuration. In your Dev Spaces instance, click File New File . Enter firewall.yml for the filename and click OK to save it in the root directory. Add the following lines to your firewall.yml file: --- - name: Open HTTPS and SSH on firewall hosts: rhel become: true tasks: - name: Use rhel system roles to allow https and ssh traffic vars: firewall: - service: https state: enabled permanent: true immediate: true zone: public - service: ssh state: enabled permanent: true immediate: true zone: public ansible.builtin.include_role: name: redhat.rhel_system_roles.firewall Note You can use Ansible Lightspeed with IBM watsonx Code Assistant from the Ansible VS Code extension to help you generate playbooks. For more information, refer to the Ansible Lightspeed with IBM watsonx Code Assistant User Guide . 3.5. Editing your firewall playbook project The Ansible plug-ins integrate OpenShift Dev Spaces to edit your Ansible projects. OpenShift Dev Spaces provides on-demand, web-based Integrated Development Environments (IDEs). Ansible Git projects provisioned using the Ansible plug-ins include best practice configurations for OpenShift Dev Spaces. These configurations include installing the Ansible VS Code extension and providing access from the IDE terminal to Ansible development tools, such as Ansible Navigator and Ansible Lint. Note OpenShift Dev Spaces is optional and it is not required to run the Ansible plug-ins. It is a separate Red Hat product and it is not included in the Ansible Automation Platform or Red Hat Developer Hub subscription. This example assumes that OpenShift Dev Spaces has been configured in the Ansible plug-ins installation. Procedure In the catalog item view of your playbook project, click Open Ansible project in OpenShift Dev Spaces . A VS Code instance of OpenShift Dev Spaces opens in a new browser tab. It automatically loads your new Ansible Playbook Git project. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_ansible_plug-ins_for_red_hat_developer_hub/rhdh-example_aap-plugin-rhdh-using"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_sap_solutions/9/html/configuring_sap_hana_scale-up_multitarget_system_replication_for_disaster_recovery/legal-notice"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/installing_ansible_plug-ins_for_red_hat_developer_hub/legal-notice"}
{"title": "Installing on any platform", "content": "Installing on any platform  Installing OpenShift Container Platform on any platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/installing_on_any_platform/index"}
{"title": "Role APIs", "content": "Role APIs  Reference guide for role APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/role_apis/index"}
{"title": "Installing on OpenStack", "content": "Installing on OpenStack  Installing OpenShift Container Platform on OpenStack Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/installing_on_openstack/index"}
{"title": "Postinstallation configuration", "content": "Postinstallation configuration  Day 2 operations for OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/postinstallation_configuration/index"}
{"title": "Windows Container Support for OpenShift", "content": "Windows Container Support for OpenShift  Red Hat OpenShift for Windows Containers Guide Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/windows_container_support_for_openshift/index"}
{"title": "Upgrading", "content": "Upgrading Red Hat OpenShift Service on AWS 4 Understanding upgrading options for Red Hat OpenShift Service on AWS Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html-single/upgrading/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/user_and_group_apis/legal-notice"}
{"title": "Installing on VMware vSphere", "content": "Installing on VMware vSphere  Installing OpenShift Container Platform on vSphere Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/installing_on_vmware_vsphere/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/template_apis/legal-notice"}
{"title": "Images", "content": "Images  Creating and managing images and imagestreams in OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/images/index"}
{"title": "Storage", "content": "Storage  Configuring and managing storage in OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/storage/index"}
{"title": "Installing on IBM Power Virtual Server", "content": "Installing on IBM Power Virtual Server  Installing OpenShift Container Platform on IBM Power Virtual Server Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/installing_on_ibm_power_virtual_server/index"}
{"title": "Red Hat Ansible Lightspeed with IBM watsonx Code Assistant User Guide", "content": "Red Hat Ansible Lightspeed with IBM watsonx Code Assistant User Guide Red Hat Ansible Lightspeed with IBM watsonx Code Assistant 2.x_latest Learn how to use Red Hat Ansible Lightspeed with IBM watsonx Code Assistant. Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant/2.x_latest/html-single/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant_user_guide/index"}
{"title": "API overview", "content": "API overview  Overview content for the OpenShift Container Platform API Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/api_overview/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/role_apis/legal-notice"}
{"title": "Installing on OCI", "content": "Installing on OCI  Installing OpenShift Container Platform on Oracle Cloud Infrastructure Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/installing_on_oci/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/schedule_and_quota_apis/legal-notice"}
{"title": "Chapter 1. Overview of available file systems", "content": "Chapter 1. Overview of available file systems Choosing the file system that is appropriate for your application is an important decision due to the large number of options available and the trade-offs involved. The following sections describe the file systems that Red Hat Enterprise Linux 9 includes by default, and recommendations on the most suitable file system for your application. 1.1. Types of file systems Red Hat Enterprise Linux 9 supports a variety of file systems (FS). Different types of file systems solve different kinds of problems, and their usage is application specific. At the most general level, available file systems can be grouped into the following major types: Table 1.1. Types of file systems and their use cases Type File system Attributes and use cases Disk or local FS XFS XFS is the default file system in RHEL. Red Hat recommends deploying XFS as your local file system unless there are specific reasons to do otherwise: for example, compatibility or corner cases around performance. ext4 ext4 has the benefit of familiarity in Linux, having evolved from the older ext2 and ext3 file systems. In many cases, it rivals XFS on performance. Support limits for ext4 filesystem and file sizes are lower than those on XFS. Network or client-and-server FS NFS Use NFS to share files between multiple systems on the same network. SMB Use SMB for file sharing with Microsoft Windows systems. Shared storage or shared disk FS GFS2 GFS2 provides shared write access to members of a compute cluster. The emphasis is on stability and reliability, with the functional experience of a local file system as possible. SAS Grid, Tibco MQ, IBM Websphere MQ, and Red Hat Active MQ have been deployed successfully on GFS2. Volume-managing FS Stratis Stratis is a volume manager built on a combination of XFS and LVM. The purpose of Stratis is to emulate capabilities offered by volume-managing file systems like Btrfs and ZFS. It is possible to build this stack manually, but Stratis reduces configuration complexity, implements best practices, and consolidates error information. 1.2. Local file systems Local file systems are file systems that run on a single, local server and are directly attached to storage. For example, a local file system is the only choice for internal SATA or SAS disks, and is used when your server has internal hardware RAID controllers with local drives. Local file systems are also the most common file systems used on SAN attached storage when the device exported on the SAN is not shared. All local file systems are POSIX-compliant and are fully compatible with all supported Red Hat Enterprise Linux releases. POSIX-compliant file systems provide support for a well-defined set of system calls, such as read() , write() , and seek() . When considering a file system choice, choose a file system based on how large the file system needs to be, what unique features it must have, and how it performs under your workload. Available local file systems XFS ext4 1.3. The XFS file system XFS is a highly scalable, high-performance, robust, and mature 64-bit journaling file system that supports very large files and file systems on a single host. It is the default file system in Red Hat Enterprise Linux 9. XFS was originally developed in the early 1990s by SGI and has a long history of running on extremely large servers and storage arrays. The features of XFS include: Reliability Metadata journaling, which ensures file system integrity after a system crash by keeping a record of file system operations that can be replayed when the system is restarted and the file system remounted Extensive run-time metadata consistency checking Scalable and fast repair utilities Quota journaling. This avoids the need for lengthy quota consistency checks after a crash. Scalability and performance Supported file system size up to 1024 TiB Ability to support a large number of concurrent operations B-tree indexing for scalability of free space management Sophisticated metadata read-ahead algorithms Optimizations for streaming video workloads Allocation schemes Extent-based allocation Stripe-aware allocation policies Delayed allocation Space pre-allocation Dynamically allocated inodes Other features Reflink-based file copies Tightly integrated backup and restore utilities Online defragmentation Online file system growing Comprehensive diagnostics capabilities Extended attributes ( xattr ). This allows the system to associate several additional name/value pairs per file. Project or directory quotas. This allows quota restrictions over a directory tree. Subsecond timestamps Performance characteristics XFS has a high performance on large systems with enterprise workloads. A large system is one with a relatively high number of CPUs, multiple HBAs, and connections to external disk arrays. XFS also performs well on smaller systems that have a multi-threaded, parallel I/O workload. XFS has a relatively low performance for single threaded, metadata-intensive workloads: for example, a workload that creates or deletes large numbers of small files in a single thread. 1.4. The ext4 file system The ext4 file system is the fourth generation of the ext file system family. It was the default file system in Red Hat Enterprise Linux 6. The ext4 driver can read and write to ext2 and ext3 file systems, but the ext4 file system format is not compatible with ext2 and ext3 drivers. ext4 adds several new and improved features, such as: Supported file system size up to 50 TiB Extent-based metadata Delayed allocation Journal checksumming Large storage support The extent-based metadata and the delayed allocation features provide a more compact and efficient way to track utilized space in a file system. These features improve file system performance and reduce the space consumed by metadata. Delayed allocation allows the file system to postpone selection of the permanent location for newly written user data until the data is flushed to disk. This enables higher performance since it can allow for larger, more contiguous allocations, allowing the file system to make decisions with much better information. File system repair time using the fsck utility in ext4 is much faster than in ext2 and ext3. Some file system repairs have demonstrated up to a six-fold increase in performance. 1.5. Comparison of XFS and ext4 XFS is the default file system in RHEL. This section compares the usage and features of XFS and ext4. Metadata error behavior In ext4, you can configure the behavior when the file system encounters metadata errors. The default behavior is to simply continue the operation. When XFS encounters an unrecoverable metadata error, it shuts down the file system and returns the EFSCORRUPTED error. Quotas In ext4, you can enable quotas when creating the file system or later on an existing file system. You can then configure the quota enforcement using a mount option. XFS quotas are not a remountable option. You must activate quotas on the initial mount. Running the quotacheck command on an XFS file system has no effect. The first time you turn on quota accounting, XFS checks quotas automatically. File system resize XFS has no utility to reduce the size of a file system. You can only increase the size of an XFS file system. In comparison, ext4 supports both extending and reducing the size of a file system. Inode numbers The ext4 file system does not support more than 2 32 inodes. XFS supports dynamic inode allocation. The amount of space inodes can consume on an XFS filesystem is calculated as a percentage of the total filesystem space. To prevent the system from running out of inodes, an administrator can tune this percentage after the filesystem has been created, given there is free space left on the file system. Certain applications cannot properly handle inode numbers larger than 2 32 on an XFS file system. These applications might cause the failure of 32-bit stat calls with the EOVERFLOW return value. Inode number exceed 2 32 under the following conditions: The file system is larger than 1 TiB with 256-byte inodes. The file system is larger than 2 TiB with 512-byte inodes. If your application fails with large inode numbers, mount the XFS file system with the -o inode32 option to enforce inode numbers below 2 32 . Note that using inode32 does not affect inodes that are already allocated with 64-bit numbers. Important Do not use the inode32 option unless a specific environment requires it. The inode32 option changes allocation behavior. As a consequence, the ENOSPC error might occur if no space is available to allocate inodes in the lower disk blocks. 1.6. Choosing a local file system To choose a file system that meets your application requirements, you must understand the target system on which you will deploy the file system. In general, use XFS unless you have a specific use case for ext4. XFS For large-scale deployments, use XFS, particularly when handling large files (hundreds of megabytes) and high I/O concurrency. XFS performs optimally in environments with high bandwidth (greater than 200MB/s) and more than 1000 IOPS. However, it consumes more CPU resources for metadata operations compared to ext4 and does not support file system shrinking. ext4 For smaller systems or environments with limited I/O bandwidth, ext4 might be a better fit. It performs better in single-threaded, lower I/O workloads and environments with lower throughput requirements. ext4 also supports offline shrinking, which can be beneficial if resizing the file system is a requirement. Benchmark your application’s performance on your target server and storage system to ensure the selected file system meets your performance and scalability requirements. Table 1.2. Summary of local file system recommendations Scenario Recommended file system No special use case XFS Large server XFS Large storage devices XFS Large files XFS Multi-threaded I/O XFS Single-threaded I/O ext4 Limited I/O capability (under 1000 IOPS) ext4 Limited bandwidth (under 200MB/s) ext4 CPU-bound workload ext4 Support for offline shrinking ext4 1.7. Network file systems Network file systems, also referred to as client/server file systems, enable client systems to access files that are stored on a shared server. This makes it possible for multiple users on multiple systems to share files and storage resources. Such file systems are built from one or more servers that export a set of file systems to one or more clients. The client nodes do not have access to the underlying block storage, but rather interact with the storage using a protocol that allows for better access control. Available network file systems The most common client/server file system for RHEL customers is the NFS file system. RHEL provides both an NFS server component to export a local file system over the network and an NFS client to import these file systems. RHEL also includes a CIFS client that supports the popular Microsoft SMB file servers for Windows interoperability. The userspace Samba server provides Windows clients with a Microsoft SMB service from a RHEL server. 1.8. Shared storage file systems Shared storage file systems, sometimes referred to as cluster file systems, give each server in the cluster direct access to a shared block device over a local storage area network (SAN). Comparison with network file systems Like client/server file systems, shared storage file systems work on a set of servers that are all members of a cluster. Unlike NFS, however, no single server provides access to data or metadata to other members: each member of the cluster has direct access to the same storage device (the shared storage ), and all cluster member nodes access the same set of files. Concurrency Cache coherency is key in a clustered file system to ensure data consistency and integrity. There must be a single version of all files in a cluster visible to all nodes within a cluster. The file system must prevent members of the cluster from updating the same storage block at the same time and causing data corruption. In order to do that, shared storage file systems use a cluster wide-locking mechanism to arbitrate access to the storage as a concurrency control mechanism. For example, before creating a new file or writing to a file that is opened on multiple servers, the file system component on the server must obtain the correct lock. The requirement of cluster file systems is to provide a highly available service like an Apache web server. Any member of the cluster will see a fully coherent view of the data stored in their shared disk file system, and all updates will be arbitrated correctly by the locking mechanisms. Performance characteristics Shared disk file systems do not always perform as well as local file systems running on the same system due to the computational cost of the locking overhead. Shared disk file systems perform well with workloads where each node writes almost exclusively to a particular set of files that are not shared with other nodes or where a set of files is shared in an almost exclusively read-only manner across a set of nodes. This results in a minimum of cross-node cache invalidation and can maximize performance. Setting up a shared disk file system is complex, and tuning an application to perform well on a shared disk file system can be challenging. Available shared storage file systems Red Hat Enterprise Linux provides the GFS2 file system. GFS2 comes tightly integrated with the Red Hat Enterprise Linux High Availability Add-On and the Resilient Storage Add-On. Red Hat Enterprise Linux supports GFS2 on clusters that range in size from 2 to 16 nodes. 1.9. Choosing between network and shared storage file systems When choosing between network and shared storage file systems, consider the following points: NFS-based network file systems are an extremely common and popular choice for environments that provide NFS servers. Network file systems can be deployed using very high-performance networking technologies like Infiniband or 10 Gigabit Ethernet. This means that you should not turn to shared storage file systems just to get raw bandwidth to your storage. If the speed of access is of prime importance, then use NFS to export a local file system like XFS. Shared storage file systems are not easy to set up or to maintain, so you should deploy them only when you cannot provide your required availability with either local or network file systems. A shared storage file system in a clustered environment helps reduce downtime by eliminating the steps needed for unmounting and mounting that need to be done during a typical fail-over scenario involving the relocation of a high-availability service. Red Hat recommends that you use network file systems unless you have a specific use case for shared storage file systems. Use shared storage file systems primarily for deployments that need to provide high-availability services with minimum downtime and have stringent service-level requirements. 1.10. Volume-managing file systems Volume-managing file systems integrate the entire storage stack for the purposes of simplicity and in-stack optimization. Available volume-managing file systems Red Hat Enterprise Linux 9 provides the Stratis volume manager. Stratis uses XFS for the file system layer and integrates it with LVM, Device Mapper, and other components. Stratis was first released in Red Hat Enterprise Linux 8.0. It is conceived to fill the gap created when Red Hat deprecated Btrfs. Stratis 1.0 is an intuitive, command line-based volume manager that can perform significant storage management operations while hiding the complexity from the user: Volume management Pool creation Thin storage pools Snapshots Automated read cache Stratis offers powerful features, but currently lacks certain capabilities of other offerings that it might be compared to, such as Btrfs or ZFS. Most notably, it does not support CRCs with self healing. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_file_systems/overview-of-available-file-systems_managing-file-systems"}
{"title": "Scalability and performance", "content": "Scalability and performance  Scaling your OpenShift Container Platform cluster and tuning performance in production environments Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/scalability_and_performance/index"}
{"title": "Installing on IBM Power", "content": "Installing on IBM Power  Installing OpenShift Container Platform on IBM Power Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/installing_on_ibm_power/index"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1"}
{"title": "Configuring and managing logical volumes", "content": "Configuring and managing logical volumes Red Hat Enterprise Linux 9 Configuring and managing LVM Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_and_managing_logical_volumes/index"}
{"title": "Common object reference", "content": "Common object reference  Reference guide common API objects Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/common_object_reference/index"}
{"title": "Chapter 11. Projects", "content": "Chapter 11. Projects A Project is a logical collection of Ansible playbooks, represented in automation controller. You can manage playbooks and playbook directories different ways: By placing them manually under the Project Base Path on your automation controller server. By placing your playbooks into a source code management (SCM) system supported by the automation controller. These include Git, Subversion, Mercurial and Red Hat Insights. For more information on creating a Red Hat Insights project, see Setting up Red Hat Insights for Red Hat Ansible Automation Platform Remediations . Note The Project Base Path is /var/lib/awx/projects . However, this can be modified by the system administrator. It is configured in /etc/tower/conf.d/custom.py . Use caution when editing this file, as incorrect settings can disable your installation. The Projects page displays the list of the projects that are currently available. A Demo Project is provided that you can work with initially. The default view is collapsed ( Compact ) with project name and its status, but you can use the next to each entry to expand for more information. For each project listed, you can get the latest SCM revision , edit the project, or copy the project attributes, using the icons next to each project. Projects can be updated while a related job is running. In cases where you have a large project (around 10 GB), disk space on /tmp may be an issue. Status indicates the state of the project and may be one of the following (note that you can also filter your view by specific status types): Pending - The source control update has been created, but not queued or started yet. Any job (not just source control updates) stays in pending until it is ready to be run by the system. Possible reasons for it not being ready are: It has dependencies that are currently running so it has to wait until they are done. There is not enough capacity to run in the locations it is configured to. Waiting - The source control update is in the queue waiting to be executed. Running - The source control update is currently in progress. Successful - The last source control update for this project succeeded. Failed - The last source control update for this project failed. Error - The last source control update job failed to run at all. Canceled - The last source control update for the project was canceled. Never updated - The project is configured for source control, but has never been updated. OK - The project is not configured for source control, and is correctly in place. Missing - Projects are absent from the project base path of /var/lib/awx/projects . This is applicable for manual or source control managed projects. Note Projects of credential type Manual cannot update or schedule source control-based actions without being reconfigured as an SCM type credential. 11.1. Adding a new project You can create a logical collection of playbooks, called projects in automation controller. Procedure From the navigation panel, select Automation Execution Projects . On the Projects page, click Create project to launch the Create Project window. Enter the appropriate details into the following required fields: Name (required) Optional: Description Organization (required): A project must have at least one organization. Select one organization now to create the project. When the project is created you can add additional organizations. Optional: Execution environment : Enter the name of the execution environment or search from a list of existing ones to run this project. For more information, see Creating and using execution environments . Source control type (required): Select an SCM type associated with this project from the menu. Options in the following sections become available depending on the type chosen. For more information, see Managing playbooks manually or Managing playbooks using source control . Optional: Content signature validation credential : Use this field to enable content verification. Specify the GPG key to use for validating content signature during project synchronization. If the content has been tampered with, the job will not run. For more information, see Project signing and verification . Click Create project . Additional resources The following describe the ways projects are sourced: Managing playbooks manually Managing playbooks using source control SCM Types - Configuring playbooks to use Git and Subversion SCM Type - Configuring playbooks to use Red Hat Insights SCM Type - Configuring playbooks to use a remote archive 11.1.1. Managing playbooks manually Procedure Create one or more directories to store playbooks under the Project Base Path, for example, /var/lib/awx/projects/ . Create or copy playbook files into the playbook directory. Ensure that the playbook directory and files are owned by the same UNIX user and group that the service runs as. Ensure that the permissions are appropriate for the playbook directories and files. Troubleshooting If you have not added any Ansible Playbook directories to the base project path an error message is displayed. Choose one of the following options to troubleshoot this error: Create the appropriate playbook directories and check out playbooks from your (Source code management) SCM. Copy playbooks into the appropriate playbook directories. 11.1.2. Managing playbooks using source control Choose one of the following options when managing playbooks using source control: SCM Types - Configuring playbooks to use Git and Subversion SCM Type - Configuring playbooks to use Red Hat Insights SCM Type - Configuring playbooks to use a remote archive 11.1.2.1. SCM Types - Configuring playbooks to use Git and Subversion Procedure From the navigation panel, select Automation Execution Projects . Click the project name you want to use. In the project Details tab, click Edit project . Select the appropriate option (Git or Subversion) from the Source control type menu. Enter the appropriate details into the following fields: Source control URL - See an example in the tooltip . Optional: Source control branch/tag/commit : Enter the SCM branch, tags, commit hashes, arbitrary refs, or revision number (if applicable) from the source control (Git or Subversion) to checkout. Some commit hashes and references might not be available unless you also give a custom refspec in the next field. If left blank, the default is HEAD which is the last checked out Branch, Tag, or Commit for this project. Source control refspec - This field is an option specific to git source control and only advanced users familiar and comfortable with git should specify which references to download from the remote repository. For more information, see Job branch overriding . Source control credential - If authentication is required, select the appropriate source control credential. Optional: Options - select the launch behavior, if applicable: Clean - Removes any local modifications before performing an update. Delete - Deletes the local repository in its entirety before performing an update. Depending on the size of the repository this can significantly increase the amount of time required to complete an update. Track submodules - Tracks the latest commit. There is more information in the tooltip . Update revision on launch - Updates the revision of the project to the current revision in the remote source control, and caching the roles directory from Ansible Galaxy support or Collections support . Automation controller ensures that the local revision matches and that the roles and collections are up-to-date with the last update. In addition, to avoid job overflows if jobs are spawned faster than the project can synchronize, selecting this enables you to configure a Cache Timeout to cache previous project synchronizations for a given number of seconds. Allow branch override - Enables a job template or an inventory source that uses this project to start with a specified SCM branch or revision other than that of the project. For more information, see Job branch overriding . Click Save project . 11.1.2.2. SCM Type - Configuring playbooks to use Red Hat Insights Procedure From the navigation panel, select Automation Execution Projects . Click the project name you want to use. In the project Details tab, click Edit project . Select Red Hat Insights from the Source Control Type menu. In the Insights credential field, select the appropriate credential for use with Insights, as Red Hat Insights requires a credential for authentication. Optional: In the Options field, select the launch behavior, if applicable: Clean - Removes any local modifications before performing an update. Delete - Deletes the local repository in its entirety before performing an update. Depending on the size of the repository this can significantly increase the amount of time required to complete an update. Update revision on launch - Updates the revision of the project to the current revision in the remote source control, and caches the roles directory from Ansible Galaxy support or Collections support . Automation controller ensures that the local revision matches, and that the roles and collections are up-to-date. If jobs are spawned faster than the project can synchronize, selecting this enables you to configure a Cache Timeout to cache previous project synchronizations for a certain number of seconds, to avoid job overflow. Click Save project . 11.1.2.3. SCM Type - Configuring playbooks to use a remote archive Playbooks that use a remote archive enable projects to be based on a build process that produces a versioned artifact, or release, containing all the requirements for that project in a single archive. Procedure From the navigation panel, select Automation Execution Projects . Click the project name you want to use. In the project Details tab, click Edit project . Select Remote Archive from the Source control type menu. Enter the appropriate details into the following fields: Source control URL - requires a URL to a remote archive, such as a GitHub Release or a build artifact stored in Artifactory and unpacks it into the project path for use. Source control credential - If authentication is required, select the appropriate source control credential. Optional: In the Options field, select the launch behavior, if applicable: Clean - Removes any local modifications before performing an update. Delete - Deletes the local repository in its entirety before performing an update. Depending on the size of the repository this can significantly increase the amount of time required to complete an update. Update revision on launch - Not recommended. This option updates the revision of the project to the current revision in the remote source control, and caches the roles directory from Ansible Galaxy support or Collections support . Allow branch override - Not recommended. This option enables a job template that uses this project to launch with a specified SCM branch or revision other than that of the project’s. Note Since this source control type is intended to support the concept of unchanging artifacts, it is advisable to disable Galaxy integration (for roles, at a minimum). Click Save project . 11.2. Updating projects from source control Procedure From the navigation panel, select Automation Execution Projects . Click the sync icon next to the project that you want to update. Note Immediately after adding a project setup to use source control, a sync starts that fetches the project details from the configured source control. Click the project’s status under the Status column for further information about the update process. This brings you to the Output tab of the Jobs section. 11.3. Work with permissions The set of permissions assigned to a project (role-based access controls) that provide the ability to read, change, and administer projects, inventories, job templates, and other elements are privileges. To access the project permissions, select the Access tab of the Projects page. This screen displays a list of users that currently have permissions to this project. You can sort and search this list by Username , First Name , or Last Name . 11.3.1. Adding project permissions Manage the permissions that users and teams have to access a project. Procedure From the navigation panel, select Automation Execution Projects . Select the project that you want to update and click the User Access or Team Access tab. Click Add roles . Select a user or team to add and click Next . Select one or more users or teams from the list by clicking the checkbox next to the name to add them as members. Click Next . Select the roles you want the selected users or teams to have. Different resources have different options available. Click Finish to apply the roles to the selected users or teams and to add them as members. The updated roles assigned for each user and team are displayed. 11.3.2. Removing permissions from a project Remove roles for a particular user. Procedure From the navigation panel, select Automation Execution Projects . Select the project that you want to update and click the User Access or Team Access tab. Click the icon next to the user role in the Roles column. Click Delete in the confirmation window to confirm the disassociation. 11.4. Ansible Galaxy support At the end of a project update, automation controller searches for the requirements.yml file in the roles directory, located at <project-top-level-directory>/roles/requirements.yml . If this file is found, the following command automatically runs: ansible-galaxy role install -r roles/requirements.yml -p <project-specific cache location>/requirements_roles -vvv This file enables you to reference Ansible Galaxy roles or roles within other repositories which can be checked out in conjunction with your own project. The addition of Ansible Galaxy access eliminates the need to create git submodules to achieve this result. Given that SCM projects, along with roles or collections, are pulled into and executed from a private job environment, a <private job directory> specific to the project within /tmp is created by default. The cache directory is a subdirectory inside the global projects folder. You can copy the content from the cache location to <job private directory>/requirements_roles . By default, automation controller has a system-wide setting that enables you to dynamically download roles from the roles/requirements.yml file for SCM projects. You can turn off this setting in the Job Settings screen from the navigation panel Settings Job , by switching the Enable Role Download toggle to Off . Whenever a project synchronization runs, automation controller determines if the project source and any roles from Galaxy or Collections are out of date with the project. Project updates download the roles inside the update. If jobs need to pick up a change made to an upstream role, updating the project ensures that this happens. A change to the role means that a new commit was pushed to the provision-role source control. To make this change take effect in a job, you do not have to push a new commit to the playbooks repository. You must update the project, which downloads roles to a local cache. For instance, say you have two git repositories in source control. The first one is playbooks and the project in automation controller points to this URL. The second one is provision-role and it is referenced by the roles/requirements.yml file inside of the playbooks git repository. Jobs download the most recent roles before every job run. Roles and collections are locally cached for performance reasons. You must select Update Revision on Launch in the project Options to ensure that the upstream role is re-downloaded before each job run: The update happens much earlier in the process than the sync, so this identifies errors and details faster and in a more logical location. For more information and examples on the syntax of the requirements.yml file, see the role requirements section in the Ansible documentation. If there are any directories that must be specifically exposed, you can specify those in the Job Settings screen from the navigation panel Settings Job , in Paths to Expose to isolated Jobs . You can also update the following entry in the settings file: AWX_ISOLATION_SHOW_PATHS = ['/list/of/', '/paths'] Note If your playbooks need to use keys or settings defined in AWX_ISOLATION_SHOW_PATHS , you must add AWX_ISOLATION_SHOW_PATHS to /var/lib/awx/.ssh . If you made changes in the settings file, be sure to restart services with the automation-controller-service restart command after your changes have been saved. In the UI, you can configure these settings in the Jobs Settings window. 11.5. Collections support Automation controller supports project-specific Ansible collections in job runs. If you specify a collections requirements file in the SCM at collections/requirements.yml , automation controller installs collections in that file in the implicit project synchronization before a job run. Automation controller has a system-wide setting that enables collections to be dynamically downloaded from the collections/requirements.yml file for SCM projects. You can turn off this setting in the Job Settings screen from the navigation panel Settings Job , by switching the Enable Collection(s) Download toggle button to Off . Roles and collections are locally cached for performance reasons, and you select Update Revision on Launch in the project Options to ensure this: Note If you also have collections installed in your execution environment, the collections specified in the project’s requirements.yml file will take precedence when running a job. This precedence applies regardless of the version of the collection. For example, if the collection specified in requirements.yml is older than the collection within the execution environment, the collection specified in requirements.yml is used. 11.5.1. Using collections with automation hub Before automation controller can use automation hub as the default source for collections content, you must create an API token in the automation hub UI. You then specify this token in automation controller. Use the following procedure to connect to private automation hub or automation hub, the only difference is which URL you specify. Procedure Go to https://console.redhat.com/ansible/automation-hub/token . Click Load token . Click the copy icon to copy the API token to the clipboard. Create a credential by choosing one of the following options: To use automation hub, create an automation hub credential by using the copied token and pointing to the URLs shown in the Server URL and SSO URL fields of the token page: Galaxy Server URL = https://console.redhat.com/ansible/automation-hub/token To use private automation hub, create an automation hub credential using a token retrieved from the Repo Management dashboard of your private automation hub and pointing to the published repository URL as shown: You can create different repositories with different namespaces or collections in them. For each repository in automation hub you must create a different credential. Copy the Ansible CLI URL from the UI in the format of /https://$<hub_url>/api/galaxy/content/<repo you want to pull from> into the Galaxy Server URL field of Create Credential : For UI specific instructions, see Red Hat Certified, validated, and Ansible Galaxy content in automation hub . Go to the organization for which you want to synchronize content from and add the new credential to the organization. This enables you to associate each organization with the credential, or repository, that you want to use content from. Example You have two repositories: Prod : Namespace 1 and Namespace 2 , each with collection A and B so: namespace1.collectionA:v2.0.0 and namespace2.collectionB:v2.0.0 Stage : Namespace 1 with only collection A so: namespace1.collectionA:v1.5.0 on , you have a repository URL for Prod and Stage . You can create a credential for each one. Then you can assign different levels of access to different organizations. For example, you can create a Developers organization that has access to both repository, while an Operations organization just has access to the Prod repository only. For UI specific instructions, see Configuring user access for container repositories in private automation hub . If automation hub has self-signed certificates, use the toggle to enable the setting Ignore Ansible Galaxy SSL Certificate Verification in Job Settings . For automation hub, which uses a signed certificate, use the toggle to disable it instead. This is a global setting: Create a project, where the source repository specifies the necessary collections in a requirements file located in the collections/requirements.yml file. For information about the syntax to use, see Using Ansible collections in the Ansible documentation. In the Projects list view, click the sync icon to update this project. Automation controller fetches the Galaxy collections from the collections/requirements.yml file and reports it as changed. The collections are installed for any job template using this project. Note If updates are required from Galaxy or Collections, a sync is performed that downloads the required roles, consuming that much more space in your /tmp file. In cases where you have a large project (around 10 GB), disk space on /tmp may be an issue. Additional resources For more information about collections, see Using Ansible Collections . For more information about how Red Hat publishes one of these official collections, which can be used to automate your install directly, see the AWX Ansible Collection documentation. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_automation_execution/controller-projects"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/security_apis/legal-notice"}
{"title": "Chapter 8. Workflow job templates", "content": "Chapter 8. Workflow job templates You can create both Job templates and Workflow job templates from Automation Execution Templates . For Job templates, see Job templates . A workflow job template links together a sequence of disparate resources that tracks the full set of jobs that were part of the release process as a single unit. These resources include the following: Job templates Workflow job templates Project syncs Inventory source syncs The Templates page shows the workflow and job templates that are currently available. The default view is collapsed (Compact), showing the template name, template type, and the statuses of the jobs that have run by using that template. You can click the arrow next to each entry to expand and view more information. This list is sorted alphabetically by name, but you can sort by other criteria, or search by various fields and attributes of a template. From this screen you can launch , edit , and copy a workflow job template. Only workflow templates have the workflow visualizer icon as a shortcut for accessing the workflow editor. Note Workflow templates can be used as building blocks for another workflow template. You can enable Prompt on Launch by setting up several settings in a workflow template, which you can edit at the workflow job template level. These do not affect the values assigned at the individual workflow template level. For further instructions, see the Workflow visualizer section. 8.1. Creating a workflow job template To create a new workflow job template, complete the following steps: Important If you set a limit to a workflow template, it is not passed down to the job template unless you check Prompt on launch for the limit. This can lead to playbook failures if the limit is mandatory for the playbook that you are running. Procedure From the navigation panel, select Automation Execution Templates . On the Templates list view, select Create workflow job template from the Create template list. Enter the appropriate details in the following fields: Note If a field has the Prompt on launch checkbox selected, either launching the workflow template, or using the workflow template within another workflow template, you are prompted for the value for that field. Most prompted values override any values set in the job template. Exceptions are noted in the following table. Field Options Prompt on Launch Name Enter a name for the job. N/A Description Enter an arbitrary description as appropriate (optional). N/A Organization Choose the organization to use with this template from the organizations available to the logged in user. N/A Inventory Optionally, select the inventory to use with this template from the inventories available to the logged in user. Yes Limit A host pattern to further constrain the list of hosts managed or affected by the playbook. You can separate many patterns by colons (:). As with core Ansible: a:b means \"in group a or b\" a:b:&c means \"in a or b but must be in c\" a:!b means \"in a, and definitely not in b\" For more information see, Patterns: targeting hosts and groups in the Ansible documentation. Yes If selected, even if a default value is supplied, you are prompted upon launch to select a limit. Source control branch Select a branch for the workflow. This branch is applied to all workflow job template nodes that prompt for a branch. Yes Labels Optionally, supply labels that describe this workflow job template, such as dev or test . Use labels to group and filter workflow job templates and completed jobs in the display. Labels are created when they are added to the workflow template. Labels are associated to a single Organization using the Project that is provided in the workflow template. Members of the Organization can create labels on a workflow template if they have edit permissions (such as the admin role). Once you save the job template, the labels appear in the workflow job template Details view. Labels are only applied to the workflow templates not the job template nodes that are used in the workflow. Select beside a label to remove it. When a label is removed, it is no longer associated with that particular Job or Job Template, but it remains associated with any other jobs that reference it. Yes If selected, even if a default value is supplied, you are prompted when launching to supply additional labels, if needed. - You cannot delete existing labels, selecting only removes the newly added labels, not existing default labels. Job tags Type and select the Create drop-down to specify which parts of the playbook should run. For more information and examples see Tags in the Ansible documentation. Yes Skip tags Type and select the Create drop-down to specify certain tasks or parts of the playbook to skip. For more information and examples see Tags in the Ansible documentation. Yes Extra variables Pass extra command line variables to the playbook. This is the \"-e\" or \"-extra-vars\" command line parameter for ansible-playbook that is documented in the Ansible documentation at Controlling how Ansible behaves: precedence rules . - Give key or value pairs by using either YAML or JSON. These variables have a maximum value of precedence and overrides other variables specified elsewhere. The following is an example value: git_branch: production release_version: 1.5 Yes If you want to be able to specify extra_vars on a schedule, you must select Prompt on launch for Extra variables on the workflow job template, or enable a survey on the job template. Those answered survey questions become extra_vars . For more information about extra variables, see Extra Variables . Specify the following Options for launching this template, if necessary: Check Enable webhook to turn on the ability to interface with a predefined SCM system web service that is used to launch a workflow job template. GitHub and GitLab are the supported SCM systems. If you enable webhooks, other fields display, prompting for additional information: Webhook service : Select which service to listen for webhooks from. Webhook URL : Automatically populated with the URL for the webhook service to POST requests to. Webhook key : Generated shared secret to be used by the webhook service to sign payloads sent to automation controller. You must configure this in the settings on the webhook service so that webhooks from this service are accepted in automation controller. For additional information about setting up webhooks, see Working with Webhooks . Check Enable concurrent jobs to allow simultaneous runs of this workflow. For more information, see Automation controller capacity determination and job impact . When you have completed configuring the workflow template, click Create workflow job template . Saving the template exits the workflow template page and the workflow visualizer opens where you can build a workflow. For more information, see the Workflow visualizer section. Otherwise, select one of these methods: Close the workflow visualizer to return to the Details tab of the newly saved template. There you can complete the following tasks: Review, edit, add permissions, notifications, schedules, and surveys View completed jobs Build a workflow template Click Launch template to start the workflow. Note Save the template before launching, or Launch template remains disabled. The Notifications tab is only present after you save the template. 8.2. Work with permissions Click the Team Access or User Access tab to review, grand, edit, and remove associated permissions for users along with team members. Click Add roles to create new permissions for this workflow template by following the prompts to assign them. 8.3. Work with notifications For information on working with notifications in workflow job templates, see Work with notifications . 8.4. View completed workflow jobs The Jobs tab provides the list of job templates that have run. Click the expand icon next to each job to view the details of each job. From this view, you can click the job ID, name of the workflow job and see its graphical representation. The following example shows the job details of a workflow job: The nodes are marked with labels to help you identify them. For more information, see the legend in the Workflow visualizer section. 8.5. Scheduling a workflow job template Select the Schedules tab to access the schedules for a particular workflow job template.. For more information about scheduling a workflow job template run, see the Scheduling job templates section. If a workflow job template used in a nested workflow has a survey, or the Prompt on launch is selected for the inventory option, the PROMPT option displays next to the SAVE and CANCEL options on the schedule form. Click PROMPT to show an optional INVENTORY step where you can give or remove an inventory or skip this step without any changes. 8.6. Surveys in workflow job templates Workflows containing job types of Run or Check provide a way to set up surveys in the workflow job template creation or editing screens. For more information on job surveys, including how to create a survey and optional survey questions in workflow job templates, see the Surveys in job templates section. 8.7. Workflow visualizer The Workflow Visualizer provides a graphical way of linking together job templates, workflow templates, project syncs, and inventory syncs to build a workflow template. Before you build a workflow template, see the Workflows in automation controller section for considerations associated with various scenarios on parent, child, and sibling nodes. 8.7.1. Building a workflow You can set up any combination of two or more of the following node types to build a workflow: Template (Job Template or Workflow Job Template) Project Sync Inventory Sync Approval Each node is represented by a rectangle while the relationships and their associated edge types are represented by a line (or link) that connects them. Procedure To launch the workflow visualizer, use one of these methods: From the navigation panel, select Automation Execution Templates . Select a workflow template and click View workflow visualizer . From the Templates list view, click the icon next to a workflow job template. Click Add step to display a list of nodes to add to your workflow. From the Node type list, select the type of node that you want to add. If you select an Approval node, see Approval nodes for more information. Selecting a node provides the available valid options associated with it. Note If you select a job template that does not have a default inventory when populating a workflow graph, the inventory of the parent workflow is used. Though a credential is not required in a job template, you cannot select a job template for your workflow if it has a credential that requires a password, unless the credential is replaced by a prompted credential. When you select a node type, the workflow begins to build, and you must specify the type of action to be taken for the selected node. This action is also referred to as edge type. If the node is a root node, the edge type defaults to Always and is non-editable. For subsequent nodes, you can select one of the following scenarios (edge type) to apply to each: Always run : Continue to execute regardless of success or failure. Run on success : After successful completion, execute the next template. Run on fail : After failure, execute a different template. Select the behavior of the node if it is a convergent node from the Convergence field: Any is the default behavior, allowing any of the nodes to complete as specified, before triggering the next converging node. If the status of one parent meets one of those run conditions, an any child node will run. An any node requires all nodes to complete, but only one node must complete with the expected outcome. Choose All to ensure that all nodes complete as specified, before converging and triggering the next node. The purpose of all * nodes is to make sure that every parent meets its expected outcome to run the child node. The workflow checks to make sure every parent behaves as expected to run the child node. Otherwise, it will not run the child node. If selected, the node is labeled as ALL in the graphical view: Note If a node is a root node, or a node that does not have any nodes converging into it, setting the Convergence rule does not apply, as its behavior is dictated by the action that triggers it. If a job template used in the workflow has Prompt on launch selected for any of its parameters, a PROMPT option appears, enabling you to change those values at the node level. Use the wizard to change the values in each of the tabs and click Confirm in the Preview tab. If a workflow template used in the workflow has Prompt on launch selected for the inventory option, use the wizard to supply the inventory at the prompt. If the parent workflow has its own inventory, it overrides any inventory that is supplied here. Note For workflow job templates with required fields that prompt details, but do not have a default, you must give those values when creating a node before the SELECT option is enabled. The following two cases disable the SELECT option until a value is provided by the PROMPT option: When you select the Prompt on launch checkbox in a workflow job template, but do not give a default. When you create a survey question that is required but do not give a default answer. However, this is not the case with credentials. Credentials that require a password on launch are not permitted when creating a workflow node, because everything required to launch the node must be provided when the node is created. If you are prompted for credentials in a workflow job template, it is not possible to select a credential that requires a password in automation controller. You must also click SELECT when the prompt wizard closes, to apply the changes at that node. Otherwise, any changes you make revert back to the values set in the job template. When the node is created, it is labeled with its job type. A template that is associated with each workflow node runs based on the selected run scenario as it proceeds. Click Legend to display the legend for each run scenario and their job types. Hover over a node to edit the node, add step and link, or delete the selected node: When you have added or edited a node, click Finish to save any modifications and render it on the graphical view. For possible ways to build your workflow, see Building nodes scenarios . When you have built your workflow job template, click Save to save your entire workflow template and return to the new workflow job template details page. Important Clicking Close does not save your work, but instead, it closes the entire Workflow Visualizer so that you have to start again. 8.7.2. Approval nodes Choosing an Approval node requires your intervention to advance a workflow. This functions as a means to pause the workflow in between playbooks so that you can give approval to continue on to the next playbook in the workflow. This gives the user a specified amount of time to intervene, but also enables you to continue as quickly as possible without having to wait on another trigger. The default for the timeout is none, but you can specify the length of time before the request expires and is automatically denied. After you select and supply the information for the approval node, it displays on the graphical view with a pause icon beside it. The approver is anyone who meets the following criteria: A user that can execute the workflow job template containing the approval nodes. A user who has organization administrator or above privileges (for the organization associated with that workflow job template). A user who has the Approve permission explicitly assigned to them within that specific workflow job template. If pending approval nodes are not approved within the specified time limit (if an expiration was assigned) or they are denied, then they are marked as \"timed out\" or \"failed\", and move on to the next \"on fail node\" or \"always node\". If approved, the \"on success\" path is taken. If you try to POST in the API to a node that has already been approved, denied or timed out, an error message notifies you that this action is redundant, and no further steps are taken. The following table shows the various levels of permissions allowed on approval workflows: 8.7.3. Building nodes scenarios Learn how to manage nodes in the following scenarios. Procedure Click the ( ) icon on the parent node and Add step and link to add a sibling node: Click Add step or Start ( ) and Add step , to add a root node to depict a split scenario. At any node where you want to create a split scenario, hover over the node from which the split scenario begins and click the plus ( ) icon on the parent node and Add step and link . This adds multiple nodes from the same parent node, creating sibling nodes. Refer to the key by clicking Legend to identify the meaning of the symbols and colors associated with the graphical depiction. Note If you remove a node that has a follow-on node attached to it in a workflow with a set of sibling nodes that has varying edge types, the attached node automatically joins the set of sibling nodes and retains its edge type: 8.7.4. Editing a node Procedure Edit a node by using one of these methods: If you want to edit a node, click the icon of the node. The pane displays the current selections, click Edit to change these. Make your changes and click Finish to apply them to the graphical view. To edit the edge type for an existing link, ( Run on success , Run on fail , Run always ), click ( ) on the existing status. To remove a link, click ( ) for the link and click Remove link . This option only appears in the pane if the target or child node has more than one parent. All nodes must be linked to at least one other node at all times so you must create a new link before removing an old one. Edit the view of the workflow diagram by using one of these methods: Click the examine icon ( ) to zoom in, the reduce icon ( ) to zoom out, the expand icon ( ) to fit to screen or the reset icon ( ) to reposition the view. Drag the workflow diagram to reposition it on the screen or use the scroll on your mouse to zoom. 8.8. Launching a workflow job template Procedure Launch a workflow job template by using one of these methods: From the navigation panel, select Automation Execution Templates and click the icon next to the job template. Click Launch template in the Details tab of the workflow job template that you want to launch. Variables added for a workflow job template are automatically added in automation controller when launching, along with any extra variables set in the workflow job template and survey. Events related to approvals on workflows are displayed in the activity stream ( ) with detailed information about the approval requests, if any. 8.9. Copying a workflow job template With automation controller you can copy a workflow job template. When you copy a workflow job template, it does not copy any associated schedule, notifications, or permissions. Schedules and notifications must be recreated by the user or system administrator creating the copy of the workflow template. The user copying the workflow template is granted the administrator permission, but no permissions are assigned (copied) to the workflow template. Procedure Open the workflow job template that you want to copy by using one of these methods: From the navigation panel, select Automation Execution Templates . In the workflow job template Details view, click next to the desired template. Click the copy ( ) icon. The new template with the name of the template from which you copied and a timestamp displays in the list of templates. Select the copied template and click Edit template . Replace the contents of the Name field with a new name, and give or change the entries in the other fields to complete this template. Click Save job template . Note If a resource has a related resource that you do not have the right level of permission to, you cannot copy the resource. For example, in the case where a project uses a credential that a current user only has Read access. However, for a workflow job template, if any of its nodes use an unauthorized job template, inventory, or credential, the workflow template can still be copied. But in the copied workflow job template, the corresponding fields in the workflow template node are absent. 8.10. Workflow job template extra variables For more information see the Extra variables section. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_automation_execution/controller-workflow-job-templates"}
{"title": "Project APIs", "content": "Project APIs  Reference guide for project APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/project_apis/index"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest"}
{"title": "Builds using BuildConfig", "content": "Builds using BuildConfig  Builds Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/builds_using_buildconfig/index"}
{"title": "Red Hat Ansible Lightspeed with IBM watsonx Code Assistant User Guide", "content": "Red Hat Ansible Lightspeed with IBM watsonx Code Assistant User Guide Red Hat Ansible Lightspeed with IBM watsonx Code Assistant 2.x_latest Learn how to use Red Hat Ansible Lightspeed with IBM watsonx Code Assistant. Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant/2.x_latest/html-single/red_hat_ansible_lightspeed_with_ibm_watsonx_code_assistant_user_guide/access.redhat.com"}
{"title": "Chapter 8. Configuring LVM on shared storage", "content": "Chapter 8. Configuring LVM on shared storage Shared storage is storage that can be accessed by multiple nodes at the same time. You can use LVM to manage shared storage. Shared storage is commonly used in cluster and high-availability setups and there are two common scenarios for how shared storage appears on the system: LVM devices are attached to a host and passed to a guest VM to use. In this case, the device is never intended to be used by the host, only by the guest VM. Machines are attached to a storage area network (SAN), for example using Fiber Channel, and the SAN LUNs are visible to multiple machines: 8.1. Configuring LVM for VM disks To prevent VM storage from being exposed to the host, you can configure LVM device access and LVM system ID . You can do this by excluding the devices in question from the host, which ensures that the LVM on the host doesn’t see or use the devices passed to the guest VM. You can protect against accidental usage of the VM’s VG on the host by setting the LVM system ID in the VG to match the guest VM. Procedure In the lvm.conf file, check if the system.devices file is enabled: use_devicesfile=1 Exclude the devices in question from the host’s devices file: $ lvmdevices --deldev <device> Optional: You can further protect LVM devices: Set the LVM system ID feature in both the host and the VM in the lvm.conf file: system_id_source = \"uname\" Set the VG’s system ID to match the VM system ID . This ensures that only the guest VM is capable of activating the VG: $ vgchange --systemid <VM_system_id> <VM_vg_name> 8.2. Configuring LVM to use SAN disks on one machine To prevent the SAN LUNs from being used by the wrong machine, exclude the LUNs from the devices file on all machines except the one machine which is meant to use them. You can also protect the VG from being used by the wrong machine by configuring a system ID on all machines, and setting the system ID in the VG to match the machine using it. Procedure In the lvm.conf file, check if the system.devices file is enabled: use_devicesfile=1 Exclude the devices in question from the host’s devices file: $ lvmdevices --deldev <device> Set the LVM system ID feature in the lvm.conf file: system_id_source = \"uname\" Set the VG’s system ID to match the system ID of the machine using this VG: $ vgchange --systemid <system_id> <vg_name> 8.3. Configuring LVM to use SAN disks for failover You can configure LUNs to be moved between machines, for example for failover purposes. You can set up the LVM by configuring the LVM devices file and including the LUNs in the devices file on all machines that may use the devices and by configuring the LVM system ID on each machine. The following procedure describes the initial LVM configuration, to finish setting up LVM for failover and move the VG between machines, you need to configure pacemaker and LVM-activate resource agent that will automatically modify the VG’s system ID to match the system ID of the machine where the VG can be used. For more information see Configuring and managing high availability clusters . Procedure In the lvm.conf file, check if the system.devices file is enabled: use_devicesfile=1 Include the devices in question in the host’s devices file: $ lvmdevices --adddev <device> Set the LVM system ID feature in all machines in the lvm.conf file: system_id_source = \"uname\" 8.4. Configuring LVM to share SAN disks among multiple machines Using the lvmlockd daemon and a lock manager such as dlm or sanlock , you can enable access to a shared VG on the SAN disks from multiple machines. The specific commands may differ based on the lock manager and operating system used. The following procedure describes the overview of the required steps to configure LVM to share SAN disks among multiple machines. Warning When using pacemaker , the system must be configured and started using the pacemaker steps shown in Configuring and managing high availability clusters instead. Procedure In the lvm.conf file, check if the system.devices file is enabled: use_devicesfile=1 For each machine that will use the shared LUN, add the LUN in the machines devices file: $ lvmdevices --adddev <device> Configure the lvm.conf file to use the lvmlockd daemon on all machines: use_lvmlockd=1 Start the lvmlockd daemon file on all machines. Start a lock manager to use with lvmlockd , such as dlm or sanlock on all machines. Create a new shared VG using the command vgcreate --shared . Start and stop access to existing shared VGs using the commands vgchange --lockstart and vgchange --lockstop on all machines. Additional resources lvmlockd(8) man page on your system 8.5. Creating shared LVM devices using the storage RHEL system role You can use the storage RHEL system role to create shared LVM devices if you want your multiple systems to access the same storage at the same time. This can bring the following notable benefits: Resource sharing Flexibility in managing storage resources Simplification of storage management tasks Prerequisites You have prepared the control node and the managed nodes You are logged in to the control node as a user who can run playbooks on the managed nodes. The account you use to connect to the managed nodes has sudo permissions on them. lvmlockd is configured on the managed node. For more information, see Configuring LVM to share SAN disks among multiple machines . Procedure Create a playbook file, for example ~/playbook.yml , with the following content: --- - name: Manage local storage hosts: managed-node-01.example.com become: true tasks: - name: Create shared LVM device ansible.builtin.include_role: name: rhel-system-roles.storage vars: storage_pools: - name: vg1 disks: /dev/vdb type: lvm shared: true state: present volumes: - name: lv1 size: 4g mount_point: /opt/test1 storage_safe_mode: false storage_use_partitions: true For details about all variables used in the playbook, see the /usr/share/ansible/roles/rhel-system-roles.storage/README.md file on the control node. Validate the playbook syntax: $ ansible-playbook --syntax-check ~/playbook.yml Note that this command only validates the syntax and does not protect against a wrong but valid configuration. Run the playbook: $ ansible-playbook ~/playbook.yml Additional resources /usr/share/ansible/roles/rhel-system-roles.storage/README.md file /usr/share/doc/rhel-system-roles/storage/ directory Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/configuring-lvm-on-shared-storage_configuring-and-managing-logical-volumes"}
{"title": "Observability", "content": "Observability builds for Red Hat  Build controller observability Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/observability/index"}
{"title": "Chapter 4. Basic logical volume management", "content": "Chapter 4. Basic logical volume management With LVM, you can do the following tasks: Create new logical volumes to extend storage capabilities of your system Extend existing volumes and thin pools to accommodate growing data Rename volumes for better organization Reduce volumes to free up unused space Safely remove volumes when they are no longer needed Activate or deactivate volumes to control the system’s access to its data 4.1. Overview of logical volume features With the Logical Volume Manager (LVM), you can manage disk storage in a flexible and efficient way that traditional partitioning schemes cannot offer. Below is a summary of key LVM features that are used for storage management and optimization. Concatenation Concatenation involves combining space from one or more physical volumes into a singular logical volume, effectively merging the physical storage. Striping Striping optimizes data I/O efficiency by distributing data across multiple physical volumes. This method enhances performance for sequential reads and writes by allowing parallel I/O operations. RAID LVM supports RAID levels 0, 1, 4, 5, 6, and 10. When you create a RAID logical volume, LVM creates a metadata subvolume that is one extent in size for every data or parity subvolume in the array. Thin provisioning Thin provisioning enables the creation of logical volumes that are larger than the available physical storage. With thin provisioning, the system dynamically allocates storage based on actual usage instead of allocating a predetermined amount upfront. Snapshots With LVM snapshots, you can create point-in-time copies of logical volumes. A snapshot starts empty. As changes occur on the original logical volume, the snapshot captures the pre-change states through copy-on-write (CoW), growing only with changes to preserve the state of the original logical volume. Caching LVM supports the use of fast block devices, such as SSD drives as write-back or write-through caches for larger slower block devices. Users can create cache logical volumes to improve the performance of their existing logical volumes or create new cache logical volumes composed of a small and fast device coupled with a large and slow device. 4.2. Creating logical volumes LVM provides a flexible approach to handling disk storage by abstracting the physical layer into logical volumes that can be created and adjusted based on your needs. 4.2.1. Creating a linear (thick) logical volume With linear logical volumes (LVs), you can merge multiple physical storage units into one virtual storage space. You can easily expand or reduce linear LVs to accommodate the data requirements. Prerequisites Administrative access. The lvm2 package is installed. The volume group is created. For more information, see Creating LVM volume group . Procedure List the names of volume groups and their size: # vgs -o vg_name,vg_size VG VSize VolumeGroupName 30.75g Create a linear LV: # lvcreate --name LogicalVolumeName --size VolumeSize VolumeGroupName Replace LogicalVolumeName with the name of the LV. Replace VolumeSize with the size for the LV. If no size suffix is provided the command defaults to MB. Replace VolumeGroupName with the name of the volume group. Verification Verify that the linear LV is created: # lvs -o lv_name,seg_type LV Type LogicalVolumeName linear Additional resources The vgs(8) , lvs(8) , lvcreate(8) man pages 4.2.2. Creating or resizing a logical volume by using the storage RHEL system role Use the storage role to perform the following tasks: To create an LVM logical volume in a volume group consisting of many disks To resize an existing file system on LVM To express an LVM volume size in percentage of the pool’s total size If the volume group does not exist, the role creates it. If a logical volume exists in the volume group, it is resized if the size does not match what is specified in the playbook. If you are reducing a logical volume, to prevent data loss you must ensure that the file system on that logical volume is not using the space in the logical volume that is being reduced. Prerequisites You have prepared the control node and the managed nodes You are logged in to the control node as a user who can run playbooks on the managed nodes. The account you use to connect to the managed nodes has sudo permissions on them. Procedure Create a playbook file, for example ~/playbook.yml , with the following content: --- - name: Manage local storage hosts: managed-node-01.example.com tasks: - name: Create logical volume ansible.builtin.include_role: name: rhel-system-roles.storage vars: storage_pools: - name: myvg disks: - sda - sdb - sdc volumes: - name: mylv size: 2G fs_type: ext4 mount_point: /mnt/data The settings specified in the example playbook include the following: size: <size> You must specify the size by using units (for example, GiB) or percentage (for example, 60%). For details about all variables used in the playbook, see the /usr/share/ansible/roles/rhel-system-roles.storage/README.md file on the control node. Validate the playbook syntax: $ ansible-playbook --syntax-check ~/playbook.yml Note that this command only validates the syntax and does not protect against a wrong but valid configuration. Run the playbook: $ ansible-playbook ~/playbook.yml Verification Verify that specified volume has been created or resized to the requested size: # ansible managed-node-01.example.com -m command -a 'lvs myvg' Additional resources /usr/share/ansible/roles/rhel-system-roles.storage/README.md file /usr/share/doc/rhel-system-roles/storage/ directory 4.2.3. Creating a striped logical volume With striped logical volume (LV), you can distribute the data across multiple physical volumes (PVs), potentially increasing the read and write speed by utilizing the bandwidth of multiple disks simultaneously. When creating a striped LV, it is important to consider the stripe number and size. The stripe number is the count of PVs across which data is distributed. Increasing the stripe number can enhance performance by utilizing multiple disks concurrently. Stripe size is the size of the data chunk written to each disk in the stripe set before moving to the next disk and is specified in kilobytes (KB). The optimal stripe size depends on your workload and the filesystem block size. The default is 64KB and can be adjusted. Prerequisites Administrative access. Procedure List the names of volume groups and their size: # vgs -o vg_name,vg_size VG VSize VolumeGroupName 30.75g Create a striped LV: # lvcreate --stripes NumberOfStripes --stripesize StripeSize --size LogicalVolumeSize --name LogicalVolumeName VolumeGroupName Replace NumberOfStripes with the number of stripes. Replace StripeSize with the stripe size in kilobytes. The --stripesize is not a required option. If you do not specify the stripe size it defaults to 64KB. Replace LogicalVolumeName with the name of the LV. Replace VolumeGroupName with the name of the volume group. Verification Verify that the striped LV is created: # lvs -o lv_name,seg_type LV Type LogicalVolumeName striped Additional resources The vgs(8) lvs(8) , lvcreate(8) man pages 4.2.4. Creating a RAID logical volume RAID logical volumes enable you to use multiple disks for redundancy and performance. LVM supports various RAID levels, including RAID0, RAID1, RAID4, RAID5, RAID6, and RAID10. With LVM you can create striped RAIDs (RAID0, RAID4, RAID5, RAID6), mirrored RAID (RAID1), or a combination of both (RAID10). RAID 4, RAID 5, and RAID 6 offer fault tolerance by storing parity data that can be used to reconstruct lost information in case of a disk failure. When creating RAID LVs, place each stripe on a separate PV. The number of stripes equals to the number of PVs that should be in the volume group (VG). Table 4.1. Minimal RAID configuration requirements RAID level Type Parity Minimum number of devices Minimum stripe number RAID0 Striping None 2 2 RAID1 Mirroring None 2 - RAID4 Striping Uses first device to store parity 3 2 RAID5 Striping Uses an extra device to store parity 3 2 RAID6 Striping Uses two extra devices to store parity 5 3 RAID10 Striping and mirroring None 4 2 Prerequisites Administrative access. Procedure List the names of volume groups and their size: # vgs -o vg_name,vg_size VG VSize VolumeGroupName 30.75g Create a RAID LV: To create a striped raid, use: # lvcreate --type raid level --stripes NumberOfStripes --stripesize StripeSize --size Size --name LogicalVolumeName VolumeGroupName Replace level with the RAID level 0, 4, 5, or 6. Replace NumberOfStripes with the number of stripes. Replace StripeSize with the stripe size in kilobytes. Replace Size with the size of the LV. Replace LogicalVolumeName with the name of the LV. To create a mirrored RAID, use: # lvcreate --type raid1 --mirrors MirrorsNumber --size Size --name LogicalVolumeName VolumeGroupName Replace MirrorsNumber with the number of mirrors. Replace Size with the size of the LV. Replace LogicalVolumeName with the name of the LV. To create a mirrored and striped RAID, use: # lvcreate --type raid10 --mirrors MirrorsNumber --stripes NumberOfStripes --stripesize StripeSize --size Size --name LogicalVolumeName VolumeGroupName Replace MirrorsNumber with the number of mirrors. Replace NumberOfStripes with the number of stripes. Replace StripeSize with the stripe size in kilobytes. Replace Size with the size of the LV. Replace LogicalVolumeName with the name of the LV. Verification Verify that the RAID LV is created: # lvs -o lv_name,seg_type LV Type LogicalVolumeName raid0 Additional resources lvmraid(7) , vgs(8) , lvs(8) , lvcreate(8) man pages 4.2.5. Creating a thin logical volume Under thin provisioning, physical extents (PEs) from a volume group (VG) are allocated to create a thin pool with a specific physical size. Logical volumes (LVs) are then allocated from this thin pool based on a virtual size, not limited by the pool’s physical capacity. With this, the virtual size of each thin LV can exceed the actual size of the thin pool leading to over-provisioning, when the collective virtual sizes of all thin LVs surpasses the physical capacity of the thin pool. Therefore, it is essential to monitor both logical and physical usage closely to avoid running out of space and outages. Thin provisioning optimizes storage efficiency by allocating space as needed, lowering initial costs and improving resource utilization. However, when using thin LVs, beware of the following drawbacks: Improper discard handling can block the release of unused storage space, causing full allocation of the space over time. Copy on Write (CoW) operation can be slower on file systems with snapshots. Data blocks can be intermixed between multiple file systems leading to random access limitations. Prerequisites Administrative access. You have created a physical volume. For more information, see Creating LVM physical volume . You have created a volume group. For more information, see Creating LVM volume group . You have created a logical volume. For more information, see Creating logical volumes . Procedure List the names of volume groups and their size: # vgs -o vg_name,vg_size VG VSize VolumeGroupName 30.75g Create a thin pool: # lvcreate --type thin-pool --size PoolSize --name ThinPoolName VolumeGroupName Replace PoolSize with the maximum amount of disk space the thin pool can use. Replace ThinPoolName with the name for the thin pool. Replace VolumeGroupName with the name of the volume group. Create a thin LV: # lvcreate --type thin --virtualsize MaxVolumeSize --name ThinVolumeName --thinpool ThinPoolName VolumeGroupName Replace MaxVolumeSize with the maximum size the volume can grow to within the thin pool. Replace ThinPoolName with the name for the thin pool. Replace VolumeGroupName with the name of the volume group. Note You can create other thin LVs within the same thin pool. Verification Verify that the thin LV is created: # lvs -o lv_name,seg_type LV Type ThinPoolName thin-pool ThinVolumeName thin Additional resources The lvs(8) , lvcreate(8) man pages 4.2.6. Creating a VDO logical volume VDO logical volumes (LVs) use the Virtual Data Optimizer (VDO) technology to enhance storage efficiency. VDO LVs have both a virtual size and a physical size. The virtual size refers to the total amount of storage presented to users and applications. The physical size is the actual amount of physical storage allocated from the VG and consumed by the VDO pool. The virtual size of a VDO LV is generally larger than the physical size of the VDO pool, making it over-provisioned. Due to over-provisioning the physical space in the VDO pool needs to be closely monitored and extended when needed. A VDO LV and a VDO pool are created as a pair, and always exist as a pair. Prerequisites Administrative access. Procedure List the names of volume groups and their size: # vgs -o vg_name,vg_size VG VSize VolumeGroupName 30.75g Create a VDO LV: # lvcreate --type vdo --virtualsize VolumeSize --size PhysicalPoolSize --name VDOVolumeName --vdopool VDOPoolName VolumeGroupName Replace the VolumeSize with the size for the volume. Replace the PhysicalPoolSize with the size for the pool. Replace the VDOVolumeName with the name for your VDO volume. Replace the VDOPoolName with the name for your VDO pool. Replace VolumeGroupName with the name of the volume group. Verification Verify that the VDO LV is created: # lvs -o name,seg_type,size LV Type LSize VDOPoolName vdo-pool 5.00g VDOVolumeName vdo 5.00g Additional resources The vgs(8) , lvs(8) , lvcreate(8) man pages 4.3. Resizing logical volumes With Logical Volume Manager (LVM), you can resize logical volumes (LVs) as needed without affecting the data stored on them. 4.3.1. Extending a linear logical volume You can extend linear (thick) LVs and their snapshots with the lvextend command. Prerequisites Administrative access. Procedure Ensure your volume group has enough space to extend your LV: # lvs -o lv_name,lv_size,vg_name,vg_size,vg_free LV LSize VG VSize VFree LogicalVolumeName 1.49g VolumeGroupName 30.75g 29.11g Extend the linear LV and resize the file system: # lvextend --size + AdditionalSize --resizefs VolumeGroupName / LogicalVolumeName Replace AdditionalSize with how much space to add to the LV. The default unit of measurement is megabytes, but you can specify other units. Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the thin volume. Verification Verify that the linear LV is extended: # lvs -o lv_name,lv_size LV LSize NewLogicalVolumeName 6.49g 4.3.2. Extending a thin logical volume You can extend the thin logical volume (LV) with the lvextend command. Prerequisites Administrative access. Procedure Ensure the thin pool has enough space for the data you plan to add: # lvs -o lv_name,lv_size,data_percent LV LSize Data% MyThinPool 20.10g 3.21 ThinVolumeName 1.10g 4.88 Extend the thin LV and resize the file system: # lvextend --size + AdditionalSize --resizefs VolumeGroupName / ThinVolumeName Replace AdditionalSize with how much space to add to the LV. The default unit of measurement is megabytes, but you can specify other units. Replace VolumeGroupName with the name of the volume group. Replace ThinVolumeName with the name of the thin volume. Verification Verify the thin LV is extended: # lvs -o lv_name,lv_size,data_percent LV LSize Data% MyThinPool 20.10g 3.21 ThinVolumeName 6.10g 0.43 4.3.3. Extending a thin pool The virtual size of thin logical volumes can exceed the physical capacity of the thin pool resulting in over-provisioning. To prevent running out of space, you must monitor and periodically extend the capacity of the thin pool. The data_percent metric indicates the percentage of the allocated data space that the thin pool currently uses. The metadata_percent metric reflects the percentage of space used for storing metadata, which is essential for managing the mappings within the thin pool. Monitoring these metrics is vital to ensure efficient storage management and to avoid capacity issues. LVM provides the option to manually extend the data or metadata capacity as needed. Alternatively, you can enable monitoring and automate the expansion of your thin pool. 4.3.3.1. Manually extending a thin pool Logical Volume Manager (LVM) provides the option to manually extend the data segment, the metadata segment, or the thin pool. 4.3.3.1.1. Extending a thin pool You can use the lvextend command to extend the thin pool. Prerequisites Administrative access. Procedure Display the data and metadata space used: # lvs -o lv_name,seg_type,data_percent,metadata_percent LV Type Data% Meta% ThinPoolName thin-pool 97.66 26.86 ThinVolumeName thin 48.80 Extend the thin pool: # lvextend -L Size VolumeGroupName/ThinPoolName Replace Size with the new size for your thin pool. Replace VolumeGroupName with the name of the volume group. Replace ThinPoolName with the name of the thin pool. The data size will be extended. The metadata size will be extended if necessary. Verification Verify that the thin pool is extended: # lvs -o lv_name,seg_type,data_percent,metadata_percent LV Type Data% Meta% ThinPoolName thin-pool 24.41 16.93 ThinVolumeName thin 24.41 Additional resources The lvs(8) , lvextend(8) man pages lvs -o help 4.3.3.1.2. Extending a thin pool data segment You can use the lvextend command to extend the data_percent segment. Prerequisites Administrative access. Procedure Display the data_percent segment: # lvs -o lv_name,seg_type,data_percent LV Type Data% ThinPoolName thin-pool 93.87 Extend the data_percent segment: # lvextend -L Size VolumeGroupName/ThinPoolName _tdata Replace Size with the size for your data segment. Replace VolumeGroupName with name of the volume group. Replace ThinPoolName with the name of the thin pool. Verification Verify that the data_percent segment is extended: # lvs -o lv_name,seg_type,data_percent LV Type Data% ThinPoolName thin-pool 40.23 Additional resources The lvs(8) , lvextend(8) man pages lvs -o help 4.3.3.1.3. Extending a thin pool metadata segment You can use the lvextend command to extend the metadata_percent segment. Prerequisites Administrative access. Procedure Display the metadata_percent segment: # lvs -o lv_name,seg_type,metadata_percent LV Type Meta% ThinPoolName thin-pool 75.00 Extend the metadata_percent segment: # lvextend -L Size VolumeGroupName/ThinPoolName _tmeta Replace Size with the size for your metadata segment. Replace VolumeGroupName with name of the volume group. Replace ThinPoolName with the name of the thin pool. Verification Verify that the metadata_percent segment is extended: # lvs -o lv_name,seg_type,metadata_percent LV Type Meta% ThinPoolName thin-pool 0.19 Additional resources The lvs(8) , lvextend(8) man pages lvs -o help 4.3.3.2. Automatically extending a thin pool You can automate the expansion of your thin pool by enabling monitoring and setting the thin_pool_autoextend_threshold and the thin_pool_autoextend_percent configuration parameters. Prerequisites Administrative access. Procedure Check if the thin pool is monitored: # lvs -o lv_name,vg_name,seg_monitor LV VG Monitor ThinPoolName VolumeGroupName not monitored Enable thin pool monitoring with the dmeventd daemon: # lvchange --monitor y VolumeGroupName/ThinPoolName Replace VolumeGroupName with the name of the volume group. Replace ThinPoolName with the name of the thin pool. As the root user, open the /etc/lvm/lvm.conf file in an editor of your choice. Uncomment the thin_pool_autoextend_threshold and thin_pool_autoextend_percent lines and set each parameter to a required value: thin_pool_autoextend_threshold = 70 thin_pool_autoextend_percent = 20 thin_pool_autoextend_threshold determines the percentage at which LVM starts to auto-extend the thin pool. For example, setting it to 70 means LVM will try to extend the thin pool when it reaches 70% capacity. thin_pool_autoextend_percent specifies by what percentage the thin pool should be extended when it reaches threshold. For example, setting it to 20 means the thin pool will be increased by 20% of its current size. Save the changes and exit the editor. Restart the lvm2-monitor : # systemctl restart lvm2-monitor Additional resources The lvs(8) , lvchange(8) , dmeventd(8) man pages 4.3.4. Extending a VDO Pool It is crucial to monitor and periodically extend the capacity of the VDO pool to prevent running out of space. Logical Volume Manager (LVM) provides the option to manually extend the VDO pool capacity as needed. Alternatively, you can enable monitoring and automate the extension of your VDO pool. 4.3.4.1. Manually extending a VDO Pool Use the lvextend command to extend a VDO pool. Prerequisites Administrative access. Procedure Display the current VDO usage: # lvs -o lv_name,vg_name,lv_size,data_percent VolumeGroupName/VDOPoolName LV VG LSize Data% VDOPoolName VolumeGroupName 5.00g 60.03 Replace VolumeGroupName with the name of the volume group. Replace the VDOPoolName with the name of the VDO pool. Extend the VDO Pool: # lvextend --size PhysicalSize VolumeGroupName/VDOPoolName Replace PhysicalSize with the new physical size. Replace VolumeGroupName with the name of the volume group. Replace the VDOPoolName with the name of the VDO pool. Verification Verify that the VDO pool is extended: # lvs -o lv_name,vg_name,lv_size,data_percent VolumeGroupName/VDOPoolName LV VG LSize Data% VDOPoolName VolumeGroupName 10.00g 30.02 Additional resources The lvs(8) , lvextend(8) man pages 4.3.4.2. Automatically extending a VDO Pool You can automate the expansion of your Virtual Data Optimizer (VDO) pool by enabling monitoring and setting the vdo_pool_autoextend_threshold and the vdo_pool_autoextend_percent parameters. Prerequisites Administrative access. Procedure Check if the VDO pool is monitored: # lvs -o name,seg_monitor VolumeGroupName / VDOPoolName LV VG Monitor VDOPoolName VolumeGroupName not monitored Replace VolumeGroupName with the name of the volume group. Replace VDOPoolName with the name of the VDO pool. Enable VDO pool monitoring with the dmeventd daemon: # lvchange --monitor y VolumeGroupName / VDOPoolName Replace VolumeGroupName with the name of the volume group. Replace VDOPoolName with the name of the VDO pool. As the root user, open the /etc/lvm/lvm.conf file in an editor of your choice. Uncomment the vdo_pool_autoextend_percent and vdo_pool_autoextend_threshold lines and set each parameter to a required value: vdo_pool_autoextend_threshold = 70 vdo_pool_autoextend_percent = 20 vdo_pool_autoextend_threshold determines the percentage at which LVM starts to auto-extend the VDO pool. For example, setting it to 70 means LVM tries to extend the VDO pool when it reaches 70% capacity. vdo_pool_autoextend_percent specifies by what percentage the VDO pool should be extended when it reaches the threshold. For example, setting it to 20 means the VDO pool will be increased by 20% of its current size. Save the changes and exit the editor. Restart the lvm2-monitor : # systemctl restart lvm2-monitor Additional resources The lvs(8) , lvchange(8) , dmeventd(8) man pages 4.3.5. Shrinking logical volumes When the size of the LV is reduced, the freed up logical extents are returned to the volume group and then can be used by other LVs. Warning Data stored in the reduced area is lost. Always back up the data and resize the file system before proceeding. Prerequisites Administrative access. Procedure List the logical volumes and their volume groups: # lvs -o lv_name,vg_name,lv_size LV VG LSize LogicalVolumeName VolumeGroupName 6.49g Check where the logical volume is mounted: # findmnt -o SOURCE,TARGET /dev/VolumeGroupName/LogicalVolumeName SOURCE TARGET /dev/mapper/VolumeGroupName-NewLogicalVolumeName /MountPoint Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume. Unmount the logical volume: # umount /MountPoint Replace /MountPoint with the mounting point for your logical volume. Check and repair any file system errors: # e2fsck -f /dev/VolumeGroupName/LogicalVolumeName Resize the LV and the file system: # lvreduce --size TargetSize --resizefs VolumeGroupName/LogicalVolumeName Replace TargetSize with the new size of the LV. Replace VolumeGroupName/LogicalVolumeName with the path to your logical volume. Remount the file system: # mount -o remount /MountPoint Replace /MountPoint with the mounting point for your file system. Verification Verify the space usage of the file system: # df -hT /MountPoint/ Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/VolumeGroupName-NewLogicalVolumeName ext4 2.9G 139K 2.7G 1% /MountPoint Replace /MountPoint with the mounting point for your logical volume. Verify the size of the LV: # lvs -o lv_name,lv_size LV LSize NewLogicalVolumeName 4.00g 4.4. Renaming logical volumes You can rename an existing logical volume, including snapshots, using the lvrename command. Prerequisites Administrative access. Procedure List the logical volumes and their volume groups: # lvs -o lv_name,vg_name LV VG LogicalVolumeName VolumeGroupName Rename the logical volume: # lvrename VolumeGroupName/LogicalVolumeName VolumeGroupName/NewLogicalVolumeName Replace VolumeGroupName with name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Replace NewLogicalVolumeName with the new logical volume name. Verification Verify that the logical volume is renamed: # lvs -o lv_name LV NewLogicalVolumeName Additional resources lvrename(8) man page on your system 4.5. Removing logical volumes You can remove an existing logical volume, including snapshots, using the lvremove command. Prerequisites Administrative access. Procedure List the logical volumes and their paths: # lvs -o lv_name,lv_path LV Path LogicalVolumeName /dev/VolumeGroupName/LogicalVolumeName Check where the logical volume is mounted: # findmnt -o SOURCE,TARGET /dev/VolumeGroupName/LogicalVolumeName SOURCE TARGET /dev/mapper/VolumeGroupName-LogicalVolumeName /MountPoint Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume. Unmount the logical volume: # umount /MountPoint Replace /MountPoint with the mounting point for your logical volume. Remove the logical volume: # lvremove VolumeGroupName/LogicalVolumeName Replace VolumeGroupName/LogicalVolumeName with the path to your logical volume. Additional resources lvs(8) , lvremove(8) man pages on your system 4.6. Activating logical volumes You can activate the logical volume with the lvchange command. Prerequisites Administrative access. Procedure List the logical volumes, their volume groups, and their paths: # lvs -o lv_name,vg_name,lv_path LV VG Path LogicalVolumeName VolumeGroupName VolumeGroupName/LogicalVolumeName Activate the logical volume: # lvchange --activate y VolumeGroupName / LogicalVolumeName Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Note When activating a thin LV that was created as a snapshot of another LV, you might need to use the --ignoreactivationskip option to activate it. Verification Verify that the LV is active: # lvdisplay VolumeGroupName / LogicalVolumeName ... LV Status available Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Additional resources The lvs(8) lvchange(8) , lvdisplay(8) man pages 4.7. Deactivating logical volumes By default, when you create a logical volume, it is in an active state. You can deactivate the logical volume with the lvchange command. Warning Deactivating a logical volume with active mounts or in use can lead to data inconsistencies and system errors. Prerequisites Administrative access. Procedure List the logical volumes, their volume groups, and their paths: # lvs -o lv_name,vg_name,lv_path LV VG Path LogicalVolumeName VolumeGroupName /dev/VolumeGroupName/LogicalVolumeName Check where the logical volume is mounted: # findmnt -o SOURCE,TARGET /dev/VolumeGroupName/LogicalVolumeName SOURCE TARGET /dev/mapper/VolumeGroupName-LogicalVolumeName /MountPoint Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume. Unmount the logical volume: # umount /MountPoint Replace /MountPoint with the mounting point for your logical volume. Deactivate the logical volume: # lvchange --activate n VolumeGroupName / LogicalVolumeName Replace VolumeGroupName with name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Verification Verify that the LV is not active: # lvdisplay VolumeGroupName/LogicalVolumeName ... LV Status NOT available Replace VolumeGroupName with name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Additional resources The lvs(8) lvchange(8) , lvdisplay(8) man pages Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/basic-logical-volume-management_configuring-and-managing-logical-volumes"}
{"title": "Authentication", "content": "Authentication builds for Red Hat  Understanding authentication at runtime Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/authentication/index"}
{"title": "Authorization APIs", "content": "Authorization APIs  Reference guide for authorization APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/authorization_apis/index"}
{"title": "Uninstall", "content": "Uninstall builds for Red Hat  Uninstalling Builds Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/uninstall/index"}
{"title": "Work with Builds", "content": "Work with Builds builds for Red Hat  Using Builds Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/work_with_builds/index"}
{"title": "Node APIs", "content": "Node APIs  Reference guide for node APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/node_apis/index"}
{"title": "Service Mesh", "content": "Service Mesh  Service Mesh installation, usage, and release notes Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/service_mesh/index"}
{"title": "Chapter 1. Getting started using the RHEL web console", "content": "Chapter 1. Getting started using the RHEL web console Learn how to install the Red Hat Enterprise Linux 9 web console, how to add and manage remote hosts through its convenient graphical interface, and how to monitor the systems managed by the web console. 1.1. What is the RHEL web console The RHEL web console is a web-based interface designed for managing and monitoring your local system, as well as Linux servers located in your network environment. The RHEL web console enables you to perform a wide range of administration tasks, including: Managing services Managing user accounts Managing and monitoring system services Configuring network interfaces and firewall Reviewing system logs Managing virtual machines Creating diagnostic reports Setting kernel dump configuration Configuring SELinux Updating software Managing system subscriptions The RHEL web console uses the same system APIs as you would use in a terminal, and actions performed in a terminal are immediately reflected in the RHEL web console. You can monitor the logs of systems in the network environment, as well as their performance, displayed as graphs. In addition, you can change the settings directly in the web console or through the terminal. 1.2. Installing and enabling the web console To access the RHEL web console, first enable the cockpit.socket service. Red Hat Enterprise Linux 9 includes the web console installed by default in many installation variants. If this is not the case on your system, install the cockpit package before enabling the cockpit.socket service. Procedure If the web console is not installed by default on your installation variant, manually install the cockpit package: # dnf install cockpit Enable and start the cockpit.socket service, which runs a web server: # systemctl enable --now cockpit.socket If the web console was not installed by default on your installation variant and you are using a custom firewall profile, add the cockpit service to firewalld to open port 9090 in the firewall: # firewall-cmd --add-service=cockpit --permanent # firewall-cmd --reload Verification To verify the previous installation and configuration, open the web console . 1.3. Logging in to the web console When the cockpit.socket service is running and the corresponding firewall port is open, you can log in to the web console in your browser for the first time. Prerequisites Use one of the following browsers to open the web console: Mozilla Firefox 52 and later Google Chrome 57 and later Microsoft Edge 16 and later System user account credentials The RHEL web console uses a specific pluggable authentication modules (PAM) stack at /etc/pam.d/cockpit . The default configuration allows logging in with the user name and password of any local account on the system. Port 9090 is open in your firewall. Procedure In your web browser, enter the following address to access the web console: https://localhost:9090 Note This provides a web-console login on your local machine. If you want to log in to the web console of a remote system, see Section 1.5, “Connecting to the web console from a remote machine” If you use a self-signed certificate, the browser displays a warning. Check the certificate, and accept the security exception to proceed with the login. The console loads a certificate from the /etc/cockpit/ws-certs.d directory and uses the last file with a .cert extension in alphabetical order. To avoid having to grant security exceptions, install a certificate signed by a certificate authority (CA). In the login screen, enter your system user name and password. Click Log In . After successful authentication, the RHEL web console interface opens. Important To switch between limited and administrative access, click Administrative access or Limited access in the top panel of the web console page. You must provide your user password to gain administrative access. 1.4. Disabling basic authentication in the web console You can modify the behavior of an authentication scheme by modifying the cockpit.conf file. Use the none action to disable an authentication scheme and only allow authentication through GSSAPI and forms. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . You have root privileges or permissions to enter administrative commands with sudo . Procedure Open or create the cockpit.conf file in the /etc/cockpit/ directory in a text editor of your preference, for example: # vi cockpit.conf Add the following text: [basic] action = none Save the file. Restart the web console for changes to take effect. # systemctl try-restart cockpit 1.5. Connecting to the web console from a remote machine You can connect to your web console interface from any client operating system and also from mobile phones or tablets. Prerequisites A device with a supported internet browser, such as: Mozilla Firefox 52 and later Google Chrome 57 and later Microsoft Edge 16 and later The RHEL 9 you want to access with an installed and accessible web console. For instructions, see Installing and enabling the web console . Procedure Open your web browser. Type the remote server’s address in one of the following formats: With the server’s host name: https:// <server.hostname.example.com> : <port-number> For example: https://example.com:9090 With the server’s IP address: https:// <server.IP_address> : <port-number> For example: https://192.0.2.2:9090 After the login interface opens, log in with your RHEL system credentials. 1.6. Connecting to the web console from a remote machine as a root user On new installations of  or later, the RHEL web console disallows root account logins by default for security reasons. You can allow the root login in the /etc/cockpit/disallowed-users file. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Open the disallowed-users file in the /etc/cockpit/ directory in a text editor of your preference, for example: # vi /etc/cockpit/disallowed-users Edit the file and remove the line for the root user: # List of users which are not allowed to login to Cockpit root Save the changes and quit the editor. Verification Log in to the web console as a root user. For details, see Logging in to the web console . 1.7. Logging in to the web console using a one-time password If your system is part of an Identity Management (IdM) domain with enabled one-time password (OTP) configuration, you can use an OTP to log in to the RHEL web console. Important It is possible to log in using a one-time password only if your system is part of an Identity Management (IdM) domain with enabled OTP configuration. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . An Identity Management server with enabled OTP configuration. A configured hardware or software device generating OTP tokens. Procedure Open the RHEL web console in your browser: Locally: https://localhost:PORT_NUMBER Remotely with the server hostname: https://example.com:PORT_NUMBER Remotely with the server IP address: https://EXAMPLE.SERVER.IP.ADDR:PORT_NUMBER If you use a self-signed certificate, the browser issues a warning. Check the certificate and accept the security exception to proceed with the login. The console loads a certificate from the /etc/cockpit/ws-certs.d directory and uses the last file with a .cert extension in alphabetical order. To avoid having to grant security exceptions, install a certificate signed by a certificate authority (CA). The Login window opens. In the Login window, enter your system user name and password. Generate a one-time password on your device. Enter the one-time password into a new field that appears in the web console interface after you confirm your password. Click Log in . Successful login takes you to the Overview page of the web console interface. 1.8. Adding a banner to the login page You can set the web console to show a content of a banner file on the login screen. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . You have root privileges or permissions to enter administrative commands with sudo . Procedure Open the /etc/issue.cockpit file in a text editor of your preference: # vi /etc/issue.cockpit Add the content you want to display as the banner to the file, for example: This is an example banner for the RHEL web console login page. You cannot include any macros in the file, but you can use line breaks and ASCII art. Save the file. Open the cockpit.conf file in the /etc/cockpit/ directory in a text editor of your preference, for example: # vi /etc/cockpit/cockpit.conf Add the following text to the file: [Session] Banner=/etc/issue.cockpit Save the file. Restart the web console for changes to take effect. # systemctl try-restart cockpit Verification Open the web console login screen again to verify that the banner is now visible: 1.9. Configuring automatic idle lock in the web console You can enable the automatic idle lock and set the idle timeout for your system through the web console interface. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . You have root privileges or permissions to enter administrative commands with sudo . Procedure Open the cockpit.conf file in the /etc/cockpit/ directory in a text editor of your preference, for example: # vi /etc/cockpit/cockpit.conf Add the following text to the file: [Session] IdleTimeout= <X> Substitute <X> with a number for a time period of your choice in minutes. Save the file. Restart the web console for changes to take effect. # systemctl try-restart cockpit Verification Check if the session logs you out after a set period of time. 1.10. Changing the web console listening port By default, the RHEL web console communicates through TCP port 9090. You can change the port number by overriding the default socket settings. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . You have root privileges or permissions to enter administrative commands with sudo . The firewalld service is running. Procedure Pick an unoccupied port, for example, <4488/tcp> , and instruct SELinux to allow the cockpit service to bind to that port: # semanage port -a -t websm_port_t -p tcp <4488> Note that a port can be used only by one service at a time, and thus an attempt to use an already occupied port implies the ValueError: Port already defined error message. Open the new port and close the former one in the firewall: # firewall-cmd --service cockpit --permanent --add-port= <4488> /tcp # firewall-cmd --service cockpit --permanent --remove-port=9090/tcp Create an override file for the cockpit.socket service: # systemctl edit cockpit.socket In the following editor screen, which opens an empty override.conf file located in the /etc/systemd/system/cockpit.socket.d/ directory, change the default port for the web console from 9090 to the previously picked number by adding the following lines: [Socket] ListenStream= ListenStream= <4488> Note that the first ListenStream= directive with an empty value is intentional. You can declare multiple ListenStream directives in a single socket unit and the empty value in the drop-in file resets the list and disables the default port 9090 from the original unit. Important Insert the previous code snippet between the lines starting with # Anything between here and # Lines below this . Otherwise, the system discards your changes. Save the changes by pressing Ctrl + O and Enter . Exit the editor by pressing Ctrl + X . Reload the changed configuration: # systemctl daemon-reload Check that your configuration is working: # systemctl show cockpit.socket -p Listen Listen=[::]:4488 (Stream) Restart cockpit.socket : # systemctl restart cockpit.socket Verification Open your web browser, and access the web console on the updated port, for example: https://machine1.example.com:4488 Additional resources firewall-cmd(1) , semanage(8) , systemd.unit(5) , and systemd.socket(5) man pages on your system Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_systems_using_the_rhel_9_web_console/getting-started-with-the-rhel-9-web-console_system-management-using-the-rhel-9-web-console"}
{"title": "Chapter 1. Red Hat Certified, validated, and Ansible Galaxy content in automation hub", "content": "Chapter 1. Red Hat Certified, validated, and Ansible Galaxy content in automation hub Ansible Certified Content Collections are included in your subscription to Red Hat Ansible Automation Platform. Using Ansible automation hub, you can access and curate a unique set of collections from all forms of Ansible content. Red Hat Ansible content contains two types of content: Ansible Certified Content Collections Ansible validated content collections You can use both Ansible Certified Content Collections or Ansible validated content collections to build your automation library. For more information on the differences between Ansible Certified Content Collections and Ansible validated content collections, see the Knowledgebase article Ansible Certified Content Collections and Ansible validated content , or Ansible validated content in this guide. You can update these collections manually by downloading their packages. You can use Ansible automation hub to distribute the relevant Red Hat Ansible Certified Content Collections to your users by creating a requirements file or a synclist. Use a requirements file to install collections to your automation hub, as synclists can only be managed by users with platform administrator privileges. Before you can use a requirements file to install content, you must: Obtain an automation hub API token Use the API token to configure a remote repository in your local hub Then, Create a requirements file . 1.1. Configuring Ansible automation hub remote repositories to synchronize content Use remote configurations to configure your private automation hub to synchronize with Ansible Certified Content Collections hosted on console.redhat.com or with your collections in Ansible Galaxy. Important To synchronize content, you can now upload a manually-created requirements file from the rh-certified remote. Remotes are configurations that allow you to synchronize content to your custom repositories from an external collection source. As of the 2.4 release you can still synchronize content, but synclists are deprecated, and will be removed in a future version. Each remote configuration located in Automation Content Remotes provides information for both the community and rh-certified repository about when the repository was last updated . You can add new content to Ansible automation hub at any time using the Edit and Sync features included on the Automation Content Repositories page. What’s the difference between Ansible Galaxy and Ansible automation hub? Collections published to Ansible Galaxy are the latest content published by the Ansible community and have no joint support claims associated with them. Ansible Galaxy is the recommended frontend directory for the Ansible community to access content. Collections published to Ansible automation hub are targeted to joint customers of Red Hat and selected partners. Customers need an Ansible subscription to access and download collections on Ansible automation hub. A certified collection means that Red Hat and partners have a strategic relationship in place and are ready to support joint customers, and that the collections may have had additional testing and validation done against them. How do I request a namespace on Ansible Galaxy? To request a namespace through an Ansible Galaxy GitHub issue, follow these steps: Send an email to ansiblepartners@redhat.com Include the GitHub username used to sign up on Ansible Galaxy. You must have logged in at least once for the system to validate. After users are added as administrators of the namespace, you can use the self-serve process to add more administrators. Are there any restrictions for Ansible Galaxy namespace naming? Collection namespaces must follow Python module name convention. This means collections should have short, all lowercase names. You can use underscores in the collection name if it improves readability. 1.1.1. Token management in automation hub Before you can interact with automation hub by uploading or downloading collections, you must create an API token. The automation hub API token authenticates your ansible-galaxy client to the Red Hat automation hub server. Note automation hub does not support basic authentication or authenticating through service accounts. You must authenticate using token management. Your method for creating the API token differs according to the type of automation hub that you are using: Automation hub uses offline token management. See Creating the offline token in automation hub . Private automation hub uses API token management. See Creating the API token in private automation hub . If you are using Keycloak to authenticate your private automation hub, follow the procedure for Creating the offline token in automation hub . 1.1.1.1. Creating the offline token in automation hub In automation hub, you can create an offline token by using Token management . The offline token is a secret token used to protect your content. Procedure Navigate to Ansible Automation Platform on the Red Hat Hybrid Cloud Console . From the navigation panel, select Automation Hub Connect to Hub . Under Offline token , click Load Token . Click the Copy to clipboard icon to copy the offline token. Paste the API token into a file and store in a secure location. Important The offline token is a secret token used to protect your content. Store your token in a secure location. The offline token is now available for configuring automation hub as your default collections server or for uploading collections by using the ansible-galaxy command line tool. Note Your offline token expires after 30 days of inactivity. For more on obtaining a new offline token, see Keeping your offline token active . 1.1.1.2. Creating the API token in private automation hub In private automation hub, you can create an API token using API token management. The API token is a secret token used to protect your content. Prerequisites Valid subscription credentials for Red Hat Ansible Automation Platform. Procedure Log in to your private automation hub. From the navigation panel, select Automation Content API token . Click Load Token . To copy the API token, click the Copy to clipboard icon. Paste the API token into a file and store in a secure location. Important The API token is a secret token used to protect your content. Store your API token in a secure location. The API token is now available for configuring automation hub as your default collections server or uploading collections using the ansible-galaxy command line tool. Note The API token does not expire. 1.1.1.3. Keeping your offline token active Offline tokens expire after 30 days of inactivity. You can keep your offline token from expiring by periodically refreshing your offline token. Keeping an online token active is useful when an application performs an action on behalf of the user; for example, this allows the application to perform a routine data backup when the user is offline. Note If your offline token expires, you must obtain a new one . Procedure Run the following command to prevent your token from expiring: curl https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token -d grant_type=refresh_token -d client_id=\"cloud-services\" -d refresh_token=\"{{ user_token }}\" --fail --silent --show-error --output /dev/null 1.1.2. Configuring the rh-certified remote repository and synchronizing Red Hat Ansible Certified Content Collection You can edit the rh-certified remote repository to synchronize collections from automation hub hosted on console.redhat.com to your private automation hub. By default, your private automation hub rh-certified repository includes the URL for the entire group of Ansible Certified Content Collections. To use only those collections specified by your organization, a private automation hub administrator can upload manually-created requirements files from the rh-certified remote. If you have collections A , B , and C in your requirements file, and a new collection X is added to console.redhat.com that you want to use, you must add X to your requirements file for private automation hub to synchronize it. Prerequisites You have valid Modify Ansible repo content permissions. For more information on permissions, see Access management and authentication . You have retrieved the Sync URL and API Token from the automation hub hosted service on console.redhat.com. You have configured access to port 443. This is required for synchronizing certified collections. For more information, see the automation hub table in the Network ports and protocols chapter of Planning your installation. Procedure Log in to your Ansible Automation Platform. From the navigation panel, select Automation Content Remotes . In the rh-certified remote repository, click Edit remote . In the URL field, paste the Sync URL . In the Token field, paste the token you acquired from console.redhat.com. Click Save remote . You can now synchronize collections between your organization synclist on console.redhat.com and your private automation hub. From the navigation panel, select Automation Content Repositories . Next to rh-certified click the More Actions icon ⋮ and select Sync repository . On the modal that appears, you can toggle the following options: Mirror : Select if you want your repository content to mirror the remote repository’s content. Optimize : Select if you want to sync only when no changes are reported by the remote server. Click Sync to complete the sync. Verification The Sync status column updates to notify you whether the Red Hat Certified Content Collections synchronization is successful. Navigate to Automation Content Collections to confirm that your collections content has synchronized successfully. 1.1.3. Configuring the community remote repository and syncing Ansible Galaxy collections You can edit the community remote repository to synchronize chosen collections from Ansible Galaxy to your private automation hub. By default, your private automation hub community repository directs to galaxy.ansible.com/api/ . Prerequisites You have Modify Ansible repo content permissions. For more information on permissions, see Access management and authentication . You have a requirements.yml file that identifies those collections to synchronize from Ansible Galaxy as in the following example: Requirements.yml example collections: # Install a collection from Ansible Galaxy. - name: community.aws version: 5.2.0 source: https://galaxy.ansible.com Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Content Remotes . In the Details tab in the Community remote, click Edit remote . In the YAML requirements field, paste the contents of your requirements.yml file. Click Save remote . You can now synchronize collections identified in your requirements.yml file from Ansible Galaxy to your private automation hub. From the navigation panel, select Automation Content Repositories . Next to the community repository, click the More Actions icon ⋮ and select Sync repository to sync collections between Ansible Galaxy and Ansible automation hub. On the modal that appears, you can toggle the following options: Mirror : Select if you want your repository content to mirror the remote repository’s content. Optimize : Select if you want to sync only when no changes are reported by the remote server. Click Sync to complete the sync. Verification The Sync status column updates to notify you whether the Ansible Galaxy collections synchronization to your Ansible automation hub is successful. Navigate to Automation Content Collections and select Community to confirm successful synchronization. 1.1.4. Configuring proxy settings If your private automation hub is behind a network proxy, you can configure proxy settings on the remote to sync content located outside of your local network. Prerequisites You have valid Modify Ansible repo content permissions. For more information on permissions, see Access management and authentication You have a proxy URL and credentials from your local network administrator. Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Content Remotes . In either the rh-certified or Community remote, click the More Actions icon ⋮ and select Edit remote . Expand the Show advanced options drop-down menu. Enter your proxy URL, proxy username, and proxy password in the appropriate fields. Click Save remote . 1.1.5. Creating a requirements file Use a requirements file to add collections to your automation hub. Requirements files are in YAML format and list the collections that you want to install in your automation hub. After you create your requirements.yml file listing the collections you want to install, you will then run the install command to add the collections to your hub instance. A standard requirements.yml file contains the following parameters: name : the name of the collection formatted as <namespace>.<collection_name> version : the collection version number Procedure Create your requirements file. In YAML format, collection information in your requirements file should look like this: collections: name: namespace.collection_name version: 1.0.0 After you have created your requirements file listing information for each collection that you want to install, navigate to the directory where the file is located and run the following command: $ ansible-galaxy collection install -r requirements.yml 1.1.5.1. Installing an individual collection from the command line To install an individual collection to your automation hub, run the following command: $ ansible-galaxy collection install namespace.collection_name 1.2. Synchronizing Ansible Content Collections in automation hub Important To synchronize content, you can now upload a manually-created requirements file from the rh-certified remote. Remotes are configurations that enable you to synchronize content to your custom repositories from an external collection source. As of the 2.4 release you can still synchronize content, but synclists are deprecated, and will be removed in a future version. 1.2.1. Explanation of Red Hat Ansible Certified Content Collections synclists A synclist is a curated group of Red Hat Certified Collections assembled by your organization administrator. It synchronizes with your local Ansible automation hub. Use synclists to manage only the content that you want and exclude unnecessary collections. Design and manage your synclist from the content available as part of Red Hat content on console.redhat.com Each synclist has its own unique repository URL that you can designate as a remote source for content in automation hub. You securely access each synclist by using an API token. 1.2.2. Creating a synclist of Red Hat Ansible Certified Content Collections You can create a synclist of curated Red Hat Ansible Certified Content in Ansible automation hub on console.redhat.com. Your synclist repository is located on the automation hub navigation panel under Automation Content Repositories , which is updated whenever you manage content within Ansible Certified Content Collections. All Ansible Certified Content Collections are included by default in your initial organization synclist. Prerequisites You have a valid Ansible Automation Platform subscription. You have organization administrator permissions for console.redhat.com. The following domain names are part of either the firewall or the proxy’s allowlist. They are required for successful connection and download of collections from automation hub or Galaxy server: galaxy.ansible.com cloud.redhat.com console.redhat.com sso.redhat.com Ansible automation hub resources are stored in Amazon Simple Storage. The following domain names must be in the allow list: automation-hub-prd.s3.us-east-2.amazonaws.com ansible-galaxy.s3.amazonaws.com SSL inspection is disabled either when using self signed certificates or for the Red Hat domains. Procedure Log in to console.redhat.com . Navigate to Automation Hub Collections . Set the Sync toggle switch on each collection to exclude or include it on your synclist. Note You will only see the Sync toggle switch if you have administrator permissions. To initiate the remote repository synchronization, navigate to your Ansible Automation Platform and select Automation Content Repositories . In the row containing the repository you want to sync, click the More Actions icon ⋮ and select Sync repository to initiate the remote repository synchronization to your private automation hub. Optional: If your remote repository is already configured, update the collections content that you made available to local users by manually synchronizing Red Hat Ansible Certified Content Collections to your private automation hub. 1.3. Collections and content signing in private automation hub As an automation administrator for your organization, you can configure private automation hub for signing and publishing Ansible content collections from different groups within your organization. For additional security, automation creators can configure Ansible-Galaxy CLI to verify these collections to ensure that they have not been changed after they were uploaded to automation hub. 1.3.1. Configuring content signing on private automation hub To successfully sign and publish Ansible Certified Content Collections, you must configure private automation hub for signing. Prerequisites Your GnuPG key pairs have been securely set up and managed by your organization. Your public-private key pair has proper access for configuring content signing on private automation hub. Procedure Create a signing script that accepts only a filename. Note This script acts as the signing service and must generate an ascii-armored detached gpg signature for that file using the key specified through the PULP_SIGNING_KEY_FINGERPRINT environment variable. The script prints out a JSON structure with the following format. {\"file\": \"filename\", \"signature\": \"filename.asc\"} All the file names are relative paths inside the current working directory. The file name must remain the same for the detached signature. Example: The following script produces signatures for content: #!/usr/bin/env bash FILE_PATH=$1 SIGNATURE_PATH=\"$1.asc\" ADMIN_ID=\"$PULP_SIGNING_KEY_FINGERPRINT\" PASSWORD=\"password\" # Create a detached signature gpg --quiet --batch --pinentry-mode loopback --yes --passphrase \\ $PASSWORD --homedir ~/.gnupg/ --detach-sign --default-key $ADMIN_ID \\ --armor --output $SIGNATURE_PATH $FILE_PATH # Check the exit status STATUS=$? if [ $STATUS -eq 0 ]; then echo {\\\"file\\\": \\\"$FILE_PATH\\\", \\\"signature\\\": \\\"$SIGNATURE_PATH\\\"} else exit $STATUS fi After you deploy a private automation hub with signing enabled to your Ansible Automation Platform cluster, new UI additions are displayed in collections. Review the Ansible Automation Platform installer inventory file for options that begin with automationhub_* . [all:vars] . . . automationhub_create_default_collection_signing_service = True automationhub_auto_sign_collections = True automationhub_require_content_approval = True automationhub_collection_signing_service_key = /abs/path/to/galaxy_signing_service.gpg automationhub_collection_signing_service_script = /abs/path/to/collection_signing.sh The two new keys ( automationhub_auto_sign_collections and automationhub_require_content_approval ) indicate that the collections must be signed and approved after they are uploaded to private automation hub. 1.3.2. Using content signing services in private automation hub After you have configured content signing on your private automation hub, you can manually sign a new collection or replace an existing signature with a new one. When users download a specific collection, this signature indicates that the collection is for them and has not been modified after certification. You can use content signing on private automation hub in the following scenarios: Your system does not have automatic signing configured and you must use a manual signing process to sign collections. The current signatures on the automatically configured collections are corrupted and need new signatures. You need additional signatures for previously signed content. You want to rotate signatures on your collections. Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Content Collection Approvals . The Approval dashboard opens and displays a list of collections. Click the thumbs up icon next to the collection you want to approve. On the modal that appears, check the box confirming that you want to approve the collection, and click Approve and sign collections . Verification Navigate to Automation Content Collections to verify that the collections you signed and approved are displayed. 1.3.3. Downloading signature public keys After you sign and approve collections, download the signature public keys from the Ansible Automation Platform UI. You must download the public key before you add it to the local system keyring. Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Content Signature Keys . The Signature Keys dashboard displays a list of multiple keys: collections and container images. To verify collections, download the key prefixed with collections- . To verify container images, download the key prefixed with container- . Choose one of the following methods to download your public key: Click the Download Key icon to download the public key. Click the Copy to clipboard next to the public key you want to copy. Use the public key that you copied to verify the content collection that you are installing. 1.3.4. Configuring Ansible-Galaxy CLI to verify collections You can configure Ansible-Galaxy CLI to verify collections. This ensures that downloaded collections are approved by your organization and have not been changed after they were uploaded to automation hub. If a collection has been signed by automation hub, the server provides ASCII armored, GPG-detached signatures to verify the authenticity of MANIFEST.json before using it to verify the collection’s contents. You must opt into signature verification by configuring a keyring for ansible-galaxy or providing the path with the --keyring option. Prerequisites Signed collections are available in automation hub to verify signature. Certified collections can be signed by approved roles within your organization. Public key for verification has been added to the local system keyring. Procedure To import a public key into a non-default keyring for use with ansible-galaxy , run the following command. gpg --import --no-default-keyring --keyring ~/.ansible/pubring.kbx my-public-key.asc Note In addition to any signatures provided by automation hub, signature sources can also be provided in the requirements file and on the command line. Signature sources should be URIs. To verify the collection name provided on the CLI with an additional signature, run the following command: ansible-galaxy collection install namespace.collection --signature https://examplehost.com/detached_signature.asc --signature file:///path/to/local/detached_signature.asc --keyring ~/.ansible/pubring.kbx You can use this option multiple times to provide multiple signatures. Confirm that the collections in a requirements file list any additional signature sources following the collection’s signatures key, as in the following example. # requirements.yml collections: - name: ns.coll version: 1.0.0 signatures: - https://examplehost.com/detached_signature.asc - file:///path/to/local/detached_signature.asc ansible-galaxy collection verify -r requirements.yml --keyring ~/.ansible/pubring.kbx When you install a collection from automation hub, the signatures provided by the server are saved along with the installed collections to verify the collection’s authenticity. (Optional) If you need to verify the internal consistency of your collection again without querying the Ansible Galaxy server, run the same command you used previously using the --offline option. Are there any recommendations for collection naming? Create a collection with company_name.product format. This format means that multiple products can have different collections under the company namespace. How do I get a namespace on automation hub? By default namespaces used on Ansible Galaxy are also used on automation hub by the Ansible partner team. For any queries and clarifications contact ansiblepartners@redhat.com . 1.4. Ansible validated content Red Hat Ansible Automation Platform includes Ansible validated content, which complements existing Red Hat Ansible Certified Content. Ansible validated content provides an expert-led path for performing operational tasks on a variety of platforms from both Red Hat and our trusted partners. 1.4.1. Configuring validated collections with the installer When you download and run the RPM bundle installer, certified and validated collections are automatically uploaded. Certified collections are uploaded into the rh-certified repository. Validated collections are uploaded into the validated repository. You can change the default configuration by using two variables: automationhub_seed_collections is a boolean that defines whether or not preloading is enabled. automationhub_collection_seed_repository`is a variable that enables you to specify the type of content to upload when it is set to `true . Possible values are certified or validated . If this variable is missing, both content sets will be uploaded. Note Changing the default configuration may require further platform configuration changes for other content you may use. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/managing_automation_content/managing-cert-valid-content"}
{"title": "Managing automation content", "content": "Managing automation content Red Hat Ansible Automation Platform 2.5 Create and manage collections, content and repositories in automation hub Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html-single/managing_automation_content/index"}
{"title": "Creating and using execution environments", "content": "Creating and using execution environments Red Hat Ansible Automation Platform 2.5 Create and use execution environment containers Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/creating_and_using_execution_environments/index"}
{"title": "GitOps", "content": "GitOps  A declarative way to implement continuous deployment for cloud native applications. Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/gitops/index"}
{"title": "Chapter 12. Grouping LVM objects with tags", "content": "Chapter 12. Grouping LVM objects with tags You can assign tags to Logical Volume Manager (LVM) objects to group them. With this feature, you can automate the control of LVM behavior, such as activation, by a group. You can also use tags instead of LVM objects arguments. 12.1. LVM object tags A Logical Volume Manager (LVM) tag groups LVM objects of the same type. You can attach tags to objects such as physical volumes, volume groups, and logical volumes . To avoid ambiguity, prefix each tag with @ . Each tag is expanded by replacing it with all the objects that possess that tag and that are of the type expected by its position on the command line. LVM tags are strings of up to 1024 characters. LVM tags cannot start with a hyphen. A valid tag consists of a limited range of characters only. The allowed characters are A-Z a-z 0-9 _ + . - / = ! : # & . Only objects in a volume group can be tagged. Physical volumes lose their tags if they are removed from a volume group; this is because tags are stored as part of the volume group metadata and that is deleted when a physical volume is removed. You can apply some commands to all volume groups (VG), logical volumes (LV), or physical volumes (PV) that have the same tag. The man page of the given command shows the syntax, such as VG|Tag , LV|Tag , or PV|Tag when you can substitute a tag name for a VG, LV, or PV name. 12.2. Adding tags to LVM objects You can add tags to LVM objects to group them by using the --addtag option with various volume management commands. Prerequisites The lvm2 package is installed. Procedure To add a tag to an existing PV, use: # pvchange --addtag <@tag> <PV> To add a tag to an existing VG, use: # vgchange --addtag <@tag> <VG> To add a tag to a VG during creation, use: # vgcreate --addtag <@tag> <VG> To add a tag to an existing LV, use: # lvchange --addtag <@tag> <LV> To add a tag to a LV during creation, use: # lvcreate --addtag <@tag> ... 12.3. Removing tags from LVM objects If you no longer want to keep your LVM objects grouped, you can remove tags from the objects by using the --deltag option with various volume management commands. Prerequisites The lvm2 package is installed. You have created tags on physical volumes (PV), volume groups (VG), or logical volumes (LV). Procedure To remove a tag from an existing PV, use: # pvchange --deltag @tag PV To remove a tag from an existing VG, use: # vgchange --deltag @tag VG To remove a tag from an existing LV, use: # lvchange --deltag @tag LV 12.4. Displaying tags on LVM objects You can display tags on your LVM objects with the following commands. Prerequisites The lvm2 package is installed. You have created tags on physical volumes (PV), volume groups (VG), or logical volumes (LV). Procedure To display all tags on an existing PV, use: # pvs -o tags <PV> To display all tags on an existing VG, use: # vgs -o tags <VG> To display all tags on an existing LV, use: # lvs -o tags <LV> 12.5. Controlling logical volume activation with tags This procedure describes how to specify in the configuration file that only certain logical volumes should be activated on that host. Procedure For example, the following entry acts as a filter for activation requests (such as vgchange -ay ) and only activates vg1/lvol0 and any logical volumes or volume groups with the database tag in the metadata on that host: activation { volume_list = [\"vg1/lvol0\", \"@database\" ] } The special match @* that causes a match only if any metadata tag matches any host tag on that machine. As another example, consider a situation where every machine in the cluster has the following entry in the configuration file: tags { hosttags = 1 } If you want to activate vg1/lvol2 only on host db2 , do the following: Run lvchange --addtag @db2 vg1/lvol2 from any host in the cluster. Run lvchange -ay vg1/lvol2 . This solution involves storing host names inside the volume group metadata. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/grouping-lvm-objects-with-tags_configuring-and-managing-logical-volumes"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/legal-notice"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/observability/legal-notice"}
{"title": "Chapter 13. Troubleshooting LVM", "content": "Chapter 13. Troubleshooting LVM You can use Logical Volume Manager (LVM) tools to troubleshoot a variety of issues in LVM volumes and groups. 13.1. Gathering diagnostic data on LVM If an LVM command is not working as expected, you can gather diagnostics in the following ways. Procedure Use the following methods to gather different kinds of diagnostic data: Add the -v argument to any LVM command to increase the verbosity level of the command output. Verbosity can be further increased by adding additional v’s . A maximum of four such v’s is allowed, for example, -vvvv . In the log section of the /etc/lvm/lvm.conf configuration file, increase the value of the level option. This causes LVM to provide more details in the system log. If the problem is related to the logical volume activation, enable LVM to log messages during the activation: Set the activation = 1 option in the log section of the /etc/lvm/lvm.conf configuration file. Execute the LVM command with the -vvvv option. Examine the command output. Reset the activation option to 0 . If you do not reset the option to 0 , the system might become unresponsive during low memory situations. Display an information dump for diagnostic purposes: # lvmdump Display additional system information: # lvs -v # pvs --all # dmsetup info --columns Examine the last backup of the LVM metadata in the /etc/lvm/backup/ directory and archived versions in the /etc/lvm/archive/ directory. Check the current configuration information: # lvmconfig Check the /run/lvm/hints cache file for a record of which devices have physical volumes on them. Additional resources lvmdump(8) man page on your system 13.2. Displaying information about failed LVM devices Troubleshooting information about a failed Logical Volume Manager (LVM) volume can help you determine the reason of the failure. You can check the following examples of the most common LVM volume failures. Example 13.1. Failed volume groups In this example, one of the devices that made up the volume group myvg failed. The volume group usability then depends on the type of failure. For example, the volume group is still usable if RAID volumes are also involved. You can also see information about the failed device. # vgs --options +devices /dev/vdb1: open failed: No such device or address /dev/vdb1: open failed: No such device or address WARNING: Couldn't find device with uuid 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s. WARNING: VG myvg is missing PV 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s (last written to /dev/sdb1). WARNING: Couldn't find all devices for LV myvg/mylv while checking used and assumed devices. VG #PV #LV #SN Attr VSize VFree Devices myvg 2 2 0 wz-pn- <3.64t <3.60t [unknown](0) myvg 2 2 0 wz-pn- <3.64t <3.60t [unknown](5120),/dev/vdb1(0) Example 13.2. Failed logical volume In this example, one of the devices failed. This can be a reason for the logical volume in the volume group to fail. The command output shows the failed logical volumes. # lvs --all --options +devices /dev/vdb1: open failed: No such device or address /dev/vdb1: open failed: No such device or address WARNING: Couldn't find device with uuid 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s. WARNING: VG myvg is missing PV 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s (last written to /dev/sdb1). WARNING: Couldn't find all devices for LV myvg/mylv while checking used and assumed devices. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert Devices mylv myvg -wi-a---p- 20.00g [unknown](0) [unknown](5120),/dev/sdc1(0) Example 13.3. Failed image of a RAID logical volume The following examples show the command output from the pvs and lvs utilities when an image of a RAID logical volume has failed. The logical volume is still usable. # pvs Error reading device /dev/sdc1 at 0 length 4. Error reading device /dev/sdc1 at 4096 length 4. Couldn't find device with uuid b2J8oD-vdjw-tGCA-ema3-iXob-Jc6M-TC07Rn. WARNING: Couldn't find all devices for LV myvg/my_raid1_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV myvg/my_raid1_rmeta_1 while checking used and assumed devices. PV VG Fmt Attr PSize PFree /dev/sda2 rhel_bp-01 lvm2 a-- <464.76g 4.00m /dev/sdb1 myvg lvm2 a-- <836.69g 736.68g /dev/sdd1 myvg lvm2 a-- <836.69g <836.69g /dev/sde1 myvg lvm2 a-- <836.69g <836.69g [unknown] myvg lvm2 a-m <836.69g 736.68g # lvs -a --options name,vgname,attr,size,devices myvg Couldn't find device with uuid b2J8oD-vdjw-tGCA-ema3-iXob-Jc6M-TC07Rn. WARNING: Couldn't find all devices for LV myvg/my_raid1_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV myvg/my_raid1_rmeta_1 while checking used and assumed devices. LV VG Attr LSize Devices my_raid1 myvg rwi-a-r-p- 100.00g my_raid1_rimage_0(0),my_raid1_rimage_1(0) [my_raid1_rimage_0] myvg iwi-aor--- 100.00g /dev/sdb1(1) [my_raid1_rimage_1] myvg Iwi-aor-p- 100.00g [unknown](1) [my_raid1_rmeta_0] myvg ewi-aor--- 4.00m /dev/sdb1(0) [my_raid1_rmeta_1] myvg ewi-aor-p- 4.00m [unknown](0) 13.3. Removing lost LVM physical volumes from a volume group If a physical volume fails, you can activate the remaining physical volumes in the volume group and remove all the logical volumes that used that physical volume from the volume group. Procedure Activate the remaining physical volumes in the volume group: # vgchange --activate y --partial myvg Check which logical volumes will be removed: # vgreduce --removemissing --test myvg Remove all the logical volumes that used the lost physical volume from the volume group: # vgreduce --removemissing --force myvg Optional: If you accidentally removed logical volumes that you wanted to keep, you can reverse the vgreduce operation: # vgcfgrestore myvg Warning If you remove a thin pool, LVM cannot reverse the operation. 13.4. Finding the metadata of a missing LVM physical volume If the volume group’s metadata area of a physical volume is accidentally overwritten or otherwise destroyed, you get an error message indicating that the metadata area is incorrect, or that the system was unable to find a physical volume with a particular UUID. This procedure finds the latest archived metadata of a physical volume that is missing or corrupted. Procedure Find the archived metadata file of the volume group that contains the physical volume. The archived metadata files are located at the /etc/lvm/archive/ volume-group-name_backup-number .vg path: # cat /etc/lvm/archive/ myvg_00000-1248998876 .vg Replace 00000-1248998876 with the backup-number. Select the last known valid metadata file, which has the highest number for the volume group. Find the UUID of the physical volume. Use one of the following methods. List the logical volumes: # lvs --all --options +devices Couldn't find device with uuid ' FmGRh3-zhok-iVI8-7qTD-S5BI-MAEN-NYM5Sk '. Examine the archived metadata file. Find the UUID as the value labeled id = in the physical_volumes section of the volume group configuration. Deactivate the volume group using the --partial option: # vgchange --activate n --partial myvg PARTIAL MODE. Incomplete logical volumes will be processed. WARNING: Couldn't find device with uuid 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s . WARNING: VG myvg is missing PV 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s (last written to /dev/vdb1 ). 0 logical volume(s) in volume group \" myvg \" now active 13.5. Restoring metadata on an LVM physical volume This procedure restores metadata on a physical volume that is either corrupted or replaced with a new device. You might be able to recover the data from the physical volume by rewriting the metadata area on the physical volume. Warning Do not attempt this procedure on a working LVM logical volume. You will lose your data if you specify the incorrect UUID. Prerequisites You have identified the metadata of the missing physical volume. For details, see Finding the metadata of a missing LVM physical volume . Procedure Restore the metadata on the physical volume: # pvcreate --uuid physical-volume-uuid \\ --restorefile /etc/lvm/archive/ volume-group-name_backup-number .vg \\ block-device Note The command overwrites only the LVM metadata areas and does not affect the existing data areas. Example 13.4. Restoring a physical volume on /dev/vdb1 The following example labels the /dev/vdb1 device as a physical volume with the following properties: The UUID of FmGRh3-zhok-iVI8-7qTD-S5BI-MAEN-NYM5Sk The metadata information contained in VG_00050.vg , which is the most recent good archived metadata for the volume group # pvcreate --uuid \"FmGRh3-zhok-iVI8-7qTD-S5BI-MAEN-NYM5Sk\" \\ --restorefile /etc/lvm/archive/VG_00050.vg \\ /dev/vdb1 ... Physical volume \"/dev/vdb1\" successfully created Restore the metadata of the volume group: # vgcfgrestore myvg Restored volume group myvg Display the logical volumes on the volume group: # lvs --all --options +devices myvg The logical volumes are currently inactive. For example: LV VG Attr LSize Origin Snap% Move Log Copy% Devices mylv myvg -wi--- 300.00G /dev/vdb1 (0),/dev/vdb1(0) mylv myvg -wi--- 300.00G /dev/vdb1 (34728),/dev/vdb1(0) If the segment type of the logical volumes is RAID, resynchronize the logical volumes: # lvchange --resync myvg/mylv Activate the logical volumes: # lvchange --activate y myvg/mylv If the on-disk LVM metadata takes at least as much space as what overrode it, this procedure can recover the physical volume. If what overrode the metadata went past the metadata area, the data on the volume may have been affected. You might be able to use the fsck command to recover that data. Verification Display the active logical volumes: # lvs --all --options +devices LV VG Attr LSize Origin Snap% Move Log Copy% Devices mylv myvg -wi--- 300.00G /dev/vdb1 (0),/dev/vdb1(0) mylv myvg -wi--- 300.00G /dev/vdb1 (34728),/dev/vdb1(0) 13.6. Rounding errors in LVM output LVM commands that report the space usage in volume groups round the reported number to 2 decimal places to provide human-readable output. This includes the vgdisplay and vgs utilities. As a result of the rounding, the reported value of free space might be larger than what the physical extents on the volume group provide. If you attempt to create a logical volume the size of the reported free space, you might get the following error: Insufficient free extents To work around the error, you must examine the number of free physical extents on the volume group, which is the accurate value of free space. You can then use the number of extents to create the logical volume successfully. 13.7. Preventing the rounding error when creating an LVM volume When creating an LVM logical volume, you can specify the number of logical extents of the logical volume to avoid rounding error. Procedure Find the number of free physical extents in the volume group: # vgdisplay myvg Example 13.5. Free extents in a volume group For example, the following volume group has 8780 free physical extents: --- Volume group --- VG Name myvg System ID Format lvm2 Metadata Areas 4 Metadata Sequence No 6 VG Access read/write [...] Free PE / Size 8780 / 34.30 GB Create the logical volume. Enter the volume size in extents rather than bytes. Example 13.6. Creating a logical volume by specifying the number of extents # lvcreate --extents 8780 --name mylv myvg Example 13.7. Creating a logical volume to occupy all the remaining space Alternatively, you can extend the logical volume to use a percentage of the remaining free space in the volume group. For example: # lvcreate --extents 100%FREE --name mylv myvg Verification Check the number of extents that the volume group now uses: # vgs --options +vg_free_count,vg_extent_count VG #PV #LV #SN Attr VSize VFree Free #Ext myvg 2 1 0 wz--n- 34.30G 0 0 8780 13.8. LVM metadata and their location on disk LVM headers and metadata areas are available in different offsets and sizes. The default LVM disk header: Is found in label_header and pv_header structures. Is in the second 512-byte sector of the disk. Note that if a non-default location was specified when creating the physical volume (PV), the header can also be in the first or third sector. The standard LVM metadata area: Begins 4096 bytes from the start of the disk. Ends 1 MiB from the start of the disk. Begins with a 512 byte sector containing the mda_header structure. A metadata text area begins after the mda_header sector and goes to the end of the metadata area. LVM VG metadata text is written in a circular fashion into the metadata text area. The mda_header points to the location of the latest VG metadata within the text area. You can print LVM headers from a disk by using the # pvck --dump headers /dev/sda command. This command prints label_header , pv_header , mda_header , and the location of metadata text if found. Bad fields are printed with the CHECK prefix. The LVM metadata area offset will match the page size of the machine that created the PV, so the metadata area can also begin 8K, 16K or 64K from the start of the disk. Larger or smaller metadata areas can be specified when creating the PV, in which case the metadata area may end at locations other than 1 MiB. The pv_header specifies the size of the metadata area. When creating a PV, a second metadata area can be optionally enabled at the end of the disk. The pv_header contains the locations of the metadata areas. 13.9. Extracting VG metadata from a disk Choose one of the following procedures to extract VG metadata from a disk, depending on your situation. For information about how to save extracted metadata, see Saving extracted metadata to a file . Note For repair, you can use backup files in /etc/lvm/backup/ without extracting metadata from disk. Procedure Print current metadata text as referenced from valid mda_header : # pvck --dump metadata <disk> Example 13.8. Metadata text from valid mda_header # pvck --dump metadata /dev/sdb metadata text at 172032 crc Oxc627522f # vgname test segno 59 --- <raw metadata from disk> --- Print the locations of all metadata copies found in the metadata area, based on finding a valid mda_header : # pvck --dump metadata_all <disk> Example 13.9. Locations of metadata copies in the metadata area # pvck --dump metadata_all /dev/sdb metadata at 4608 length 815 crc 29fcd7ab vg test seqno 1 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 5632 length 1144 crc 50ea61c3 vg test seqno 2 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 7168 length 1450 crc 5652ea55 vg test seqno 3 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv Search for all copies of metadata in the metadata area without using an mda_header , for example, if headers are missing or damaged: # pvck --dump metadata_search <disk> Example 13.10. Copies of metadata in the metadata area without using an mda_header # pvck --dump metadata_search /dev/sdb Searching for metadata at offset 4096 size 1044480 metadata at 4608 length 815 crc 29fcd7ab vg test seqno 1 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 5632 length 1144 crc 50ea61c3 vg test seqno 2 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 7168 length 1450 crc 5652ea55 vg test seqno 3 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv Include the -v option in the dump command to show the description from each copy of metadata: # pvck --dump metadata -v <disk> Example 13.11. Showing description from each copy of metadata # pvck --dump metadata -v /dev/sdb metadata text at 199680 crc 0x628cf243 # vgname my_vg seqno 40 --- my_vg { id = \"dmEbPi-gsgx-VbvS-Uaia-HczM-iu32-Rb7iOf\" seqno = 40 format = \"lvm2\" status = [\"RESIZEABLE\", \"READ\", \"WRITE\"] flags = [] extent_size = 8192 max_lv = 0 max_pv = 0 metadata_copies = 0 physical_volumes { pv0 { id = \"8gn0is-Hj8p-njgs-NM19-wuL9-mcB3-kUDiOQ\" device = \"/dev/sda\" device_id_type = \"sys_wwid\" device_id = \"naa.6001405e635dbaab125476d88030a196\" status = [\"ALLOCATABLE\"] flags = [] dev_size = 125829120 pe_start = 8192 pe_count = 15359 } pv1 { id = \"E9qChJ-5ElL-HVEp-rc7d-U5Fg-fHxL-2QLyID\" device = \"/dev/sdb\" device_id_type = \"sys_wwid\" device_id = \"naa.6001405f3f9396fddcd4012a50029a90\" status = [\"ALLOCATABLE\"] flags = [] dev_size = 125829120 pe_start = 8192 pe_count = 15359 } This file can be used for repair. The first metadata area is used by default for dump metadata. If the disk has a second metadata area at the end of the disk, you can use the --settings \"mda_num=2\" option to use the second metadata area for dump metadata instead. 13.10. Saving extracted metadata to a file If you need to use dumped metadata for repair, it is required to save extracted metadata to a file with the -f option and the --setings option. Procedure If -f <filename> is added to --dump metadata , the raw metadata is written to the named file. You can use this file for repair. If -f <filename> is added to --dump metadata_all or --dump metadata_search , then raw metadata from all locations is written to the named file. To save one instance of metadata text from --dump metadata_all|metadata_search add --settings \"metadata_offset=<offset>\" where <offset> is from the listing output \"metadata at <offset>\". Example 13.12. Output of the command # pvck --dump metadata_search --settings metadata_offset=5632 -f meta.txt /dev/sdb Searching for metadata at offset 4096 size 1044480 metadata at 5632 length 1144 crc 50ea61c3 vg test seqno 2 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv # head -2 meta.txt test { id = \"FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv\" 13.11. Repairing a disk with damaged LVM headers and metadata using the pvcreate and the vgcfgrestore commands You can restore metadata and headers on a physical volume that is either corrupted or replaced with a new device. You might be able to recover the data from the physical volume by rewriting the metadata area on the physical volume. Warning These instructions should be used with extreme caution, and only if you are familiar with the implications of each command, the current layout of the volumes, the layout that you need to achieve, and the contents of the backup metadata file. These commands have the potential to corrupt data, and as such, it is recommended that you contact Red Hat Global Support Services for assistance in troubleshooting. Prerequisites You have identified the metadata of the missing physical volume. For details, see Finding the metadata of a missing LVM physical volume . Procedure Collect the following information needed for the pvcreate and vgcfgrestore commands. You can collect the information about your disk and UUID by running the # pvs -o+uuid command. metadata-file is the path to the most recent metadata backup file for the VG, for example, /etc/lvm/backup/ <vg-name> vg-name is the name of the VG that has the damaged or missing PV. UUID of the PV that was damaged on this device is the value taken from the output of the # pvs -i+uuid command. disk is the name of the disk where the PV is supposed to be, for example, /dev/sdb . Be certain this is the correct disk, or seek help, otherwise following these steps may lead to data loss. Recreate LVM headers on the disk: # pvcreate --restorefile <metadata-file> --uuid <UUID> <disk> Optionally, verify that the headers are valid: # pvck --dump headers <disk> Restore the VG metadata on the disk: # vgcfgrestore --file <metadata-file> <vg-name> Optionally, verify the metadata is restored: # pvck --dump metadata <disk> If there is no metadata backup file for the VG, you can get one by using the procedure in Saving extracted metadata to a file . Verification To verify that the new physical volume is intact and the volume group is functioning correctly, check the output of the following command: # vgs Additional resources pvck(8) man page Extracting LVM metadata backups from a physical volume How to repair metadata on physical volume online? (Red Hat Knowledgebase) How do I restore a volume group in Red Hat Enterprise Linux if one of the physical volumes that constitute the volume group has failed? (Red Hat Knowledgebase) 13.12. Repairing a disk with damaged LVM headers and metadata using the pvck command This is an alternative to the Repairing a disk with damaged LVM headers and metadata using the pvcreate and the vgcfgrestore commands . There may be cases where the pvcreate and the vgcfgrestore commands do not work. This method is more targeted at the damaged disk. This method uses a metadata input file that was extracted by pvck --dump , or a backup file from /etc/lvm/backup . When possible, use metadata saved by pvck --dump from another PV in the same VG, or from a second metadata area on the PV. For more information, see Saving extracted metadata to a file . Procedure Repair the headers and metadata on the disk: # pvck --repair -f <metadata-file> <disk> where <metadata-file> is a file containing the most recent metadata for the VG. This can be /etc/lvm/backup/ vg-name , or it can be a file containing raw metadata text from the pvck --dump metadata_search command output. <disk> is the name of the disk where the PV is supposed to be, for example, /dev/sdb . To prevent data loss, verify that is the correct disk. If you are not certain the disk is correct, contact Red Hat Support. Note If the metadata file is a backup file, the pvck --repair should be run on each PV that holds metadata in VG. If the metadata file is raw metadata that has been extracted from another PV, the pvck --repair needs to be run only on the damaged PV. Verification To check that the new physical volume is intact and the volume group is functioning correctly, check outputs of the following commands: # vgs <vgname> # pvs <pvname> # lvs <lvname> Additional resources pvck(8) man page Extracting LVM metadata backups from a physical volume . How to repair metadata on physical volume online? (Red Hat Knowledgebase) How do I restore a volume group in Red Hat Enterprise Linux if one of the physical volumes that constitute the volume group has failed? (Red Hat Knowledgebase) 13.13. Troubleshooting LVM RAID You can troubleshoot various issues in LVM RAID devices to correct data errors, recover devices, or replace failed devices. 13.13.1. Checking data coherency in a RAID logical volume LVM provides scrubbing support for RAID logical volumes. RAID scrubbing is the process of reading all the data and parity blocks in an array and checking to see whether they are coherent. The lvchange --syncaction repair command initiates a background synchronization action on the array. Procedure Optional: Control the rate at which a RAID logical volume is initialized by setting any one of the following options: --maxrecoveryrate Rate[bBsSkKmMgG] sets the maximum recovery rate for a RAID logical volume so that it will not expel nominal I/O operations. --minrecoveryrate Rate[bBsSkKmMgG] sets the minimum recovery rate for a RAID logical volume to ensure that I/O for sync operations achieves a minimum throughput, even when heavy nominal I/O is present # lvchange --maxrecoveryrate 4K my_vg/my_lv Logical volume _my_vg/my_lv_changed. Replace 4K with the recovery rate value, which is an amount per second for each device in the array. If you provide no suffix, the options assume kiB per second per device. # lvchange --syncaction repair my_vg/my_lv When you perform a RAID scrubbing operation, the background I/O required by the sync actions can crowd out other I/O to LVM devices, such as updates to volume group metadata. This might cause the other LVM operations to slow down. Note You can also use these maximum and minimum I/O rate while creating a RAID device. For example, lvcreate --type raid10 -i 2 -m 1 -L 10G --maxrecoveryrate 128 -n my_lv my_vg creates a 2-way RAID10 array my_lv, which is in the volume group my_vg with 3 stripes that is 10G in size with a maximum recovery rate of 128 kiB/sec/device. Display the number of discrepancies in the array, without repairing them: # lvchange --syncaction check my_vg/my_lv This command initiates a background synchronization action on the array. Optional: View the var/log/syslog file for the kernel messages. Correct the discrepancies in the array: # lvchange --syncaction repair my_vg/my_lv This command repairs or replaces failed devices in a RAID logical volume. You can view the var/log/syslog file for the kernel messages after executing this command. Verification Display information about the scrubbing operation: # lvs -o +raid_sync_action,raid_mismatch_count my_vg/my_lv LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert SyncAction Mismatches my_lv my_vg rwi-a-r--- 500.00m 100.00 idle 0 Additional resources lvchange(8) and lvmraid(7) man pages on your system Minimum and maximum I/O rate options 13.13.2. Replacing a failed RAID device in a logical volume RAID is not similar to traditional LVM mirroring. In case of LVM mirroring, remove the failed devices. Otherwise, the mirrored logical volume would hang while RAID arrays continue running with failed devices. For RAID levels other than RAID1, removing a device would mean converting to a lower RAID level, for example, from RAID6 to RAID5, or from RAID4 or RAID5 to RAID0. Instead of removing a failed device and allocating a replacement, with LVM, you can replace a failed device that serves as a physical volume in a RAID logical volume by using the --repair argument of the lvconvert command. Prerequisites The volume group includes a physical volume that provides enough free capacity to replace the failed device. If no physical volume with enough free extents is available on the volume group, add a new, sufficiently large physical volume by using the vgextend utility. Procedure View the RAID logical volume: # lvs --all --options name,copy_percent,devices my_vg LV Cpy%Sync Devices my_lv 100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] /dev/sdc1(1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] /dev/sdc1(0) [my_lv_rmeta_2] /dev/sdd1(0) View the RAID logical volume after the /dev/sdc device fails: # lvs --all --options name,copy_percent,devices my_vg /dev/sdc: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. LV Cpy%Sync Devices my_lv 100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] [unknown](1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] [unknown](0) [my_lv_rmeta_2] /dev/sdd1(0) Replace the failed device: # lvconvert --repair my_vg/my_lv /dev/sdc: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. Attempt to replace failed RAID images (requires full device resync)? [y/n]: y Faulty devices in my_vg/my_lv successfully replaced. Optional: Manually specify the physical volume that replaces the failed device: # lvconvert --repair my_vg/my_lv replacement_pv Examine the logical volume with the replacement: # lvs --all --options name,copy_percent,devices my_vg /dev/sdc: open failed: No such device or address /dev/sdc1: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. LV Cpy%Sync Devices my_lv 43.79 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] /dev/sdb1(1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] /dev/sdb1(0) [my_lv_rmeta_2] /dev/sdd1(0) Until you remove the failed device from the volume group, LVM utilities still indicate that LVM cannot find the failed device. Remove the failed device from the volume group: # vgreduce --removemissing my_vg Verification View the available physical volumes after removing the failed device: # pvscan PV /dev/sde1 VG rhel_virt-506 lvm2 [<7.00 GiB / 0 free] PV /dev/sdb1 VG my_vg lvm2 [<60.00 GiB / 59.50 GiB free] PV /dev/sdd1 VG my_vg lvm2 [<60.00 GiB / 59.50 GiB free] PV /dev/sdd1 VG my_vg lvm2 [<60.00 GiB / 59.50 GiB free] Examine the logical volume after the replacing the failed device: # lvs --all --options name,copy_percent,devices my_vg my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] /dev/sdb1(1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] /dev/sdb1(0) [my_lv_rmeta_2] /dev/sdd1(0) Additional resources lvconvert(8) and vgreduce(8) man pages on your system 13.14. Troubleshooting duplicate physical volume warnings for multipathed LVM devices When using LVM with multipathed storage, LVM commands that list a volume group or logical volume might display messages such as the following: Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/dm-5 not /dev/sdd Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/emcpowerb not /dev/sde Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/sddlmab not /dev/sdf You can troubleshoot these warnings to understand why LVM displays them, or to hide the warnings. 13.14.1. Root cause of duplicate PV warnings When a multipath software such as Device Mapper Multipath (DM Multipath), EMC PowerPath, or Hitachi Dynamic Link Manager (HDLM) manages storage devices on the system, each path to a particular logical unit (LUN) is registered as a different SCSI device. The multipath software then creates a new device that maps to those individual paths. Because each LUN has multiple device nodes in the /dev directory that point to the same underlying data, all the device nodes contain the same LVM metadata. Table 13.1. Example device mappings in different multipath software Multipath software SCSI paths to a LUN Multipath device mapping to paths DM Multipath /dev/sdb and /dev/sdc /dev/mapper/mpath1 or /dev/mapper/mpatha EMC PowerPath /dev/emcpowera HDLM /dev/sddlmab As a result of the multiple device nodes, LVM tools find the same metadata multiple times and report them as duplicates. 13.14.2. Cases of duplicate PV warnings LVM displays the duplicate PV warnings in either of the following cases: Single paths to the same device The two devices displayed in the output are both single paths to the same device. The following example shows a duplicate PV warning in which the duplicate devices are both single paths to the same device. Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/sdd not /dev/sdf If you list the current DM Multipath topology using the multipath -ll command, you can find both /dev/sdd and /dev/sdf under the same multipath map. These duplicate messages are only warnings and do not mean that the LVM operation has failed. Rather, they are alerting you that LVM uses only one of the devices as a physical volume and ignores the others. If the messages indicate that LVM chooses the incorrect device or if the warnings are disruptive to users, you can apply a filter. The filter configures LVM to search only the necessary devices for physical volumes, and to leave out any underlying paths to multipath devices. As a result, the warnings no longer appear. Multipath maps The two devices displayed in the output are both multipath maps. The following examples show a duplicate PV warning for two devices that are both multipath maps. The duplicate physical volumes are located on two different devices rather than on two different paths to the same device. Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/mapper/mpatha not /dev/mapper/mpathc Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/emcpowera not /dev/emcpowerh This situation is more serious than duplicate warnings for devices that are both single paths to the same device. These warnings often mean that the machine is accessing devices that it should not access: for example, LUN clones or mirrors. Unless you clearly know which devices you should remove from the machine, this situation might be unrecoverable. Red Hat recommends that you contact Red Hat Technical Support to address this issue. 13.14.3. Example LVM device filters that prevent duplicate PV warnings The following examples show LVM device filters that avoid the duplicate physical volume warnings that are caused by multiple storage paths to a single logical unit (LUN). You can configure the filter for logical volume manager (LVM) to check metadata for all devices. Metadata includes local hard disk drive with the root volume group on it and any multipath devices. By rejecting the underlying paths to a multipath device (such as /dev/sdb , /dev/sdd ), you can avoid these duplicate PV warnings, because LVM finds each unique metadata area once on the multipath device itself. To accept the second partition on the first hard disk drive and any device mapper (DM) Multipath devices and reject everything else, enter: filter = [ \"a|/dev/sda2$|\", \"a|/dev/mapper/mpath.*|\", \"r|.*|\" ] To accept all HP SmartArray controllers and any EMC PowerPath devices, enter: filter = [ \"a|/dev/cciss/.*|\", \"a|/dev/emcpower.*|\", \"r|.*|\" ] To accept any partitions on the first IDE drive and any multipath devices, enter: filter = [ \"a|/dev/hda.*|\", \"a|/dev/mapper/mpath.*|\", \"r|.*|\" ] 13.14.4. Additional resources Additional resources Limiting LVM device visibility and usage The LVM device filter Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/troubleshooting-lvm_configuring-and-managing-logical-volumes"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/authentication/legal-notice"}
{"title": "Chapter 1. Uninstalling Builds", "content": "Chapter 1. Uninstalling Builds As a cluster administrator, you can uninstall the Builds for Red Hat OpenShift Operator. Uninstalling the Operator does not automatically remove the custom resources (CRs) and Builds components. To uninstall the Builds for Red Hat OpenShift Operator, perform the following tasks: Delete the custom resources (CRs) that were created by default when you installed the Operator. Note If you uninstall the Operator without removing the CRs, you cannot remove them later. Uninstall the Operator. 1.1. Deleting the Builds components and custom resources Delete the custom resources (CRs) that were created by default during Operator installation. Prerequisites You are logged in to the OpenShift Container Platform cluster as an administrator. Procedure In the Administrator perspective of the web console, navigate to Administration Custom Resource Definition . In the Filter by name box, enter shipwright.io to search for the Builds for Red Hat OpenShift Operator CRs. Click CRD Config to see the Custom Resource Definition Details page. Click the Actions drop-down list, and select Delete Custom Resource Definition . Note Deleting the CRs deletes the Builds controller and all the related components and tasks on the cluster. Click Delete to confirm the deletion of the CRs. 1.2. Uninstalling the Builds for Red Hat OpenShift Operator You can uninstall the Builds for Red Hat OpenShift Operator by using the Administrator perspective in the web console. Prerequisites You are logged in to the OpenShift Container Platform cluster as an administrator. You have deleted the Builds components and custom resources. Procedure Navigate to Operators OperatorHub . In the OperatorHub page, search for the Builds for Red Hat OpenShift Operator by using the Filter by keyword box. Click the Builds for Red Hat OpenShift Operator tile. The Operator tile indicates that the Operator is installed. In the Builds for Red Hat OpenShift Operator description page, click Uninstall . Additional resources Deleting Operators from a cluster Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/uninstall/uninstalling-builds"}
{"title": "Getting started with resource optimization for OpenShift", "content": "Getting started with resource optimization for OpenShift Cost Management Service 1-latest Learn about resource optimization for OpenShift Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/getting_started_with_resource_optimization_for_openshift/index"}
{"title": "Visualizing your costs using cost explorer", "content": "Visualizing your costs using cost explorer Cost Management Service 1-latest Use Cost Explorer to visualize and understand your costs Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/visualizing_your_costs_using_cost_explorer/index"}
{"title": "Chapter 5. Managing subscriptions in the web console", "content": "Chapter 5. Managing subscriptions in the web console You can manage your Red Hat product subscriptions in the Red Hat Enterprise Linux 9 web console. Prerequisites Your Red Hat Customer Portal or a subscription activation key. 5.1. Subscription management in the web console The RHEL 9 web console provides an interface for using Red Hat Subscription Manager installed on your local system. The Subscription Manager connects to the Red Hat Customer Portal and verifies available: Active subscriptions Expired subscriptions Renewed subscriptions If you want to renew the subscription or get a different one on the Red Hat Customer Portal, you do not have to update the Subscription Manager data manually. The Subscription Manager synchronizes data with the Red Hat Customer Portal automatically. 5.2. Registering subscriptions with credentials in the web console You can register a newly installed Red Hat Enterprise Linux with your account credentials in the RHEL web console. Prerequisites A valid user account on the Red Hat Customer Portal. See the Create a Red Hat Login page. An active subscription for your RHEL system. You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . In the Health filed in the Overview page, click the Not registered warning, or click Subscriptions in the main menu to move to page with your subscription information. In the Overview field, click Register . In the Register system dialog box, select Account to register by using your account credentials. Enter your username. Enter your password. Optional: Enter your organization’s name or ID. If your account belongs to more than one organization on the Red Hat Customer Portal, you must add the organization name or organization ID. To get the org ID, go to your Red Hat contact point. If you do not want to connect your system to Red Hat Insights, clear the Insights check box. Click Register . 5.3. Registering subscriptions with activation keys in the web console You can register a newly installed Red Hat Enterprise Linux with an activation key in the RHEL web console. Prerequisites An activation key of your Red Hat product subscription. You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . In the Health field on the Overview page, click the Not registered warning, or click Subscriptions in the main menu to move to the page with your subscription information. . In the Overview filed, click Register . In the Register system dialog box, select Activation key to register using an activation key. Enter your key or keys. Enter your organization’s name or ID. To get the organization ID, go to your Red Hat contact point. If you do not want to connect your system to Red Hat Insights, clear the Insights check box. Click Register . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_systems_using_the_rhel_9_web_console/managing-subscriptions-in-the-web-console_system-management-using-the-rhel-9-web-console"}
{"title": "Chapter 4. Managing software updates in the web console", "content": "Chapter 4. Managing software updates in the web console Learn how to manage software updates in the RHEL 9 web console and ways to automate them. The Software Updates module in the web console is based on the dnf utility. For more information about updating software with dnf , see the Updating packages section. 4.1. Managing manual software updates in the web console You can manually update your software by using the web console. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Software Updates . The list of available updates refreshes automatically after 24 hours. To trigger a refresh, click the Check for Updates button. Apply updates. You can watch the update log while the update is running. To install all available updates, click the Install all updates button. If you have security updates available, you can install them separately by clicking the Install Security Updates button. If you have kpatch updates available, you can install them separately by clicking the Install kpatch updates button. Optional: You can turn on the Reboot after completion switch for an automatic restart of your system. If you perform this step, you can skip the remaining steps of this procedure. After the system applies updates, you get a recommendation to restart your system. Restart the system if the update included a new kernel or system services that you do not want to restart individually. Click Ignore to cancel the restart, or Restart Now to proceed with restarting your system. After the system restart, log in to the web console and go to the Software Updates page to verify that the update is successful. 4.2. Managing automatic software updates in the web console In the web console, you can choose to apply all updates, or security updates and also manage periodicity and time of your automatic updates. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Software Updates . In the Settings table, click the Edit button. Pick one of the types of automatic updates. You can select from Security updates only , or All updates . To modify the day of the automatic update, click on the every day drop-down menu and select a specific day. To modify the time of the automatic update, click into the 6:00 field and select or type a specific time. If you want to disable automatic software updates, select the No updates type. 4.3. Managing on-demand restarting after applying software updates in the web console The intelligent restarting feature informs the users whether it is necessary to reboot the whole system after you apply a software update or if it is sufficient to only restart certain services. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Software Updates . Apply an update of your system. After a successful update, click Reboot system…​ , Restart services…​ , or Ignore If you decide to ignore, you can return to the restart or reboot menu by doing one of the following: Rebooting: Click the Reboot system button in the Status field of the Software Updates page. Optional: Write a message to the logged in users. Select a delay from the Delay drop-down menu. Click Reboot . Restarting services: Click the Restart services button in the Status field of the Software Updates page. You will see a list of all the services that require a restart. Click Restart services . Depending on your choice, the system will reboot or your services will restart. 4.4. Applying patches with kernel live patching in the web console You can configure the kpatch framework, which applies kernel security patches without forcing reboots, in the RHEL web console. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Software Updates . Check the status of your kernel patching settings. If the patching is not installed, click Install . To enable kernel patching, click Enable . Check the check box for applying kernel patches. Select whether you want to apply patches for current and future kernels or the current kernel only. If you decide to subscribe to applying patches for future kernels, the system also applies patches for the upcoming kernel releases. Click Apply . Verification Check that the kernel patching is now Enabled in the Settings table of the Software updates section. Additional resources Applying patches with kernel live patching Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_systems_using_the_rhel_9_web_console/managing-software-updates-in-the-web-console_system-management-using-the-rhel-9-web-console"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/builds_for_red_hat_openshift/1.1/html/work_with_builds/legal-notice"}
{"title": "Provisioning APIs", "content": "Provisioning APIs  Reference guide for provisioning APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/provisioning_apis/index"}
{"title": "Chapter 3. Installing web console add-ons and creating custom pages", "content": "Chapter 3. Installing web console add-ons and creating custom pages Depending on how you want to use your Red Hat Enterprise Linux system, you can add additional available applications to the web console or create custom pages based on your use case. 3.1. Add-ons for the RHEL web console While the cockpit package is a part of Red Hat Enterprise Linux by default, you can install add-on applications on demand by using the following command: # dnf install <add-on> In the previous command, replace <add-on> by a package name from the list of available add-on applications for the RHEL web console. Feature name Package name Usage Composer cockpit-composer Building custom OS images File manager cockpit-files Managing files and directories in the standard web-console interface Machines cockpit-machines Managing libvirt virtual machines PackageKit cockpit-packagekit Software updates and application installation (usually installed by default) PCP cockpit-pcp Persistent and more fine-grained performance data (installed on demand from the UI) Podman cockpit-podman Managing containers and managing container images Session Recording cockpit-session-recording Recording and managing user sessions Storage cockpit-storaged Managing storage through udisks 3.2. Creating new pages in the web console If you want to add customized functions to your Red Hat Enterprise Linux web console, you must add the package directory that contains the HTML and JavaScript files for the page that runs the required function. For detailed information about adding custom pages, see Creating Plugins for the Cockpit User Interface on the Cockpit Project website. Additional resources Cockpit Packages section in the Cockpit Project Developer Guide 3.3. Overriding the manifest settings in the web console You can modify the menu of the web console for a particular user and all users of the system. In the cockpit project, a package name is a directory name. A package contains the manifest.json file along with other files. Default settings are present in the manifest.json file. You can override the default cockpit menu settings by creating a <package-name> .override.json file at a specific location for the specified user. Prerequisites You have installed the RHEL 9 web console. For instructions, see Installing and enabling the web console . Procedure Override manifest settings in the <systemd> .override.json file in a text editor of your choice, for example: To edit for all users, enter: # vi /etc/cockpit/ <systemd> .override.json To edit for a single user, enter: # vi ~/.config/cockpit/ <systemd> .override.json Edit the required file with the following details: { \"menu\": { \"services\": null, \"logs\": { \"order\": -1 } } } The null value hides the services tab The -1 value moves the logs tab to the first place. Restart the cockpit service: # systemctl restart cockpit.service Additional resources cockpit(1) man page on your system Manifest overrides Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_systems_using_the_rhel_9_web_console/cockpit-add-ons-_system-management-using-the-rhel-9-web-console"}
{"title": "Appendix A. Automation execution environments precedence", "content": "Appendix A. Automation execution environments precedence Project updates will always use the control plane automation execution environments by default, however, jobs will use the first available automation execution environments as follows: The execution_environment defined on the template (job template or inventory source) that created the job. The default_environment defined on the project that the job uses. The default_environment defined on the organization of the job. The default_environment defined on the organization of the inventory the job uses. The current DEFAULT_EXECUTION_ENVIRONMENT setting (configurable at api/v2/settings/system/ ) Any image from the GLOBAL_JOB_EXECUTION_ENVIRONMENTS setting. Any other global execution environment. Note If more than one execution environment fits a criteria (applies for 6 and 7), the most recently created one is used. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/creating_and_using_execution_environments/con-ee-precedence"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/creating_and_using_execution_environments/legal-notice"}
{"title": "Chapter 8. Open source license", "content": "Chapter 8. Open source license Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\" ) shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: You must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/creating_and_using_execution_environments/assembly-open-source-license"}
{"title": "Chapter 6. Network ports and protocols", "content": "Chapter 6. Network ports and protocols Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server in order for it to work. Ensure that these ports are available and are not blocked by the server firewall. The following architectural diagram is an example of a fully deployed Ansible Automation Platform with all possible components. Note In some of the following use cases, hop nodes are used instead of a direct link from an execution node. Hop nodes are an option for connecting control and execution nodes. Hop nodes use minimal CPU and memory, so vertically scaling hop nodes does not impact system capacity. Note Direct connections shown in the diagram between the Client and automation hub, Event-Driven Ansible, and automation controller only apply to systems upgraded from Red Hat Ansible Automation Platform 2.4 to Red Hat Ansible Automation Platform 2.5 to provide backward compatibility. The connection does not exist for Red Hat Ansible Automation Platform 2.5. The following table indicates the destination port and the direction of network traffic: Figure 6.1. Ansible Automation Platform Network ports and protocols Note The following default destination ports and installer inventory listed are configurable. If you choose to configure them to suit your environment, you might experience a change in behavior. Table 6.1. Network ports and protocols Port Protocol Service Source Destination Required for Installer Inventory Variable 22 TCP SSH Installer node Automation hub Installation (temporary) ansible_port 22 TCP SSH Installer node Controller node Installation (temporary) ansible_port 22 TCP SSH Installer node Event-Driven Ansible node Installation (temporary) ansible_port 22 TCP SSH Installer node Execution node Installation (temporary) ansible_port 22 TCP SSH Installer node Hop node Installation (temporary) ansible_port 22 TCP SSH Installer node Hybrid node Installation (temporary) ansible_port 22 TCP SSH Installer node PostgreSQL database Remote access during installation (temporary) pg_port 80/443 TCP HTTP/HTTPS Installer node Automation hub Allows installer node to push the execution environment image to automation hub when using the bundle installer. Fixed value 80/443 TCP HTTP/HTTPS Event-Driven Ansible node Automation hub Pull container decision environments Fixed value 80/443 TCP HTTP/HTTPS Event-Driven Ansible node Automation controller Launch automation controller jobs Fixed value 80/443 TCP HTTP/HTTPS Automation controller Automation hub Pull collections Fixed value 80/443 TCP HTTP/HTTPS Execution node Automation hub Allows execution nodes to pull the execution environment image from automation hub. Fixed value 80/443 TCP HTTP/HTTPS Controller node OpenShift Container Platform Only required when using container groups to run jobs. Host name of OpenShift API server 80/443 TCP HTTP/HTTPS HA Proxy load balancer/Ingress Node Platform gateway This is the ingress above the platform gateway that is customer controlled and can load balance requests to multiple gateways. This port is customer managed outside of Ansible Automation Platform. 80/443 TCP HTTP/HTTPS Platform gateway Automation controller Link between gateway and Ansible Automation Platform components 80/443 TCP HTTP/HTTPS Platform gateway Automation hub Link between gateway and Ansible Automation Platform components 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible Link between gateway and Ansible Automation Platform components 80/443 TCP HTTP/HTTPS HA Proxy load balancer/Ingress Node Automation controller Only relevant if accessing the component directly from Platform gateway automationgateway_main_url 80/443 TCP HTTP/HTTPS HA Proxy load balancer/Ingress Node Automation hub Only relevant if accessing the component directly from Platform gateway automationgatweway_main_url 80/443 TCP HTTP/HTTPS HA Proxy load balancer/Ingress Node Event-Driven Ansible Only relevant if accessing the component directly from Platform gateway automationgateway_main_url 443 TCP HTTPS Remote execution node (Client) Controller node Web UI/API nginx_https_port 5432 TCP PostgreSQL Controller node PostgreSQL database Open only if the internal database is used along with another component. Otherwise, this port should not be open. automationcontroller_pg_port 5432 TCP PostgreSQL Event-Driven Ansible node PostgreSQL database Open only if the internal database is used along with another component. Otherwise, this port should not be open. automationedacontroller_pg_port 5432 TCP PostgreSQL Automation hub PostgreSQL database Open only if the internal database is used along with another component. Otherwise, this port should not be open. automationhub_pg_port 5432 TCP PostgreSQL Platform gateway External database Open only if the internal database is used along with another component. Otherwise, this port should not be open. automationgateway_pg_port 6379 TCP PostgreSQL Event-Driven Ansible Redis node Job launching 6379 TCP PostgreSQL Platform gateway Redis node Data storage and retrieval 8443 TCP HTTPS Platform gateway Platform gateway nginx 16379 TCP Redis Redis nodes Redis nodes Redis cluster bus port for a resilient Redis configuration 27199 TCP Receptor Controller node Execution node Configurable Mesh nodes directly peered to controllers. Direct nodes involved. 27199 communication can be both ways (depending on installation inventory) for execution nodes receptor_listener_port peers 27199 TCP Receptor Controller node Hop node Configurable ENABLE connections from hop nodes to Receptor port if relayed through hop nodes. receptor_listener_port peers 27199 TCP Receptor Controller node Hybrid node Configurable ENABLE connections from controllers to Receptor port if relayed through non-hop connected nodes. receptor_listener_port peers 27199 TCP Receptor Execution node Hop node Configurable Mesh 27199 communication can be both ways (depending on installation inventory) for execution nodes ALLOW connection from controller(s) to Receptor port receptor_listener_port peers 27199 TCP Receptor Hop node Execution node Configurable Mesh 27199 communication can be both ways (depending on installation inventory) for execution nodes receptor_listener_port peers 27199 TCP Receptor Execution node Controller node Configurable Mesh 27199 communication can be both ways (depending on installation inventory) for execution nodes ALLOW connection from controller(s) to Receptor port receptor_listener_port peers 27199 TCP Receptor OCP cluster Execution node Note Hybrid nodes act as a combination of control and execution nodes, and therefore Hybrid nodes share the connections of both. If receptor_listener_port is defined, the machine also requires an available open port on which to establish inbound TCP connections, for example, 27199. It might be the case that some servers do not listen on receptor port (the default is 27199) Suppose you have a Control plane with nodes A, B, C, D The RPM installer creates a strongly connected peering between the control plane nodes with a least privileged approach and opens the tcp listener only on those nodes where it is required. All the receptor connections are bidirectional, so once the connection is created, the receptor can communicate in both directions. The following is an example peering set up for three controller nodes: Controller node A -→ Controller node B Controller node A -→ Controller node C Controller node B -→ Controller node C You can force the listener by setting receptor_listener=True However, a connection Controller B -→ A is likely to be rejected as that connection already exists. This means that nothing connects to Controller A as Controller A is creating the connections to the other nodes, and the following command does not return anything on Controller A: [root@controller1 ~]# ss -ntlp | grep 27199 [root@controller1 ~]# Table 6.2. Red Hat Insights for Red Hat Ansible Automation Platform URL Required for https://api.access.redhat.com:443 General account services, subscriptions https://cert-api.access.redhat.com:443 Insights data upload https://cert.console.redhat.com:443 Inventory upload and Cloud Connector connection https://console.redhat.com:443 Access to Insights dashboard Table 6.3. Automation Hub URL Required for https://console.redhat.com:443 General account services, subscriptions https://catalog.redhat.com:443 Indexing execution environments https://sso.redhat.com:443 TCP https://automation-hub-prd.s3.amazonaws.com https://automation-hub-prd.s3.us-east-2.amazonaws.com Firewall access https://galaxy.ansible.com:443 Ansible Community curated Ansible content https://ansible-galaxy-ng.s3.dualstack.us-east-1.amazonaws.com Dual Stack IPv6 endpoint for Community curated Ansible content repository https://registry.redhat.io:443 Access to container images provided by Red Hat and partners https://cert.console.redhat.com:443 Red Hat and partner curated Ansible Collections Table 6.4. Execution Environments (EE) URL Required for https://registry.redhat.io:443 Access to container images provided by Red Hat and partners cdn.quay.io:443 Access to container images provided by Red Hat and partners cdn01.quay.io:443 Access to container images provided by Red Hat and partners cdn02.quay.io:443 Access to container images provided by Red Hat and partners cdn03.quay.io:443 Access to container images provided by Red Hat and partners Important As of April 1st, 2025 , quay.io is adding three additional endpoints. As a result, customers must adjust allow/block lists within their firewall systems lists to include the following endpoints: cdn04.quay.io cdn05.quay.io cdn06.quay.io To avoid problems pulling container images, customers must allow outbound TCP connections (ports 80 and 443) to the following hostnames: cdn.quay.io cdn01.quay.io cdn02.quay.io cdn03.quay.io cdn04.quay.io cdn05.quay.io cdn06.quay.io This change should be made to any firewall configuration that specifically enables outbound connections to registry.redhat.io or registry.access.redhat.com . Use the hostnames instead of IP addresses when configuring firewall rules. After making this change, you can continue to pull images from registry.redhat.io or registry.access.redhat.com . You do not require a quay.io login, or need to interact with the quay.io registry directly in any way to continue pulling Red Hat container images. For more information, see Firewall changes for container image pulls 2024/2025 . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/planning_your_installation/ref-network-ports-protocols_planning"}
{"title": "Pipelines", "content": "Pipelines  A cloud-native continuous integration and continuous delivery solution based on Kubernetes resources Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/pipelines/index"}
{"title": "Chapter 6. Setting up your container repository", "content": "Chapter 6. Setting up your container repository When you set up your container repository, you must add a description, include a README, add teams that can access the repository, and tag automation execution environments. 6.1. Prerequisites to setting up your remote registry You are logged in to Ansible Automation Platform. You have permissions to change the repository. 6.2. Adding a README to your container repository Add a README to your container repository to provide instructions to your users on how to work with the container. Automation hub container repositories support Markdown for creating a README. By default, the README is empty. Prerequisites You have permissions to change containers. Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Content Execution Environments . Select your execution environment. On the Detail tab, click Add . In the Raw Markdown text field, enter your README text in Markdown. Click Save when you are finished. After you add a README, you can edit it at any time by clicking Edit and repeating steps 4 and 5. 6.3. Providing access to your automation execution environments Provide access to your automation execution environments for users who need to work with the images. Adding a team allows you to modify the permissions the team can have to the container repository. You can use this option to extend or restrict permissions based on what the team is assigned. Prerequisites You have change container namespace permissions. Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Content Execution Environments . Select your automation execution environment. From the Team Access tab, click Add roles . Select the team or teams to which you want to grant access and click Next . Select the roles that you want to add to this execution environment and click Next . Click Finish . 6.4. Tagging container images Tag automation execution environments to add an additional name to automation execution environments stored in your automation hub container repository. If no tag is added to an automation execution environment, automation hub defaults to latest for the name. Prerequisites You have change automation execution environment tags permissions. Procedure From the navigation panel, select Automation Content Execution Environments . Select your automation execution environments. Click the Images tab. Click the More Actions icon ⋮ , and click Manage tags . Add a new tag in the text field and click Add . Optional: Remove current tags by clicking x on any of the tags for that image. Verification Click the Activity tab and review the latest changes. 6.5. Creating a credential To pull automation execution environments images from a password or token-protected registry, you must create a credential. In earlier versions of Ansible Automation Platform, you were required to deploy a registry to store execution environment images. On Ansible Automation Platform 2.0 and later, the system operates as if you already have a remote registry up and running. To store execution environment images, add the credentials of only your selected remote registries. Procedure Log in to Ansible Automation Platform. From the navigation panel, select Automation Execution Infrastructure Credentials . Click Create credential to create a new credential. Enter an authorization Name , Description , and Organization . In the Credential Type drop-down, select Container Registry . Enter the Authentication URL . This is the remote registry address. Enter the Username and Password or Token required to log in to the remote registry. Optional: To enable SSL verification, select Verify SSL . Click Create credential . Filling in at least one of the fields organization, user, or team is mandatory, and can be done through the user interface. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/creating_and_using_execution_environments/setting-up-container-repository"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/getting_started_with_resource_optimization_for_openshift/legal-notice"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_gitops/1.15"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/visualizing_your_costs_using_cost_explorer/legal-notice"}
{"title": "Chapter 6. Updating RHEL 9 content", "content": "Chapter 6. Updating RHEL 9 content With DNF , you can check if your system has any pending updates. You can list packages that need updating and choose to update a single package, multiple packages, or all packages at once. If any of the packages you choose to update have dependencies, these dependencies are updated as well. 6.1. Checking for updates To identify which packages installed on your system have available updates, you can list them. Procedure Check the available updates for installed packages: # dnf check-update The output returns the list of packages and their dependencies that have an update available. 6.2. Updating packages You can use DNF to update a single package, a package group, or all packages and their dependencies at once. Important When applying updates to the kernel, dnf always installs a new kernel regardless of whether you are using the dnf upgrade or dnf install command. Note that this only applies to packages identified by using the installonlypkgs DNF configuration option. Such packages include, for example, the kernel , kernel-core , and kernel-modules packages. Procedure Depending on your scenario, use one of the following options to apply updates: To update all packages and their dependencies, enter: # dnf upgrade To update a single package, enter: # dnf upgrade <package_name> To update packages only from a specific package group, enter: # dnf group upgrade <group_name> Important If you upgraded the GRUB boot loader packages on a BIOS or IBM Power system, reinstall GRUB. See Reinstalling GRUB . 6.3. Updating security-related packages You can use DNF to update security-related packages. Procedure Depending on your scenario, use one of the following options to apply updates: To upgrade to the latest available packages that have security errata, enter: # dnf upgrade --security To upgrade to the last security errata packages, enter: # dnf upgrade-minimal --security Important If you upgraded the GRUB boot loader packages on a BIOS or IBM Power system, reinstall GRUB. See Reinstalling GRUB . Additional resources Managing and monitoring security updates Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_software_with_the_dnf_tool/assembly_updating-rhel-9-content_managing-software-with-the-dnf-tool"}
{"title": "Chapter 17. Managing containers by using the RHEL web console", "content": "Chapter 17. Managing containers by using the RHEL web console You can use the Red Hat Enterprise Linux web console to manage your containers and pods. With the web console, you can create containers as a non-root or root user. As a root user, you can create system containers with extra privileges and options. As a non-root user, you have two options: To only create user containers, you can use the web console in its default mode - Limited access . To create both user and system containers, click Administrative access in the top panel of the web console page. For details about differences between root and rootless containers, see Special considerations for rootless containers . 17.1. Creating containers in the web console You can create a container and add port mappings, volumes, environment variables, health checks, and so on. Prerequisites You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. Click Create container . In the Name field, enter the name of your container. Provide additional information in the Details tab. Available only with the administrative access : Select the Owner of the container: System or User. In the Image drop-down list select or search the container image in selected registries. Optional: Check the Pull latest image checkbox to pull the latest container image. The Command field specifies the command. You can change the default command if you need. Optional: Check the With terminal checkbox to run your container with a terminal. The Memory limit field specifies the memory limit for the container. To change the default memory limit, check the checkbox and specify the limit. Available only for system containers : In the CPU shares field , specify the relative amount of CPU time. Default value is 1024. Check the checkbox to modify the default value. Available only for system containers : In the Restart policy drop-down menu, select one of the following options: No (default value): No action. On Failure : Restarts a container on failure. Always : Restarts a container when exits or after rebooting the system. Provide the required information in the Integration tab. Click Add port mapping to add port mapping between the container and host system. Enter the IP address , Host port , Container port and Protocol . Click Add volume to add volume. Enter the host path , Container path . You can check the Writable option checkbox to create a writable volume. In the SELinux drop-down list, select one of the following options: No Label , Shared or Private . Click Add variable to add environment variable. Enter the Key and Value . Provide the required information in the Health check tab. In the Command fields, enter the 'healthcheck' command. Specify the healthcheck options: Interval (default is 30 seconds) Timeout (default is 30 seconds) Start period Retries (default is 3) When unhealthy: Select one of the following options: No action (default): Take no action. Restart : Restart the container. Stop : Stop the container. Force stop : Force stops the container, it does not wait for the container to exit. Click Create and run to create and run the container. Note You can click Create to only create the container. Verification Click Podman containers in the main menu. You can see the newly created container in the Containers table. 17.2. Inspecting containers in the web console You can display detailed information about a container in the web console. Prerequisites The container was created. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. Click the > arrow icon to see details of the container. In the Details tab, you can see container ID, Image, Command, Created (timestamp when the container was created), and its State. Available only for system containers : You can also see IP address, MAC address, and Gateway address. In the Integration tab, you can see environment variables, port mappings, and volumes. In the Log tab, you can see container logs. In the Console tab, you can interact with the container using the command line. 17.3. Changing the state of containers in the web console In the Red Hat Enterprise Linux web console, you can start, stop, restart, pause, and rename containers on the system. Prerequisites The container was created. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the container you want to modify and click the overflow menu and select the action you want to perform: Start Stop Force stop Restart Force restart Pause Rename 17.4. Committing containers in the web console You can create a new image based on the current state of the container. Prerequisites The container was created. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the container you want to modify and click the overflow menu and select Commit . In the Commit container form, add the following details: In the New image name field, enter the image name. Optional: In the Tag field, enter the tag. Optional: In the Author field, enter your name. Optional: In the Command field, change command if you need. Optional: Check the Options you need: Pause container when creating image: The container and its processes are paused while the image is committed. Use legacy Docker format: if you do not use the Docker image format, the OCI format is used. Click Commit . Verification Click the Podman containers in the main menu. You can see the newly created image in the Images table. 17.5. Creating a container checkpoint in the web console Using the web console, you can set a checkpoint on a running container or an individual application and store its state to disk. Note Creating a checkpoint is available only for system containers. Prerequisites The container is running. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the container you want to modify and click the overflow icon menu and select Checkpoint . Optional: In the Checkpoint container form, check the options you need: Keep all temporary checkpoint files: keep all temporary log and statistics files created by CRIU during checkpointing. These files are not deleted if checkpointing fails for further debugging. Leave running after writing checkpoint to disk: leave the container running after checkpointing instead of stopping it. Support preserving established TCP connections Click Checkpoint . Verification Click the Podman containers in the main menu. Select the container you checkpointed, click the overflow menu icon and verify that there is a Restore option. 17.6. Restoring a container checkpoint in the web console You can use data saved to restore the container after a reboot at the same point in time it was checkpointed. Note Creating a checkpoint is available only for system containers. Prerequisites The container was checkpointed. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the container you want to modify and click the overflow menu and select Restore . Optional: In the Restore container form, check the options you need: Keep all temporary checkpoint files : Keep all temporary log and statistics files created by CRIU during checkpointing. These files are not deleted if checkpointing fails for further debugging. Restore with established TCP connections Ignore IP address if set statically : If the container was started with IP address the restored container also tries to use that IP address and restore fails if that IP address is already in use. This option is applicable if you added port mapping in the Integration tab when you create the container. Ignore MAC address if set statically : If the container was started with MAC address the restored container also tries to use that MAC address and restore fails if that MAC address is already in use. Click Restore . Verification Click the Podman containers in the main menu. You can see that the restored container in the Containers table is running. 17.7. Deleting containers in the web console You can delete an existing container using the web console. Prerequisites The container exists on your system. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the container you want to delete and click the overflow menu and select Delete . The pop-up window appears. Click Delete to confirm your choice. Verification Click the Podman containers in the main menu. The deleted container should not be listed in the Containers table. 17.8. Creating pods in the web console You can create pods in the RHEL web console interface. Prerequisites You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. Click Create pod . Provide additional information in the Create pod form: Available only with the administrative access : Select the Owner of the container: System or User. In the Name field, enter the name of your container. Click Add port mapping to add port mapping between container and host system. Enter the IP address, Host port, Container port and Protocol. Click Add volume to add volume. Enter the host path, Container path. You can check the Writable checkbox to create a writable volume. In the SELinux drop-down list, select one of the following options: No Label, Shared or Private. Click Create . Verification Click Podman containers in the main menu. You can see the newly created pod in the Containers table. 17.9. Creating containers in the pod in the web console You can create a container in a pod. Prerequisites You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. Click Create container in pod . In the Name field, enter the name of your container. Provide the required information in the Details tab. Available only with the administrative access : Select the Owner of the container: System or User. In the Image drop down list select or search the container image in selected registries. Optional: Check the Pull latest image checkbox to pull the latest container image. The Command field specifies the command. You can change the default command if you need. Optional: Check the With terminal checkbox to run your container with a terminal. The Memory limit field specifies the memory limit for the container. To change the default memory limit, check the checkbox and specify the limit. Available only for system containers : In the CPU shares field , specify the relative amount of CPU time. Default value is 1024. Check the checkbox to modify the default value. Available only for system containers : In the Restart policy drop down menu, select one of the following options: No (default value): No action. On Failure : Restarts a container on failure. Always : Restarts container when exits or after system boot. Provide the required information in the Integration tab. Click Add port mapping to add port mapping between the container and host system. Enter the IP address , Host port , Container port and Protocol . Click Add volume to add volume. Enter the host path , Container path . You can check the Writable option checkbox to create a writable volume. In the SELinux drop down list, select one of the following options: No Label , Shared , or Private . Click Add variable to add environment variable. Enter the Key and Value . Provide the required information in the Health check tab. In the Command fields, enter the healthcheck command. Specify the healthcheck options: Interval (default is 30 seconds) Timeout (default is 30 seconds) Start period Retries (default is 3) When unhealthy: Select one of the following options: No action (default): Take no action. Restart : Restart the container. Stop : Stop the container. Force stop : Force stops the container, it does not wait for the container to exit. Note The owner of the container is the same as the owner of the pod. Note In the pod, you can inspect containers, change the status of containers, commit containers, or delete containers. Verification Click Podman containers in the main menu. You can see the newly created container in the pod under the Containers table. 17.10. Changing the state of pods in the web console You can change the status of the pod. Prerequisites The pod was created. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the pod you want to modify and click the overflow menu and select the action you want to perform: Start Stop Force stop Restart Force restart Pause 17.11. Deleting pods in the web console You can delete an existing pod using the web console. Prerequisites The pod exists on your system. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Containers table, select the pod you want to delete and click the overflow menu and select Delete . In the following pop-up window, click Delete to confirm your choice. Warning You remove all containers in the pod. Verification Click the Podman containers in the main menu. The deleted pod should not be listed in the Containers table. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/managing-containers-by-using-the-rhel-web-console"}
{"title": "Chapter 16. Managing container images by using the RHEL web console", "content": "Chapter 16. Managing container images by using the RHEL web console You can use the RHEL web console web-based interface to pull, prune, or delete your container images. 16.1. Pulling container images in the web console You can download container images to your local system and use them to create your containers. Prerequisites You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Images table, click the overflow menu in the upper-right corner and select Download new image . The Search for an image dialog box appears. In the Search for field, enter the name of the image or specify its description. In the in drop-down list, select the registry from which you want to pull the image. Optional: In the Tag field, enter the tag of the image. Click Download . Verification Click Podman containers in the main menu. You can see the newly downloaded image in the Images table. Note You can create a container from the downloaded image by clicking the Create container in the Images table. To create the container, follow steps 3-8 in Creating containers in the web console . 16.2. Pruning container images in the web console You can remove all unused images that do not have any containers based on it. Prerequisites At least one container image is pulled. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Images table, click the overflow menu in the upper-right corner and select Prune unused images . The pop-up window with the list of images appears. Click Prune to confirm your choice. Verification Click Podman containers in the main menu. The deleted images should not be listed in the Images table. 16.3. Deleting container images in the web console You can delete a previously pulled container image using the web console. Prerequisites At least one container image is pulled. You have installed the RHEL 9 web console. You have enabled the cockpit service. Your user account is allowed to log in to the web console. For instructions, see Installing and enabling the web console . The cockpit-podman add-on is installed: # dnf install cockpit-podman Procedure Log in to the RHEL 9 web console. For details, see Logging in to the web console . Click Podman containers in the main menu. In the Images table, select the image you want to delete and click the overflow menu and select Delete . The window appears. Click Delete tagged images to confirm your choice. Verification Click the Podman containers in the main menu. The deleted container should not be listed in the Images table. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/managing-container-images-by-using-the-rhel-web-console_building-running-and-managing-containers"}
{"title": "Providing feedback on Red Hat documentation", "content": "Providing feedback on Red Hat documentation We appreciate and prioritize your feedback regarding our documentation. Provide as much detail as possible, so that your request can be quickly addressed. Prerequisites You are logged in to the Red Hat Customer Portal. Procedure To provide feedback, perform the following steps: Click the following link: Create Issue . Describe the issue or enhancement in the Summary text box. Provide details about the issue or requested enhancement in the Description text box. Type your name in the Reporter text box. Click the Create button. This action creates a documentation ticket and routes it to the appropriate documentation team. Thank you for taking the time to provide feedback. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/visualizing_your_costs_using_cost_explorer/proc-providing-feedback-on-redhat-documentation"}
{"title": "Chapter 1. Using Cost Explorer", "content": "Chapter 1. Using Cost Explorer The cost management Cost Explorer enables you to see your costs through time. With Cost Explorer, you can filter through your expenditure to find answers to your questions, view more details, and look for trends in data. 1.1. Understanding Cost Explorer The cost management Cost Explorer enables you to see your costs through time. With Cost Explorer, you can filter through your expenditure to find answers to your questions, view more details, and look for trends in data. With Cost Explorer you can: Identify abnormal events. Understand how your cost data changes over time. Create custom bar charts of your cost and usage data. Export custom cost data files. Figure 1.1. The Cost Explorer Interface A single bar in the Cost Explorer chart represents one day of cost and usage data corresponding to the filtered options. The five most significant metrics are individually displayed and all other metrics placed into the Others category. See Section 1.2, “Filtering cost data with Cost Explorer” for more information about filtering your cost and usage data. 1.1.1. Cost of unallocated resources in an OpenShift cluster Unallocated costs are visible when viewing OpenShift cost data grouped by project. Rows named Platform unallocated and Worker unallocated are available when viewing OpenShift cost data by project. Platform unallocated costs The costs for parts of primary and infrastructure nodes that are not allocated to run workloads. In this case, those workloads are the OpenShift platform or control plane. Worker unallocated costs The costs that represent any unused part of your worker node’s usage and request capacity. Network unattributed costs Costs associated with ingress and egress network traffic for individual nodes. 1.1.2. Unattributed Storage project for OpenShift on Cloud Note Azure and AWS are the only cloud service providers that create an Unattributed Storage project, but in the future, more providers might. Unattributed Storage is a type of project that gets created when cost management is unable to correlate a portion of the cloud cost to an OpenShift namespace. There are two scenarios where Unattributed Storage can happen: Volumes without a claim A persistent volume (PV) exists, but there are no persistent volume claims (PVCs) that use the volume. Without a claim, the cloud cost cannot be associated with a node or namespace. Unutilized disk space In some situations, cost management creates its own project because it cannot determine which project should be charged with the money from the cloud integration. Consider the following example: You have a disk that has 30 GiBs. Project A creates a persistent volume that has a capacity of 20 GiBs. The cloud integration charges $60 for that disk. Cost management uses the following equation to calculate how much to charge Project A based on disk capacity. In this example, the cost is $40: (PV’s Capacity) / Disk Capacity * Cost of Disk 20/30 * 60 = $40 The cloud integration still bills you for the $60 dollars, but {product-title} can only attribute $20 to Project A based off how much they requested. The remaining $40 is assigned to the *Unattributed Storage* project. {product-title-up} calculates the remaining portion of that disk with the following equation. In this example, the cost is $20: ((Disk Capacity - Sum(PV capacity) / Disk Capacity) * Cost of Disk (30 - 20) / 30 * 60 = $20 1.2. Filtering cost data with Cost Explorer To customize the cost data that appears, select items by using the filter options in the Cost Explorer. To change how your cloud and OpenShift costs display, select items in the Perspective menu. To display those costs at different levels of your OpenShift instance, select items in the Project menu. To see how costs are distributed based on CPU or memory metrics in project cost breakdowns, select Overhead cost . Prerequisites You must have your OpenShift cluster OpenShift cluster added as a cost management data integration. For instructions see Integrating OpenShift Container Platform data into cost management . You must have your cloud infrastructure account added as a cost management data integration. For instructions for your cloud provider type see Adding integrations to cost management . Procedure To view cumulative costs from OpenShift or a cloud provider, go to the Cost Explorer and select an item from the Perspective menu. For example, to view your cumulative OpenShift Container Platform costs, select All OpenShift Cost . To further refine your costs at different levels in your OpenShift instance, select a grouping. Depending on your cloud provider you can group by Projects , Nodes , Clusters , or Tags . To search for a particular node, cluster or tag, you can filter by searching for one or more Projects , Nodes , Clusters , or Tags . To refine the time period in your search, click Month to date to select a different time period from the menu. For example, select Last 90 days to view the cost data from the last 90 days. Next Steps After filtering your data, you can export it into a CSV file. See Section 1.3, “Exporting cost data” for more information. 1.2.1. Tracking the cost of ROSA You can track expenses associated with Red Hat OpenShift Service on AWS (ROSA) with Cost Explorer. To find costs associated with ROSA, filter the costs in Cost Explorer and view them grouped by services. On Cost Explorer or AWS details page, view ROSA costs on the Amazon Web Service Filtered by OpenShift view by grouping by service and looking for virtual machine licenses. Procedure In cost management, go to the Cost Explorer page. On the Cost Explorer page, in the Perspective menu, select Amazon Web Services filtered by OpenShift . In the Group by field, select Service . In the results, find the Red Hat OpenShift Service on AWS . Next Steps After you filter your data, you can export it into a CSV file. See Section 1.3, “Exporting cost data” for more information. 1.2.2. Tracking the cost of ARO in OpenShift You can track expenses associated with Microsoft Azure Red Hat OpenShift (ARO) when you calculate the cost of OpenShift for Microsoft Azure. To find costs associated with ARO, filter the costs in Cost Explorer and view them grouped by services. The overall service cost includes all virtual machine license fees on the underlying nodes. For example, if the node is also running Red Hat Enterprise Linux, the specific cost of running ARO is included. Procedure In cost management, go to the Cost Explorer page. On the Cost Explorer page, in the Perspective menu, select Microsoft Azure filtered by OpenShift . In the Group by field, select Service . In the results, search for the virtual machine licences. The service cost includes all of the virtual machine licences fees on each node. Next Steps After you filter your data, you can export it into a CSV file. See Section 1.3, “Exporting cost data” for more information. 1.3. Exporting cost data Use the Cost Explorer to export customized CSV files of cost data to your local workstation. These files contain more details about your cost data that you can apply in your desired reporting tool. Prerequisites Your OpenShift cluster added as a cost management data integration. See Integrating OpenShift Container Platform data into cost management for instructions. Your cloud infrastructure account added as a cost management data integration. See Adding integrations to cost management for instructions for your cloud provider type. Procedure Navigate to the Cost Explorer application. Use the Cost Explorer filter options to create the desired information. Select items to be exported in the list populated below the Cost Explorer graph. Click the Export button. Click Generate and Download . The CSV file will download to your local system. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/visualizing_your_costs_using_cost_explorer/assembly-using-cost-explorer"}
{"title": "Chapter 7. Pulling images from a container repository", "content": "Chapter 7. Pulling images from a container repository Pull automation execution environments from the automation hub remote registry to make a copy to your local machine. Automation hub provides the podman pull command for each latest automation execution environments in the container repository. You can copy and paste this command into your terminal, or use podman pull to copy an automation execution environments based on an automation execution environments tag. 7.1. Pulling an image You can pull automation execution environments from the automation hub remote registry to make a copy to your local machine. Prerequisites You must have permission to view and pull from a private container repository. Procedure If you are pulling automation execution environments from a password or token-protected registry, create a credential before pulling the automation execution environments. From the navigation panel, select Automation Content Execution Environments . Select your automation execution environments. In the Pull this image entry, click Copy to clipboard . Paste and run the command in your terminal. Verification Run podman images to view images on your local machine. 7.2. Syncing images from a container repository You can pull automation execution environments from the automation hub remote registry to sync an image to your local machine. To sync an automation execution environment from a remote registry, you must first configure a remote registry. Prerequisites You must have permission to view and pull from a private container repository. Procedure From the navigation panel, select Automation Content Execution Environments . Add https://registry.redhat.io to the registry. Add any required credentials to authenticate. Note Some remote registries are aggressive with rate limiting. Set a rate limit under Advanced Options . From the navigation panel, select Automation Content Execution Environments . Click Create execution environment in the page header. Select the registry you want to pull from. The Name field displays the name of the automation execution environments displayed on your local registry. Note The Upstream name field is the name of the image on the remote server. For example, if the upstream name is set to \"alpine\" and the Name field is \"local/alpine\", the alpine image is downloaded from the remote and renamed to \"local/alpine\". Set a list of tags to include or exclude. Syncing automation execution environments with a large number of tags is time consuming and uses a lot of disk space. Additional resources See Red Hat Container Registry Authentication for a list of registries. See the What is Podman? documentation for options to use when pulling images. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/creating_and_using_execution_environments/pulling-images-container-repository"}
{"title": "Chapter 3. Resource optimization for OpenShift optimization reports", "content": "Chapter 3. Resource optimization for OpenShift optimization reports Access resource optimization for OpenShift from the Red Hat Hybrid Cloud Console to see detailed recommendations for how to optimize your Red Hat OpenShift clusters. 3.1. Enabling optimization To receive resource optimization recommendations for your namespaces, you must first enable each namespace. To enable a namespace, label it with insights_cost_management_optimizations='true' . In the CLI, run: oc label namespace NAMESPACE insights_cost_management_optimizations=\"true\" --overwrite=true 3.2. Viewing optimization reports Prerequisites You added an OpenShift integration to Red Hat Hybrid Cloud Console . You uploaded at least 24 hours of data from the operator. You logged in to the Red Hat Hybrid Cloud Console . Procedure In cost management, click the tab Optimizations . Search for an optimization or use the filter. Click the link to the optimization that you selected. View details about the recommendation and toggle between Cost optimizations and Performance optimizations . For more information, see Optimizing for cost or for performance . 3.3. Optimizing for cost or for performance After you select an optimization, you can toggle between two tabs called Cost optimizations and Performance optimizations . Optimizing for cost uses less resources and is useful when you are performing tests where there is no impact to users. Optimizing for performance provides all the resources possible and is helpful for apps running in a production cluster. In Cost optimizations , recommendations get generated when CPU usage is at or above the 60th percentile and memory usage is at the 100th percentile. In Performance optimizations , recommendations get generated when CPU usage is at or above the 98th percentile and when memory usage is at the 100th percentile. 3.4. Understanding box plots On the Optimizations page, there are two box plots for your Current CPU utilization and your Current memory utilization . These visualizations can help you understand resource distribution and identify outliers in your data. You can export the data in CSV and JSON format. The box plots display the following data points in millicpu (m) for CPU and in mebibytes (Mi) for memory: Minimum Maximum Median First quartile (Q1): value where 25% of data points are when they are arranged in increasing order Third quartile (Q3): value where 75% of data points are when they are arranged in increasing order Recommended limit Recommended request The data points are based on the time period that you selected: 1 day = 4 data points 7 days = 7 data points 14 days = 14 data points Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/getting_started_with_resource_optimization_for_openshift/optimizations-ros"}
{"title": "Chapter 2. Prerequisites", "content": "Chapter 2. Prerequisites To use resource optimization for OpenShift, you must have completed the following steps: You installed the Cost Management Metrics Operator version 3.3.0 or newer. You added your OpenShift integration to cost management. For more information, see Integrating OpenShift Container Platform data into cost management . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/getting_started_with_resource_optimization_for_openshift/prerequisites"}
{"title": "RBAC APIs", "content": "RBAC APIs  Reference guide for RBAC APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/rbac_apis/index"}
{"title": "Chapter 7. Choosing and obtaining a Red Hat Ansible Automation Platform installer", "content": "Chapter 7. Choosing and obtaining a Red Hat Ansible Automation Platform installer Choose the Red Hat Ansible Automation Platform installer you need based on your Red Hat Enterprise Linux environment internet connectivity. Review the following scenarios to decide which Red Hat Ansible Automation Platform installer meets your needs. 7.1. Installing with internet access Choose the Red Hat Ansible Automation Platform installer if your Red Hat Enterprise Linux environment is connected to the internet. Installing with internet access retrieves the latest required repositories, packages, and dependencies. Choose one of the following ways to set up your Ansible Automation Platform installer. Tarball install Navigate to the Red Hat Ansible Automation Platform download page. Click Download Now for the Ansible Automation Platform <latest-version> Setup . Transfer the file to the target server using scp or curl : Using scp : Run the following command, replacing private_key.pem , user , and server_ip with your appropriate values: $ scp -i private_key.pem aap-bundled-installer.tar.gz user@server_ip: Using curl : If the setup file URL is available, you can download it directly to the target server using curl . Replace <download_url> with the file URL: $ curl -0 <download_url> Note If the file needs to be extracted after downloading, run the following command: $ tar xvzf aap-bundled-installer.tar.gz RPM install Install Ansible Automation Platform Installer Package v.2.5 for RHEL 8 for x86_64 $ sudo dnf install --enablerepo=ansible-automation-platform-2.5-for-rhel-8-x86_64-rpms ansible-automation-platform-installer v.2.5 for RHEL 9 for x86_64 $ sudo dnf install --enablerepo=ansible-automation-platform-2.5-for-rhel-9-x86_64-rpms ansible-automation-platform-installer Note dnf install enables the repo as the repo is disabled by default. When you use the RPM installer, the files are placed under the /opt/ansible-automation-platform/installer directory. 7.2. Installing without internet access Use the Red Hat Ansible Automation Platform Bundle installer if you are unable to access the internet, or would prefer not to install separate components and dependencies from online repositories. Access to Red Hat Enterprise Linux repositories is still needed. All other dependencies are included in the tar archive. Procedure Go to the Red Hat Ansible Automation Platform download page. Click Download Now for the Ansible Automation Platform <latest-version> Setup Bundle . Extract the files: $ tar xvzf ansible-automation-platform-setup-bundle-<latest-version>.tar.gz Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/planning_your_installation/choosing_and_obtaining_a_red_hat_ansible_automation_platform_installer"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/planning_your_installation/legal-notice"}
{"title": "Chapter 4. Overview of tested deployment models", "content": "Chapter 4. Overview of tested deployment models Red Hat tests Ansible Automation Platform 2.5 with a defined set of topologies to give you opinionated deployment options. Deploy all components of Ansible Automation Platform so that all features and capabilities are available for use without the need to take further action. Red Hat tests the installation of Ansible Automation Platform 2.5 based on a defined set of infrastructure topologies or reference architectures. Enterprise organizations can use one of the enterprise topologies for production deployments to ensure the highest level of uptime, performance, and continued scalability. Organizations or deployments that are resource constrained can use a \"growth\" topology. It is possible to install the Ansible Automation Platform on different infrastructure topologies and with different environment configurations. Red Hat does not fully test topologies outside of published reference architectures. Use a tested topology for all new deployments. 4.1. Installation and deployment models The following table outlines the different ways to install or deploy Ansible Automation Platform: Table 4.1. Ansible Automation Platform installation and deployment models Mode Infrastructure Description Tested topologies RPM Virtual machines and bare metal The RPM installer deploys Ansible Automation Platform on Red Hat Enterprise Linux by using RPMs to install the platform on host machines. Customers manage the product and infrastructure lifecycle. RPM growth topology RPM mixed growth topology RPM enterprise topology RPM mixed enterprise topology Containers Virtual machines and bare metal The containerized installer deploys Ansible Automation Platform on Red Hat Enterprise Linux by using Podman which runs the platform in containers on host machines. Customers manage the product and infrastructure lifecycle. Container growth topology Container enterprise topology Operator Red Hat OpenShift The Operator uses Red Hat OpenShift Operators to deploy Ansible Automation Platform within Red Hat OpenShift. Customers manage the product and infrastructure lifecycle. Operator growth topology Operator enterprise topology Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/planning_your_installation/overview-tested-deployment-models"}
{"title": "Jenkins", "content": "Jenkins  Jenkins Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/jenkins/index"}
{"title": "Appendix A. DNF commands list", "content": "Appendix A. DNF commands list In the following sections, examine DNF commands for listing, installing, and removing content in Red Hat Enterprise Linux 9. A.1. Commands for listing content in RHEL 9 The following are the commonly used DNF commands for finding content and its details in Red Hat Enterprise Linux 9: Command Description dnf search term Search for a package by using term related to the package. dnf repoquery package Search for enabled DNF repositories for a selected package and its version. dnf list List information about all installed and available packages. dnf list --installed dnf repoquery --installed List all packages installed on your system. dnf list --available dnf repoquery List all packages in all enabled repositories that are available to install. dnf repolist List all enabled repositories on your system. dnf repolist --disabled List all disabled repositories on your system. dnf repolist --all List both enabled and disabled repositories. dnf repoinfo List additional information about the repositories. dnf info package_name dnf repoquery --info package_name Display details of an available package. dnf repoquery --info --installed package_name Display details of a package installed on your system. dnf module list List modules and their current status. dnf module info module_name Display details of a module. dnf module list module_name Display the current status of a module. dnf module info --profile module_name Display packages associated with available profiles of a selected module. dnf module info --profile module_name:stream Display packages associated with available profiles of a module by using a specified stream. dnf module provides package Determine which modules, streams, and profiles provide a package. Note that if the package is available outside any modules, the output of this command is empty. dnf group summary View the number of installed and available groups. dnf group list List all installed and available groups. dnf group info group_name List mandatory and optional packages included in a particular group. A.2. Commands for installing content in RHEL 9 The following are the commonly used DNF commands for installing content in Red Hat Enterprise Linux 9: Command Description dnf install package_name Install a package. If the package is provided by a module stream, dnf resolves the required module stream and enables it automatically while installing this package. This also happens recursively for all package dependencies. If more module streams satisfy the requirement, the default ones are used. dnf install package_name_1 package_name_2 Install multiple packages and their dependencies simultaneously. dnf install package_name.arch Specify the architecture of the package by appending it to the package name when installing packages on a multilib system (AMD64, Intel 64 machine). dnf install /usr/sbin/binary_file Install a binary by using the path to the binary as an argument. dnf install /path/ Install a previously downloaded package from a local directory. dnf install package_url Install a remote package by using a package URL. dnf module enable module_name:stream Enable a module by using a specific stream. Note that running this command does not install any RPM packages. dnf module install module_name:stream dnf install @ module_name:stream Install a default profile from a specific module stream. Note that running this command also enables the specified stream. dnf module install module_name:stream/profile dnf install @ module_name:stream/profile Install a selected profile by using a specific stream. dnf group install group_name Install a package group by a group name. dnf group install group_ID Install a package group by the groupID. A.3. Commands for removing content in RHEL 9 The following are the commonly used DNF commands for removing content in Red Hat Enterprise Linux 9: Command Description dnf remove package_name Remove a particular package and all dependent packages. dnf remove package_name_1 package_name_2 Remove multiple packages and their unused dependencies simultaneously. dnf group remove group_name Remove a package group by the group name. dnf group remove group_ID Remove a package group by the groupID. dnf module remove --all module_name:stream Remove all packages from the specified stream. Note that running this command can remove critical packages from your system. dnf module remove module_name:stream/profile Remove packages from an installed profile. dnf module remove module_name:stream Remove packages from all installed profiles within the specified stream. dnf module reset module_name Reset a module to the initial state. Note that running this command does not remove packages from the specified module. dnf module disable module_name Disable a module and all its streams. Note that running this command does not remove packages from the specified module. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_software_with_the_dnf_tool/assembly_yum-commands-list_managing-software-with-the-dnf-tool"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/managing_software_with_the_dnf_tool/legal-notice"}
{"title": "Chapter 25. Running special container images", "content": "Chapter 25. Running special container images You can run some special types of container images. Some container images have built-in labels called runlabels that enable you to run those containers with preset options and arguments. The podman container runlabel <label> command, you can execute the command defined in the <label> for the container image. Supported labels are install , run and uninstall . 25.1. Opening privileges to the host There are several differences between privileged and non-privileged containers. For example, the toolbox container is a privileged container. Here are examples of privileges that may or may not be open to the host from a container: Privileges : A privileged container disables the security features that isolate the container from the host. You can run a privileged container using the podman run --privileged <image_name> command. You can, for example, delete files and directories mounted from the host that are owned by the root user. Process tables : You can use the podman run --privileged --pid=host <image_name> command to use the host PID namespace for the container. Then you can use the ps -e command within a privileged container to list all processes running on the host. You can pass a process ID from the host to commands that run in the privileged container (for example, kill <PID> ). Network interfaces : By default, a container has only one external network interface and one loopback network interface. You can use the podman run --net=host <image_name> command to access host network interfaces directly from within the container. Inter-process communications : The IPC facility on the host is accessible from within the privileged container. You can run commands such as ipcs to see information about active message queues, shared memory segments, and semaphore sets on the host. 25.2. Container images with runlabels Some Red Hat images include labels that provide pre-set command lines for working with those images. Using the podman container runlabel <label> command, you can use the podman command to execute the command defined in the <label> for the image. Existing runlabels include: install : Sets up the host system before executing the image. Typically, this results in creating files and directories on the host that the container can access when it is run later. run : Identifies podman command line options to use when running the container. Typically, the options will open privileges on the host and mount the host content the container needs to remain permanently on the host. uninstall : Cleans up the host system after you finish running the container. 25.3. Running rsyslog with runlabels The rhel9/rsyslog container image is made to run a containerized version of the rsyslogd daemon. The rsyslog image contains the following runlabels: install , run and uninstall . The following procedure steps you through installing, running, and uninstalling the rsyslog image: Prerequisites The container-tools meta-package is installed. Procedure Pull the rsyslog image: # podman pull registry.redhat.io/rhel9/rsyslog Display the install runlabel for rsyslog : # podman container runlabel install --display rhel9/rsyslog command: podman run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=registry.redhat.io/rhel9/rsyslog:latest -e NAME=rsyslog registry.redhat.io/rhel9/rsyslog:latest /bin/install.sh This shows that the command will open privileges to the host, mount the host root filesystem on /host in the container, and run an install.sh script. Run the install runlabel for rsyslog : # podman container runlabel install rhel9/rsyslog command: podman run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=registry.redhat.io/rhel9/rsyslog:latest -e NAME=rsyslog registry.redhat.io/rhel9/rsyslog:latest /bin/install.sh Creating directory at /host//etc/pki/rsyslog Creating directory at /host//etc/rsyslog.d Installing file at /host//etc/rsyslog.conf Installing file at /host//etc/sysconfig/rsyslog Installing file at /host//etc/logrotate.d/syslog This creates files on the host system that the rsyslog image will use later. Display the run runlabel for rsyslog : # podman container runlabel run --display rhel9/rsyslog command: podman run -d --privileged --name rsyslog --net=host --pid=host -v /etc/pki/rsyslog:/etc/pki/rsyslog -v /etc/rsyslog.conf:/etc/rsyslog.conf -v /etc/sysconfig/rsyslog:/etc/sysconfig/rsyslog -v /etc/rsyslog.d:/etc/rsyslog.d -v /var/log:/var/log -v /var/lib/rsyslog:/var/lib/rsyslog -v /run:/run -v /etc/machine-id:/etc/machine-id -v /etc/localtime:/etc/localtime -e IMAGE=registry.redhat.io/rhel9/rsyslog:latest -e NAME=rsyslog --restart=always registry.redhat.io/rhel9/rsyslog:latest /bin/rsyslog.sh This shows that the command opens privileges to the host and mount specific files and directories from the host inside the container, when it launches the rsyslog container to run the rsyslogd daemon. Execute the run runlabel for rsyslog : # podman container runlabel run rhel9/rsyslog command: podman run -d --privileged --name rsyslog --net=host --pid=host -v /etc/pki/rsyslog:/etc/pki/rsyslog -v /etc/rsyslog.conf:/etc/rsyslog.conf -v /etc/sysconfig/rsyslog:/etc/sysconfig/rsyslog -v /etc/rsyslog.d:/etc/rsyslog.d -v /var/log:/var/log -v /var/lib/rsyslog:/var/lib/rsyslog -v /run:/run -v /etc/machine-id:/etc/machine-id -v /etc/localtime:/etc/localtime -e IMAGE=registry.redhat.io/rhel9/rsyslog:latest -e NAME=rsyslog --restart=always registry.redhat.io/rhel9/rsyslog:latest /bin/rsyslog.sh 28a0d719ff179adcea81eb63cc90fcd09f1755d5edb121399068a4ea59bd0f53 The rsyslog container opens privileges, mounts what it needs from the host, and runs the rsyslogd daemon in the background ( -d ). The rsyslogd daemon begins gathering log messages and directing messages to files in the /var/log directory. Display the uninstall runlabel for rsyslog : # podman container runlabel uninstall --display rhel9/rsyslog command: podman run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=registry.redhat.io/rhel9/rsyslog:latest -e NAME=rsyslog registry.redhat.io/rhel9/rsyslog:latest /bin/uninstall.sh Run the uninstall runlabel for rsyslog : # podman container runlabel uninstall rhel9/rsyslog command: podman run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=registry.redhat.io/rhel9/rsyslog:latest -e NAME=rsyslog registry.redhat.io/rhel9/rsyslog:latest /bin/uninstall.sh Note In this case, the uninstall.sh script just removes the /etc/logrotate.d/syslog file. It does not clean up the configuration files. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_running-special-container-images"}
{"title": "Chapter 24. Using Podman in HPC environment", "content": "Chapter 24. Using Podman in HPC environment You can use Podman with Open MPI (Message Passing Interface) to run containers in a High Performance Computing (HPC) environment. 24.1. Using Podman with MPI The example is based on the ring.c program taken from Open MPI. In this example, a value is passed around by all processes in a ring-like fashion. Each time the message passes rank 0, the value is decremented. When each process receives the 0 message, it passes it on to the next process and then quits. By passing the 0 first, every process gets the 0 message and can quit normally. Prerequisites The container-tools meta-package is installed. Procedure Install Open MPI: # dnf install openmpi To activate the environment modules, type: $ . /etc/profile.d/modules.sh Load the mpi/openmpi-x86_64 module: $ module load mpi/openmpi-x86_64 Optionally, to automatically load mpi/openmpi-x86_64 module, add this line to the .bashrc file: $ echo \"module load mpi/openmpi-x86_64\" >> .bashrc To combine mpirun and podman , create a container with the following definition: $ cat Containerfile FROM registry.access.redhat.com/ubi9/ubi RUN dnf -y install openmpi-devel wget && \\ dnf clean all RUN wget https://raw.githubusercontent.com/open-mpi/ompi/master/test/simple/ring.c && \\ /usr/lib64/openmpi/bin/mpicc ring.c -o /home/ring && \\ rm -f ring.c Build the container: $ podman build --tag=mpi-ring . Start the container. On a system with 4 CPUs this command starts 4 containers: $ mpirun \\ --mca orte_tmpdir_base /tmp/podman-mpirun \\ podman run --env-host \\ -v /tmp/podman-mpirun:/tmp/podman-mpirun \\ --userns=keep-id \\ --net=host --pid=host --ipc=host \\ mpi-ring /home/ring Rank 2 has cleared MPI_Init Rank 2 has completed ring Rank 2 has completed MPI_Barrier Rank 3 has cleared MPI_Init Rank 3 has completed ring Rank 3 has completed MPI_Barrier Rank 1 has cleared MPI_Init Rank 1 has completed ring Rank 1 has completed MPI_Barrier Rank 0 has cleared MPI_Init Rank 0 has completed ring Rank 0 has completed MPI_Barrier As a result, mpirun starts up 4 Podman containers and each container is running one instance of the ring binary. All 4 processes are communicating over MPI with each other. Additional resources Podman in HPC environments 24.2. The mpirun options The following mpirun options are used to start the container: --mca orte_tmpdir_base /tmp/podman-mpirun line tells Open MPI to create all its temporary files in /tmp/podman-mpirun and not in /tmp . If using more than one node this directory will be named differently on other nodes. This requires mounting the complete /tmp directory into the container which is more complicated. The mpirun command specifies the command to start, the podman command. The following podman options are used to start the container: run command runs a container. --env-host option copies all environment variables from the host into the container. -v /tmp/podman-mpirun:/tmp/podman-mpirun line tells Podman to mount the directory where Open MPI creates its temporary directories and files to be available in the container. --userns=keep-id line ensures the user ID mapping inside and outside the container. --net=host --pid=host --ipc=host line sets the same network, PID and IPC namespaces. mpi-ring is the name of the container. /home/ring is the MPI program in the container. Additional resources Podman in HPC environments Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_using-podman-in-hpc-environment"}
{"title": "Troubleshooting issues", "content": "Troubleshooting issues Red Hat OpenShift GitOps 1.15 Troubleshooting topics for OpenShift GitOps and your cluster Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_gitops/1.15/html/troubleshooting_issues/index"}
{"title": "Chapter 23. Using Toolbx for development and troubleshooting", "content": "Chapter 23. Using Toolbx for development and troubleshooting Installing software on a system presents certain risks: it can change a system’s behavior, and can leave unwanted files and directories behind after they are no longer needed. You can prevent these risks by installing your favorite development and debugging tools, editors, and software development kits (SDKs) into the Toolbx fully mutable container without affecting the base operating system. You can perform changes on the host system with commands such as less , lsof , rsync , ssh , sudo , and unzip . The Toolbx utility performs the following actions: Pulling the registry.access.redhat.com/ubi9/toolbox:latest image to your local system Starting up a container from the image Running a shell inside the container from which you can access the host system Note Toolbx can run a root container or a rootless container, depending on the rights of the user who creates the Toolbx container. Utilities that would require root rights on the host system also should be run in root containers. The default container name is rhel-toolbox . 23.1. Starting a Toolbx container You can create a Toolbx container by using the toolbox create command. You can then enter the container with the toolbox enter command. Procedure Create a Toolbx container: As a rootless user: $ toolbox create <mytoolbox> As a root user: $ sudo toolbox create <mytoolbox> Created container: <mytoolbox> Enter with: toolbox enter Verify that you pulled the correct image: [user@toolbox ~]$ toolbox list IMAGE ID IMAGE NAME CREATED fe0ae375f149 registry.access.redhat.com/ubi{ProductVersion}/toolbox 5 weeks ago CONTAINER ID CONTAINER NAME CREATED STATUS IMAGE NAME 5245b924c2cb <mytoolbox> 7 minutes ago created registry.access.redhat.com/ubi{ProductVersion}/toolbox:8.9-6 Enter the Toolbx container: [user@toolbox ~]$ toolbox enter <mytoolbox> Verification Enter a command inside the <mytoolbox> container and display the name of the container and the image: ⬢ [user@toolbox ~]$ cat /run/.containerenv engine=\"podman-4.8.2\" name=\" <mytoolbox> \" id=\"5245b924c2cb...\" image=\"registry.access.redhat.com/ubi{ProductVersion}/toolbox\" imageid=\"fe0ae375f14919cbc0596142e3aff22a70973a36e5a165c75a86ea7ec5d8d65c\" 23.2. Using Toolbx for development You can use a Toolbx container as a rootless user for installation of development tools, such as editors, compilers, and software development kits (SDKs). After installation, you can continue using those tools as a rootless user. Prerequisites The Toolbx container is created and is running. You entered the Toolbx container. You do not need to create the Toolbx container with root privileges. See Starting a Toolbox container . Procedure Install the tools of your choice, for example, the Emacs text editor, GCC compiler and GNU Debugger (GDB): ⬢[user@toolbox ~]$ sudo dnf install emacs gcc gdb Verification Verify that the tools are installed: ⬢[user@toolbox ~]$ dnf repoquery --info --installed <package_name> 23.3. Using Toolbx for troubleshooting a host system You can use a Toolbx container with root privileges to find the root cause of various problems with the host system by using tools such as systemd , journalctl , and nmap , without installing them on the host system. Inside the Toolbx container you can, for example, perform the following actions. Prerequisites The Toolbx container is created and is running. You entered the Toolbx container. You need to create the Toolbx container with root privileges. See Starting a Toolbox container . Procedure Install the systemd suite to be able to run the journalctl command: ⬢[root@toolbox ~]# dnf install systemd Display log messages for all processes running on the host: ⬢[root@toolbox ~]# j journalctl --boot -0 Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: microcode: updated ear> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: Linux version 6.6.8-10> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: Command line: BOOT_IMA> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: x86/split lock detecti> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: BIOS-provided physical> Display log messages for the kernel: ⬢[root@toolbox ~]# journalctl --boot -0 --dmesg Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: microcode: updated ear> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: Linux version 6.6.8-10> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: Command line: BOOT_IMA> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: x86/split lock detecti> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: BIOS-provided physical> Jan 02 09:06:48 user-thinkpadp1gen4i.brq.csb kernel: BIOS-e820: [mem 0x0000> Install the nmap network scanning tool: ⬢[root@toolbox ~]# dnf install nmap Scan IP addresses and ports in a network: ⬢[root@toolbox ~]# nmap -sS scanme.nmap.org Starting Nmap 7.93 ( https://nmap.org ) at 2024-01-02 10:39 CET Stats: 0:01:01 elapsed; 0 hosts completed (0 up), 256 undergoing Ping Scan Ping Scan Timing: About 29.79% done; ETC: 10:43 (0:02:24 remaining) Nmap done: 256 IP addresses (0 hosts up) scanned in 206.45 seconds The -sS option performs a TCP SYN scan. Most of Nmap’s scan types are only available to privileged users, because they send and receive raw packets, which requires root access on UNIX systems. 23.4. Stopping the Toolbx container Use the exit command to leave the Toolbox container and the podman stop command to stop the container. Procedure Leave the container and return to the host: ⬢ [user@toolbox ~]$ exit Stop the toolbox container: ⬢ [user@toolbox ~]$ podman stop <mytoolbox> Optional: Remove the toolbox container: ⬢ [user@toolbox ~]$ toolbox rm <mytoolbox> Alternatively, you can also use the podman rm command to remove the container. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/using-toolbx-for-development-and-troubleshooting"}
{"title": "Role APIs", "content": "Role APIs  Reference guide for role APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/role_apis/index"}
{"title": "Chapter 22. Creating and restoring container checkpoints", "content": "Chapter 22. Creating and restoring container checkpoints Checkpoint/Restore In Userspace (CRIU) is a software that enables you to set a checkpoint on a running container or an individual application and store its state to disk. You can use data saved to restore the container after a reboot at the same point in time it was checkpointed. Warning The kernel does not support pre-copy checkpointing on AArch64. 22.1. Creating and restoring a container checkpoint locally This example is based on a Python based web server which returns a single integer which is incremented after each request. Prerequisites The container-tools meta-package is installed. Procedure Create a Python based server: # cat counter.py #!/usr/bin/python3 import http.server counter = 0 class handler(http.server.BaseHTTPRequestHandler): def do_GET(s): global counter s.send_response(200) s.send_header('Content-type', 'text/html') s.end_headers() s.wfile.write(b'%d\\n' % counter) counter += 1 server = http.server.HTTPServer(('', 8088), handler) server.serve_forever() Create a container with the following definition: # cat Containerfile FROM registry.access.redhat.com/ubi9/ubi COPY counter.py /home/counter.py RUN useradd -ms /bin/bash counter RUN dnf -y install python3 && chmod 755 /home/counter.py USER counter ENTRYPOINT /home/counter.py The container is based on the Universal Base Image (UBI 8) and uses a Python based server. Build the container: # podman build . --tag counter Files counter.py and Containerfile are the input for the container build process ( podman build ). The built image is stored locally and tagged with the tag counter . Start the container as root: # podman run --name criu-test --detach counter To list all running containers, enter: # podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e4f82fd84d48 localhost/counter:latest 5 seconds ago Up 4 seconds ago criu-test Display IP address of the container: # podman inspect criu-test --format \"{{.NetworkSettings.IPAddress}}\" 10.88.0.247 Send requests to the container: # curl 10.88.0.247:8088 0 # curl 10.88.0.247:8088 1 Create a checkpoint for the container: # podman container checkpoint criu-test Reboot the system. Restore the container: # podman container restore --keep criu-test Send requests to the container: # curl 10.88.0.247:8080 2 # curl 10.88.0.247:8080 3 # curl 10.88.0.247:8080 4 The result now does not start at 0 again, but continues at the previous value. This way you can easily save the complete container state through a reboot. Additional resources Podman checkpoint 22.2. Reducing startup time using container restore You can use container migration to reduce startup time of containers which require a certain time to initialize. Using a checkpoint, you can restore the container multiple times on the same host or on different hosts. This example is based on the container from the Creating and restoring a container checkpoint locally . Prerequisites The container-tools meta-package is installed. Procedure Create a checkpoint of the container, and export the checkpoint image to a tar.gz file: # podman container checkpoint criu-test --export /tmp/chkpt.tar.gz Restore the container from the tar.gz file: # podman container restore --import /tmp/chkpt.tar.gz --name counter1 # podman container restore --import /tmp/chkpt.tar.gz --name counter2 # podman container restore --import /tmp/chkpt.tar.gz --name counter3 The --name ( -n ) option specifies a new name for containers restored from the exported checkpoint. Display ID and name of each container: # podman ps -a --format \"{{.ID}} {{.Names}}\" a8b2e50d463c counter3 faabc5c27362 counter2 2ce648af11e5 counter1 Display IP address of each container: #️ podman inspect counter1 --format \"{{.NetworkSettings.IPAddress}}\" 10.88.0.248 #️ podman inspect counter2 --format \"{{.NetworkSettings.IPAddress}}\" 10.88.0.249 #️ podman inspect counter3 --format \"{{.NetworkSettings.IPAddress}}\" 10.88.0.250 Send requests to each container: #️ curl 10.88.0.248:8080 4 #️ curl 10.88.0.249:8080 4 #️ curl 10.88.0.250:8080 4 Note, that the result is 4 in all cases, because you are working with different containers restored from the same checkpoint. Using this approach, you can quickly start up stateful replicas of the initially checkpointed container. Additional resources Container migration with Podman on . Migrating containers among systems You can migrate the running containers from one system to another, without losing the state of the applications running in the container. This example is based on the container from the Creating and restoring a container checkpoint locally section tagged with counter . Important Migrating containers among systems with the podman container checkpoint and podman container restore commands is supported only when the configurations of the systems match completely, as shown below: Podman version OCI runtime (runc/crun) Network stack (CNI/Netavark) Cgroups version kernel version CPU features You can migrate to a CPU with more features, but not to a CPU which does not have a certain feature that you are using. The low-level tool doing the checkpointing (CRIU) has the possibility to check for CPU feature compatibility: https://criu.org/Cpuinfo . Prerequisites The container-tools meta-package is installed. The following steps are not necessary if the container is pushed to a registry as Podman will automatically download the container from a registry if it is not available locally. This example does not use a registry, you have to export previously built and tagged container (see Creating and restoring a container checkpoint locally ). Export previously built container: # podman save --output counter.tar counter Copy exported container image to the destination system ( other_host ): # scp counter.tar other_host : Import exported container on the destination system: # ssh other_host podman load --input counter.tar Now the destination system of this container migration has the same container image stored in its local container storage. Procedure Start the container as root: # podman run --name criu-test --detach counter Display IP address of the container: # podman inspect criu-test --format \"{{.NetworkSettings.IPAddress}}\" 10.88.0.247 Send requests to the container: # curl 10.88.0.247:8080 0 # curl 10.88.0.247:8080 1 Create a checkpoint of the container, and export the checkpoint image to a tar.gz file: # podman container checkpoint criu-test --export /tmp/chkpt.tar.gz Copy the checkpoint archive to the destination host: # scp /tmp/chkpt.tar.gz other_host :/tmp/ Restore the checkpoint on the destination host ( other_host ): # podman container restore --import /tmp/chkpt.tar.gz Send a request to the container on the destination host ( other_host ): # *curl 10.88.0.247:8080* 2 As a result, the stateful container has been migrated from one system to another without losing its state. Additional resources Container migration with Podman on RHEL Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_creating-and-restoring-container-checkpoints"}
{"title": "Chapter 21. Monitoring containers", "content": "Chapter 21. Monitoring containers Use Podman commands to manage a Podman environment. With that, you can determine the health of the container, by displaying system and pod information, and monitoring Podman events. 21.1. Using a health check on a container You can use the health check to determine the health or readiness of the process running inside the container. If the health check succeeds, the container is marked as \"healthy\"; otherwise, it is \"unhealthy\". You can compare a health check with running the podman exec command and examining the exit code. The zero exit value means that the container is \"healthy\". Health checks can be set when building an image using the HEALTHCHECK instruction in the Containerfile or when creating the container on the command line. You can display the health-check status of a container using the podman inspect or podman ps commands. A health check consists of six basic components: Command Retries Interval Start-period Timeout Container recovery The description of health check components follows: Command ( --health-cmd option) Podman executes the command inside the target container and waits for the exit code. The other five components are related to the scheduling of the health check and they are optional. Retries ( --health-retries option) Defines the number of consecutive failed health checks that need to occur before the container is marked as \"unhealthy\". A successful health check resets the retry counter. Interval ( --health-interval option) Describes the time between running the health check command. Note that small intervals cause your system to spend a lot of time running health checks. The large intervals cause struggles with catching time outs. Start-period ( --health-start-period option) Describes the time between when the container starts and when you want to ignore health check failures. Timeout ( --health-timeout option) Describes the period of time the health check must complete before being considered unsuccessful. Note The values of the Retries, Interval, and Start-period components are time durations, for example “30s” or “1h15m”. Valid time units are \"ns,\" \"us,\" or \"µs\", \"ms,\" \"s,\" \"m,\" and \"h\". Container recovery ( --health-on-failure option) Determines which actions to perform when the status of a container is unhealthy. When the application fails, Podman restarts it automatically to provide robustness. The --health-on-failure option supports four actions: none : Take no action, this is the default action. kill : Kill the container. restart : Restart the container. stop : Stop the container. Note The --health-on-failure option is available in Podman version 4.2 and later. Warning Do not combine the restart action with the --restart option. When running inside of a systemd unit, consider using the kill or stop action instead, to make use of systemd restart policy. Health checks run inside the container. Health checks only make sense if you know what the health state of the service is and can differentiate between a successful and unsuccessful health check. Additional resources podman-healthcheck and podman-run man pages on your system Podman at the edge: Keeping services alive with custom healthcheck actions Monitoring container vitality and availability with Podman 21.2. Performing a health check using the command line You can set a health check when creating the container on the command line. Prerequisites The container-tools meta-package is installed. Procedure Define a health check: $ podman run -dt --name=hc-container -p 8080:8080 --health-cmd='curl http://localhost:8080 || exit 1' --health-interval=0 registry.access.redhat.com/ubi8/httpd-24 The --health-cmd option sets a health check command for the container. The --health-interval=0 option with 0 value indicates that you want to run the health check manually. Check the health status of the hc-container container: Using the podman inspect command: $ podman inspect --format='{{json .State.Health.Status}}' hc-container healthy Using the podman ps command: $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a680c6919fe localhost/hc-container:latest /usr/bin/run-http... 2 minutes ago Up 2 minutes (healthy) hc-container Using the podman healthcheck run command: $ podman healthcheck run hc-container healthy Additional resources podman-healthcheck and podman-run man pages on your system Podman at the edge: Keeping services alive with custom healthcheck actions Monitoring container vitality and availability with Podman 21.3. Performing a health check using a Containerfile You can set a health check by using the HEALTHCHECK instruction in the Containerfile . Prerequisites The container-tools meta-package is installed. Procedure Create a Containerfile : $ cat Containerfile FROM registry.access.redhat.com/ubi8/httpd-24 EXPOSE 8080 HEALTHCHECK CMD curl http://localhost:8080 || exit 1 Note The HEALTHCHECK instruction is supported only for the docker image format. For the oci image format, the instruction is ignored. Build the container and add an image name: $ podman build --format=docker -t hc-container . STEP 1/3: FROM registry.access.redhat.com/ubi8/httpd-24 STEP 2/3: EXPOSE 8080 --> 5aea97430fd STEP 3/3: HEALTHCHECK CMD curl http://localhost:8080 || exit 1 COMMIT health-check Successfully tagged localhost/health-check:latest a680c6919fe6bf1a79219a1b3d6216550d5a8f83570c36d0dadfee1bb74b924e Run the container: $ podman run -dt --name=hc-container localhost/hc-container Check the health status of the hc-container container: Using the podman inspect command: $ podman inspect --format='{{json .State.Health.Status}}' hc-container healthy Using the podman ps command: $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a680c6919fe localhost/hc-container:latest /usr/bin/run-http... 2 minutes ago Up 2 minutes (healthy) hc-container Using the podman healthcheck run command: $ podman healthcheck run hc-container healthy Additional resources podman-healthcheck and podman-run man pages on your system Podman at the edge: Keeping services alive with custom healthcheck actions Monitoring container vitality and availability with Podman 21.4. Displaying Podman system information The podman system command enables you to manage the Podman systems by displaying system information. Prerequisites The container-tools meta-package is installed. Procedure Display Podman system information: To show Podman disk usage, enter: $ podman system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 3 2 1.085GB 233.4MB (0%) Containers 2 0 28.17kB 28.17kB (100%) Local Volumes 3 0 0B 0B (0%) To show detailed information about space usage, enter: $ podman system df -v Images space usage: REPOSITORY TAG IMAGE ID CREATED SIZE SHARED SIZE UNIQUE SIZE CONTAINERS registry.access.redhat.com/ubi9 latest b1e63aaae5cf 13 days 233.4MB 233.4MB 0B 0 registry.access.redhat.com/ubi9/httpd-24 latest 0d04740850e8 13 days 461.5MB 0B 461.5MB 1 registry.redhat.io/rhel8/podman latest dce10f591a2d 13 days 390.6MB 233.4MB 157.2MB 1 Containers space usage: CONTAINER ID IMAGE COMMAND LOCAL VOLUMES SIZE CREATED STATUS NAMES 311180ab99fb 0d04740850e8 /usr/bin/run-httpd 0 28.17kB 16 hours exited hc1 bedb6c287ed6 dce10f591a2d podman run ubi9 echo hello 0 0B 11 hours configured dazzling_tu Local Volumes space usage: VOLUME NAME LINKS SIZE 76de0efa83a3dae1a388b9e9e67161d28187e093955df185ea228ad0b3e435d0 0 0B 8a1b4658aecc9ff38711a2c7f2da6de192c5b1e753bb7e3b25e9bf3bb7da8b13 0 0B d9cab4f6ccbcf2ac3cd750d2efff9d2b0f29411d430a119210dd242e8be20e26 0 0B To display information about the host, current storage stats, and build of Podman, enter: $ podman system info host: arch: amd64 buildahVersion: 1.22.3 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.0.29-1.module+el8.5.0+12381+e822eb26.x86_64 path: /usr/bin/conmon version: 'conmon version 2.0.29, commit: 7d0fa63455025991c2fc641da85922fde889c91b' cpus: 2 distribution: distribution: '\"rhel\"' version: \"8.5\" eventLogger: file hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 4.18.0-323.el8.x86_64 linkmode: dynamic memFree: 352288768 memTotal: 2819129344 ociRuntime: name: runc package: runc-1.0.2-1.module+el8.5.0+12381+e822eb26.x86_64 path: /usr/bin/runc version: |- runc version 1.0.2 spec: 1.0.2-dev go: go1.16.7 libseccomp: 2.5.1 os: linux remoteSocket: path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.1.8-1.module+el8.5.0+12381+e822eb26.x86_64 version: |- slirp4netns version 1.1.8 commit: d361001f495417b880f20329121e3aa431a8f90f libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.1 swapFree: 3113668608 swapTotal: 3124752384 uptime: 11h 24m 12.52s (Approximately 0.46 days) registries: search: - registry.fedoraproject.org - registry.access.redhat.com - registry.centos.org - docker.io store: configFile: /home/user/.config/containers/storage.conf containerStore: number: 2 paused: 0 running: 0 stopped: 2 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.7.1-1.module+el8.5.0+12381+e822eb26.x86_64 Version: |- fusermount3 version: 3.2.1 fuse-overlayfs: version 1.7.1 FUSE library version 3.2.1 using FUSE kernel interface version 7.26 graphRoot: /home/user/.local/share/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: \"false\" Supports d_type: \"true\" Using metacopy: \"false\" imageStore: number: 3 runRoot: /run/user/1000/containers volumePath: /home/user/.local/share/containers/storage/volumes version: APIVersion: 3.3.1 Built: 1630360721 BuiltTime: Mon Aug 30 23:58:41 2021 GitCommit: \"\" GoVersion: go1.16.7 OsArch: linux/amd64 Version: 3.3.1 To remove all unused containers, images and volume data, enter: $ podman system prune WARNING! This will remove: - all stopped containers - all stopped pods - all dangling images - all build cache Are you sure you want to continue? [y/N] y The podman system prune command removes all unused containers (both dangling and unreferenced), pods and optionally, volumes from local storage. Use the --all option to delete all unused images. Unused images are dangling images and any image that does not have any containers based on it. Use the --volume option to prune volumes. By default, volumes are not removed to prevent important data from being deleted if there is currently no container using the volume. Additional resources podman-system-df , podman-system-info , and podman-system-prune man pages on your system 21.5. Podman event types You can monitor events that occur in Podman. Several event types exist and each event type reports different statuses. The container event type reports the following statuses: attach checkpoint cleanup commit create exec export import init kill mount pause prune remove restart restore start stop sync unmount unpause The pod event type reports the following statuses: create kill pause remove start stop unpause The image event type reports the following statuses: prune push pull save remove tag untag The system type reports the following statuses: refresh renumber The volume type reports the following statuses: create prune remove Additional resources podman-events man page on your system 21.6. Monitoring Podman events You can monitor and print events that occur in Podman using the podman events command. Each event will include a timestamp, a type, a status, name, if applicable, and image, if applicable. Prerequisites The container-tools meta-package is installed. Procedure Run the myubi container: $ podman run -q --rm --name=myubi registry.access.redhat.com/ubi8/ubi:latest Display the Podman events: To display all Podman events, enter: $ now=$(date --iso-8601=seconds) $ podman events --since=now --stream=false 2023-03-08 14:27:20.696167362 +0100 CET container create d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi,...) 2023-03-08 14:27:20.652325082 +0100 CET image pull registry.access.redhat.com/ubi8/ubi:latest 2023-03-08 14:27:20.795695396 +0100 CET container init d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi...) 2023-03-08 14:27:20.809205161 +0100 CET container start d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi...) 2023-03-08 14:27:20.809903022 +0100 CET container attach d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi...) 2023-03-08 14:27:20.831710446 +0100 CET container died d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi...) 2023-03-08 14:27:20.913786892 +0100 CET container remove d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi...) The --stream=false option ensures that the podman events command exits when reading the last known event. You can see several events that happened when you enter the podman run command: container create when creating a new container. image pull when pulling an image if the container image is not present in the local storage. container init when initializing the container in the runtime and setting a network. container start when starting the container. container attach when attaching to the terminal of a container. That is because the container runs in the foreground. container died is emitted when the container exits. container remove because the --rm flag was used to remove the container after it exits. You can also use the journalctl command to display Podman events: $ journalctl --user -r SYSLOG_IDENTIFIER=podman Mar 08 14:27:20 fedora podman[129324]: 2023-03-08 14:27:20.913786892 +0100 CET m=+0.066920979 container remove ... Mar 08 14:27:20 fedora podman[129289]: 2023-03-08 14:27:20.696167362 +0100 CET m=+0.079089208 container create d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72f> To show only Podman create events, enter: $ podman events --filter event=create 2023-03-08 14:27:20.696167362 +0100 CET container create d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72fe09 (image=registry.access.redhat.com/ubi8/ubi:latest, name=myubi,...) You can also use the journalctl command to display Podman create events: $ journalctl --user -r PODMAN_EVENT=create Mar 08 14:27:20 fedora podman[129289]: 2023-03-08 14:27:20.696167362 +0100 CET m=+0.079089208 container create d4748226a2bcd271b1bc4b9f88b54e8271c13ffea9b30529968291c62d72f> Additional resources podman-events man page on your system Container Events and Auditing 21.7. Using Podman events for auditing Previously, the events had to be connected to an event to interpret them correctly. For example, the container-create event had to be linked with an image-pull event to know which image had been used. The container-create event also did not include all data, for example, the security settings, volumes, mounts, and so on. Beginning with Podman v4.4, you can gather all relevant information about a container directly from a single event and journald entry. The data is in JSON format, the same as from the podman container inspect command and includes all configuration and security settings of a container. You can configure Podman to attach the container-inspect data for auditing purposes. Prerequisites The container-tools meta-package is installed. Procedure Modify the ~/.config/containers/containers.conf file and add the events_container_create_inspect_data=true option to the [engine] section: $ cat ~/.config/containers/containers.conf [engine] events_container_create_inspect_data=true For the system-wide configuration, modify the /etc/containers/containers.conf or /usr/share/container/containers.conf file. Create the container: $ podman create registry.access.redhat.com/ubi8/ubi:latest 19524fe3c145df32d4f0c9af83e7964e4fb79fc4c397c514192d9d7620a36cd3 Display the Podman events: Using the podman events command: $ now=$(date --iso-8601=seconds) $ podman events --since $now --stream=false --format \"{{.ContainerInspectData}}\" | jq “.Config.CreateCommand\" [ \"/usr/bin/podman\", \"create\", \"registry.access.redhat.com/ubi8\" ] The --format \"{{.ContainerInspectData}}\" option displays the inspect data. The jq \".Config.CreateCommand\" transforms the JSON data into a more readable format and displays the parameters for the podman create command. Using the journalctl command: $ journalctl --user -r PODMAN_EVENT=create --all -o json | jq \".PODMAN_CONTAINER_INSPECT_DATA | fromjson\" | jq \".Config.CreateCommand\" [ \"/usr/bin/podman\", \"create\", \"registry.access.redhat.com/ubi8\" ] The output data for the podman events and journalctl commands are the same. Additional resources podman-events and containers.conf man pages on your system Container Events and Auditing Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_monitoring-containers"}
{"title": "Chapter 20. Working with containers using Buildah", "content": "Chapter 20. Working with containers using Buildah With Buildah, you can do several operations on a container image or container from the command line. Examples of operations are: create a working container from scratch or from a container image as a starting point, create an image from a working container or using a Containerfile , configure a container’s entrypoint, labels, port, shell, and working directory. You can mount working containers directories for filesystem manipulation, delete a working container or container image, and more. You can then create an image from a working container and push the image to the registry. 20.1. Running commands inside of the container Use the buildah run command to execute a command from the container. Prerequisites The container-tools meta-package is installed. A pulled image is available on the local system. Procedure Display the operating system version: # buildah run ubi-working-container cat /etc/redhat-release Red Hat Enterprise Linux release 8.4 (Ootpa) Additional resources buildah-run man page on your system 20.2. Inspecting containers and images with Buildah Use the buildah inspect command to display information about a container or image. Prerequisites The container-tools meta-package is installed. An image was built using instructions from Containerfile. For details, see section Building an image from a Containerfile with Buildah . Procedure Inspect the image: To inspect the myecho image, enter: # buildah inspect localhost/myecho { \"Type\": \"buildah 0.0.1\", \"FromImage\": \"localhost/myecho:latest\", \"FromImageID\": \"b28cd00741b38c92382ee806e1653eae0a56402bcd2c8d31bdcd36521bc267a4\", \"FromImageDigest\": \"sha256:0f5b06cbd51b464fabe93ce4fe852a9038cdd7c7b7661cd7efef8f9ae8a59585\", \"Config\": ... \"Entrypoint\": [ \"/bin/sh\", \"-c\", \"\\\"/usr/local/bin/myecho\\\"\" ], ... } To inspect the working container from the myecho image: Create a working container based on the localhost/myecho image: # buildah from localhost/myecho Inspect the myecho-working-container container: # buildah inspect ubi-working-container { \"Type\": \"buildah 0.0.1\", \"FromImage\": \"registry.access.redhat.com/ubi8/ubi:latest\", \"FromImageID\": \"272209ff0ae5fe54c119b9c32a25887e13625c9035a1599feba654aa7638262d\", \"FromImageDigest\": \"sha256:77623387101abefbf83161c7d5a0378379d0424b2244009282acb39d42f1fe13\", \"Config\": ... \"Container\": \"ubi-working-container\", \"ContainerID\": \"01eab9588ae1523746bb706479063ba103f6281ebaeeccb5dc42b70e450d5ad0\", \"ProcessLabel\": \"system_u:system_r:container_t:s0:c162,c1000\", \"MountLabel\": \"system_u:object_r:container_file_t:s0:c162,c1000\", ... } Additional resources buildah-inspect man page on your system 20.3. Modifying a container using buildah mount Use the buildah mount command to display information about a container or image. Prerequisites The container-tools meta-package is installed. An image built using instructions from Containerfile. For details, see section Building an image from a Containerfile with Buildah . Procedure Create a working container based on the registry.access.redhat.com/ubi8/ubi image and save the name of the container to the mycontainer variable: # mycontainer=$(buildah from localhost/myecho) # echo $mycontainer myecho-working-container Mount the myecho-working-container container and save the mount point path to the mymount variable: # mymount=$(buildah mount $mycontainer) # echo $mymount /var/lib/containers/storage/overlay/c1709df40031dda7c49e93575d9c8eebcaa5d8129033a58e5b6a95019684cc25/merged Modify the myecho script and make it executable: # echo 'echo \"We modified this container.\"' >> $mymount/usr/local/bin/myecho # chmod +x $mymount/usr/local/bin/myecho Create the myecho2 image from the myecho-working-container container: # buildah commit $mycontainer containers-storage:myecho2 Verification List all images in local storage: # buildah images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/myecho2 latest 4547d2c3e436 4 minutes ago 234 MB localhost/myecho latest b28cd00741b3 56 minutes ago 234 MB Run the myecho2 container based on the docker.io/library/myecho2 image: # podman run --name=myecho2 docker.io/library/myecho2 This container works! We even modified it. Additional resources buildah-mount and buildah-commit man pages on your system 20.4. Modifying a container using buildah copy and buildah config Use buildah copy command to copy files to a container without mounting it. You can then configure the container using the buildah config command to run the script you created by default. Prerequisites The container-tools meta-package is installed. An image built using instructions from Containerfile. For details, see section Building an image from a Containerfile with Buildah . Procedure Create a script named newecho and make it executable: # cat newecho echo \"I changed this container\" # chmod 755 newecho Create a new working container: # buildah from myecho:latest myecho-working-container-2 Copy the newecho script to /usr/local/bin directory inside the container: # buildah copy myecho-working-container-2 newecho /usr/local/bin Change the configuration to use the newecho script as the new entrypoint: # buildah config --entrypoint \"/bin/sh -c /usr/local/bin/newecho\" myecho-working-container-2 Optional: Run the myecho-working-container-2 container whixh triggers the newecho script to be executed: # buildah run myecho-working-container-2 -- sh -c '/usr/local/bin/newecho' I changed this container Commit the myecho-working-container-2 container to a new image called mynewecho : # buildah commit myecho-working-container-2 containers-storage:mynewecho Verification List all images in local storage: # buildah images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/mynewecho latest fa2091a7d8b6 8 seconds ago 234 MB Additional resources buildah-copy , buildah-config , buildah-commit , buildah-run man pages on your system 20.5. Pushing containers to a private registry Use buildah push command to push an image from local storage to a public or private repository. Prerequisites The container-tools meta-package is installed. An image was built using instructions from Containerfile. For details, see section Building an image from a Containerfile with Buildah . Procedure Create the local registry on your machine: # podman run -d -p 5000:5000 registry:2 Push the myecho:latest image to the localhost registry: # buildah push --tls-verify=false myecho:latest localhost:5000/myecho:latest Getting image source signatures Copying blob sha256:e4efd0... ... Writing manifest to image destination Storing signatures Verification List all images in the localhost repository: # curl http://localhost:5000/v2/_catalog {\"repositories\":[\"myecho2]} # curl http://localhost:5000/v2/myecho2/tags/list {\"name\":\"myecho\",\"tags\":[\"latest\"]} Inspect the docker://localhost:5000/myecho:latest image: # skopeo inspect --tls-verify=false docker://localhost:5000/myecho:latest | less { \"Name\": \"localhost:5000/myecho\", \"Digest\": \"sha256:8999ff6050...\", \"RepoTags\": [ \"latest\" ], \"Created\": \"2021-06-28T14:44:05.919583964Z\", \"DockerVersion\": \"\", \"Labels\": { \"architecture\": \"x86_64\", \"authoritative-source-url\": \"registry.redhat.io\", ... } Pull the localhost:5000/myecho image: # podman pull --tls-verify=false localhost:5000/myecho2 # podman run localhost:5000/myecho2 This container works! Additional resources buildah-push man page on your system 20.6. Pushing containers to the Docker Hub Use your Docker Hub credentials to push and pull images from the Docker Hub with the buildah command. Prerequisites The container-tools meta-package is installed. An image built using instructions from Containerfile. For details, see section Building an image from a Containerfile with Buildah . Procedure Push the docker.io/library/myecho:latest to your Docker Hub. Replace username and password with your Docker Hub credentials: # buildah push --creds username:password \\ docker.io/library/myecho:latest docker://testaccountXX/myecho:latest Verification Get and run the docker.io/testaccountXX/myecho:latest image: Using Podman tool: # podman run docker.io/testaccountXX/myecho:latest This container works! Using Buildah and Podman tools: # buildah from docker.io/testaccountXX/myecho:latest myecho2-working-container-2 # podman run myecho-working-container-2 Additional resources buildah-push man page on your system 20.7. Removing containers with Buildah Use the buildah rm command to remove containers. You can specify containers for removal with the container ID or name. Prerequisites The container-tools meta-package is installed. At least one container has been stopped. Procedure List all containers: # buildah containers CONTAINER ID BUILDER IMAGE ID IMAGE NAME CONTAINER NAME 05387e29ab93 * c37e14066ac7 docker.io/library/myecho:latest myecho-working-container Remove the myecho-working-container container: # buildah rm myecho-working-container 05387e29ab93151cf52e9c85c573f3e8ab64af1592b1ff9315db8a10a77d7c22 Verification Ensure that containers were removed: # buildah containers Additional resources buildah-rm man page on your system Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/working-with-containers-using-buildah"}
{"title": "Observability overview", "content": "Observability overview  Contains information about CI/CD for OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/observability_overview/index"}
{"title": "Chapter 4. Operator topologies", "content": "Chapter 4. Operator topologies The Ansible Automation Platform Operator uses Red Hat OpenShift Operators to deploy Ansible Automation Platform within Red Hat OpenShift. Customers manage the product and infrastructure lifecycle. Important You can only install a single instance of the Ansible Automation Platform Operator into a single namespace. Installing multiple instances in the same namespace can lead to improper operation for both Operator instances. 4.1. Operator growth topology The growth topology is intended for organizations that are getting started with Ansible Automation Platform and do not require redundancy or higher compute for large volumes of automation. This topology allows for smaller footprint deployments. 4.1.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 4.1. Infrastructure topology diagram A Single Node OpenShift (SNO) cluster has been tested with the following requirements: 32 GB RAM, 16 CPUs, 128 GB local disk, and 3000 IOPS. Table 4.1. Infrastructure topology Count Component 1 Automation controller web pod 1 Automation controller task pod 1 Automation hub API pod 2 Automation hub content pod 2 Automation hub worker pod 1 Automation hub Redis pod 1 Event-Driven Ansible API pod 1 Event-Driven Ansible activation worker pod 1 Event-Driven Ansible default worker pod 1 Event-Driven Ansible event stream pod 1 Event-Driven Ansible scheduler pod 1 Platform gateway pod 1 Database pod 1 Redis pod Note You can deploy multiple isolated instances of Ansible Automation Platform into the same Red Hat OpenShift Container Platform cluster by using a namespace-scoped deployment model. This approach allows you to use the same cluster for several deployments. 4.1.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 4.2. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Operating system Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9 CPU architecture x86_64, AArch64, s390x (IBM Z), ppc64le (IBM Power) Red Hat OpenShift Version: 4.14 num_of_control_nodes: 1 num_of_worker_nodes: 1 Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome. Database PostgreSQL 15 4.1.3. Example custom resource file Use the following example custom resource (CR) to add your Ansible Automation Platform instance to your project: apiVersion: aap.ansible.com/v1alpha1 kind: AnsibleAutomationPlatform metadata: name: <aap instance name> spec: eda: automation_server_ssl_verify: 'no' hub: storage_type: 's3' object_storage_s3_secret: '<name of the Secret resource holding s3 configuration>' 4.1.4. Nonfunctional requirements Ansible Automation Platform’s performance characteristics and capacity are impacted by its resource allocation and configuration. With OpenShift, each Ansible Automation Platform component is deployed as a pod. You can specify resource requests and limits for each pod. Use the Ansible Automation Platform Custom Resource (CR) to configure resource allocation for OpenShift installations. Each configurable item has default settings. These settings are the minimum requirements for an installation, but might not meet your production workload needs. By default, each component’s deployments are set for minimum resource requests but no resource limits. OpenShift only schedules the pods with available resource requests, but the pods are allowed to consume unlimited RAM or CPU provided that the OpenShift worker node itself is not under node pressure. In the Operator growth topology, Ansible Automation Platform is deployed on a Single Node OpenShift (SNO) with 32 GB RAM, 16 CPUs, 128 GB Local disk, and 3000 IOPS. This is not a shared environment, so Ansible Automation Platform pods have full access to all of the compute resources of the OpenShift SNO. In this scenario, the capacity calculation for the automation controller task pods is derived from the underlying OpenShift Container Platform node that runs the pod. It does not have access to the entire node. This capacity calculation influences how many concurrent jobs automation controller can run. OpenShift manages storage distinctly from VMs. This impacts how automation hub stores its artifacts. In the Operator growth topology, we use S3 storage because automation hub requires a ReadWriteMany type storage, which is not a default storage type in OpenShift. 4.1.5. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 4.3. Network ports and protocols Port number Protocol Service Source Destination 80/443 HTTP/HTTPS Receptor Execution node OpenShift Container Platform ingress 80/443 HTTP/HTTPS Receptor Hop node OpenShift Container Platform ingress 80/443 HTTP/HTTPS Platform Customer clients OpenShift Container Platform ingress 27199 TCP Receptor OpenShift Container Platform cluster Execution node 27199 TCP Receptor OpenShift Container Platform cluster Hop node 4.2. Operator enterprise topology The enterprise topology is intended for organizations that require Ansible Automation Platform to be deployed with redundancy or higher compute for large volumes of automation. 4.2.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 4.2. Infrastructure topology diagram The following infrastructure topology describes an OpenShift Cluster with 3 primary nodes and 2 worker nodes. Each OpenShift Worker node has been tested with the following component requirements: 16 GB RAM, 4 CPUs, 128 GB local disk, and 3000 IOPS. Table 4.4. Infrastructure topology Count Component 1 Automation controller web pod 1 Automation controller task pod 1 Automation hub API pod 2 Automation hub content pod 2 Automation hub worker pod 1 Automation hub Redis pod 1 Event-Driven Ansible API pod 2 Event-Driven Ansible activation worker pod 2 Event-Driven Ansible default worker pod 2 Event-Driven Ansible event stream pod 1 Event-Driven Ansible scheduler pod 1 Platform gateway pod 2 Mesh ingress pod N/A Externally managed database service N/A Externally managed Redis N/A Externally managed object storage service (for automation hub) 4.2.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 4.5. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Operating system Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9 CPU architecture x86_64, AArch64, s390x (IBM Z), ppc64le (IBM Power) Red Hat OpenShift Red Hat OpenShift on AWS Hosted Control Planes 4.15.16 2 worker nodes in different availability zones (AZs) at t3.xlarge Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome. AWS RDS PostgreSQL service engine: \"postgres\" engine_version: 15\" parameter_group_name: \"default.postgres15\" allocated_storage: 20 max_allocated_storage: 1000 storage_type: \"gp2\" storage_encrypted: true instance_class: \"db.t4g.small\" multi_az: true backup_retention_period: 5 database: must have ICU support AWS Memcached Service engine: \"redis\" engine_version: \"6.2\" auto_minor_version_upgrade: \"false\" node_type: \"cache.t3.micro\" parameter_group_name: \"default.redis6.x.cluster.on\" transit_encryption_enabled: \"true\" num_node_groups: 2 replicas_per_node_group: 1 automatic_failover_enabled: true s3 storage HTTPS only accessible through AWS Role assigned to automation hub SA at runtime by using AWS Pod Identity 4.2.3. Nonfunctional requirements Ansible Automation Platform’s performance characteristics and capacity are impacted by its resource allocation and configuration. With OpenShift, each Ansible Automation Platform component is deployed as a pod. You can specify resource requests and limits for each pod. Use the Ansible Automation Platform custom resource to configure resource allocation for OpenShift installations. Each configurable item has default settings. These settings are the exact configuration used within the context of this reference deployment architecture and presumes that the environment is being deployed and managed by an Enterprise IT organization for production purposes. By default, each component’s deployments are set for minimum resource requests but no resource limits. OpenShift only schedules the pods with available resource requests, but the pods are allowed to consume unlimited RAM or CPU provided that the OpenShift worker node itself is not under node pressure. In the Operator enterprise topology, Ansible Automation Platform is deployed on a Red Hat OpenShift on AWS (ROSA) Hosted Control Plane (HCP) cluster with 2 t3.xlarge worker nodes spread across 2 AZs within a single AWS Region. This is not a shared environment, so Ansible Automation Platform pods have full access to all of the compute resources of the ROSA HCP cluster. In this scenario, the capacity calculation for the automation controller task pods is derived from the underlying HCP worker node that runs the pod. It does not have access to the CPU or memory resources of the entire node. This capacity calculation influences how many concurrent jobs automation controller can run. OpenShift manages storage distinctly from VMs. This impacts how automation hub stores its artifacts. In the Operator enterprise topology, we use S3 storage because automation hub requires a ReadWriteMany type storage, which is not a default storage type in OpenShift. Externally provided Redis, PostgreSQL, and object storage for automation hub are specified. This provides the Ansible Automation Platform deployment with additional scalability and reliability features, including specialized backup, restore, and replication services and scalable storage. 4.2.4. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 4.6. Network ports and protocols Port number Protocol Service Source Destination 80/443 HTTP/HTTPS Object storage OpenShift Container Platform cluster External object storage service 80/443 HTTP/HTTPS Receptor Execution node OpenShift Container Platform ingress 80/443 HTTP/HTTPS Receptor Hop node OpenShift Container Platform ingress 5432 TCP PostgreSQL OpenShift Container Platform cluster External database service 6379 TCP Redis OpenShift Container Platform cluster External Redis service 27199 TCP Receptor OpenShift Container Platform cluster Execution node 27199 TCP Receptor OpenShift Container Platform cluster Hop node Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/ocp-topologies"}
{"title": "Chapter 2. RPM topologies", "content": "Chapter 2. RPM topologies The RPM installer deploys Ansible Automation Platform on Red Hat Enterprise Linux by using RPMs to install the platform on host machines. Customers manage the product and infrastructure lifecycle. 2.1. RPM growth topology The growth topology is intended for organizations that are getting started with Ansible Automation Platform and do not require redundancy or higher compute for large volumes of automation. This topology allows for smaller footprint deployments. 2.1.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 2.1. Infrastructure topology diagram Each virtual machine (VM) has been tested with the following component requirements: 16 GB RAM, 4 CPUs, 60 GB local disk, and 3000 IOPS. Table 2.1. Infrastructure topology VM count Purpose Example VM group names 1 Platform gateway with colocated Redis automationgateway 1 Automation controller automationcontroller 1 Private automation hub automationhub 1 Event-Driven Ansible automationedacontroller 1 Automation mesh execution node execution_nodes 1 Database database 2.1.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 2.2. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Operating system Red Hat Enterprise Linux 8.8 or later minor versions of Red Hat Enterprise Linux 8. Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9. CPU architecture x86_64, AArch64, s390x (IBM Z), ppc64le (IBM Power) Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome Database PostgreSQL 15 2.1.3. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 2.3. Network ports and protocols Port number Protocol Service Source Destination 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation hub 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation controller 80/443 TCP HTTP/HTTPS Automation controller Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Automation controller 80/443 TCP HTTP/HTTPS Platform gateway Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible 5432 TCP PostgreSQL Event-Driven Ansible Database 5432 TCP PostgreSQL Platform gateway Database 5432 TCP PostgreSQL Automation hub Database 5432 TCP PostgreSQL Automation controller Database 6379 TCP Redis Event-Driven Ansible Redis node 6379 TCP Redis Platform gateway Redis node 8443 TCP HTTPS Platform gateway Platform gateway 27199 TCP Receptor Automation controller Execution node 2.1.4. Example inventory file Use the example inventory file to perform an installation for this topology: # This is the Ansible Automation Platform installer inventory file intended for the RPM growth deployment topology. # Consult the Ansible Automation Platform product documentation about this topology's tested hardware configuration. # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/rpm-topologies # # Consult the docs if you are unsure what to add # For all optional variables consult the Ansible Automation Platform documentation: # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation # This section is for your platform gateway hosts # ----------------------------------------------------- [automationgateway] gateway.example.org # This section is for your automation controller hosts # ----------------------------------------------------- [automationcontroller] controller.example.org [automationcontroller:vars] peers=execution_nodes # This section is for your Ansible Automation Platform execution hosts # ----------------------------------------------------- [execution_nodes] exec.example.org # This section is for your automation hub hosts # ----------------------------------------------------- [automationhub] hub.example.org # This section is for your Event-Driven Ansible controller hosts # ----------------------------------------------------- [automationedacontroller] eda.example.org # This section is for the Ansible Automation Platform database # ----------------------------------------------------- [database] db.example.org [all:vars] # Common variables # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-general-inventory-variables # ----------------------------------------------------- registry_username=<your RHN username> registry_password=<your RHN password> redis_mode=standalone # Platform gateway # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-gateway-variables # ----------------------------------------------------- automationgateway_admin_password=<set your own> automationgateway_pg_host=db.example.org automationgateway_pg_password=<set your own> # Automation controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-controller-variables # ----------------------------------------------------- admin_password=<set your own> pg_host=db.example.org pg_password=<set your own> # Automation hub # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-hub-variables # ----------------------------------------------------- automationhub_admin_password=<set your own> automationhub_pg_host=db.example.org automationhub_pg_password=<set your own> # Event-Driven Ansible controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#event-driven-ansible-controller # ----------------------------------------------------- automationedacontroller_admin_password=<set your own> automationedacontroller_pg_host=db.example.org automationedacontroller_pg_password=<set your own> 2.2. RPM mixed growth topology The growth topology is intended for organizations that are getting started with Ansible Automation Platform and do not require redundancy or higher compute for large volumes of automation. This topology allows for smaller footprint deployments. The mixed topology has different versions of Ansible Automation Platform intended for configuring a new installation of Event-Driven Ansible 1.1 with automation controller 4.4 or 4.5. 2.2.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 2.2. Infrastructure topology diagram Note Here, automation controller and automation hub are at 2.4x while the Event-Driven Ansible and platform gateway components are at 2.5 Each virtual machine (VM) has been tested with the following component requirements: 16 GB RAM, 4 CPUs, 60 GB local disk, and 3000 IOPS. Table 2.4. Infrastructure topology VM count Purpose Ansible Automation Platform version Example VM group names 1 Platform gateway with colocated Redis 2.5 automationgateway 1 Automation controller 2.4 automationcontroller 1 Private automation hub 2.4 automationhub 1 Event-Driven Ansible 2.5 automationedacontroller 1 Automation mesh execution node 2.4 execution_nodes 1 Database 2.4 database 2.2.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 2.5. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Operating system Red Hat Enterprise Linux 8.8 or later minor versions of Red Hat Enterprise Linux 8. Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9. CPU architecture x86_64, AArch64 Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome Database PostgreSQL 15 2.2.3. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 2.6. Network ports and protocols Port number Protocol Service Source Destination 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation hub 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation controller 80/443 TCP HTTP/HTTPS Automation controller Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Automation controller 80/443 TCP HTTP/HTTPS Platform gateway Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible 5432 TCP PostgreSQL Event-Driven Ansible Database 5432 TCP PostgreSQL Platform gateway Database 5432 TCP PostgreSQL Automation hub Database 5432 TCP PostgreSQL Automation controller Database 6379 TCP Redis Event-Driven Ansible Redis node 6379 TCP Redis Platform gateway Redis node 8443 TCP HTTPS Platform gateway Platform gateway 27199 TCP Receptor Automation controller Execution node 2.2.4. Example inventory file Use the example inventory file to perform an installation for this topology: # This is the Ansible Automation Platform installer inventory file intended for the mixed RPM growth deployment topology. # Consult the Ansible Automation Platform product documentation about this topology's tested hardware configuration. # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/rpm-topologies # # Consult the docs if you are unsure what to add # For all optional variables consult the Red Hat documentation: # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation # This section is for your platform gateway hosts # ----------------------------------------------------- [automationgateway] gateway.example.org # This section is for your Event-Driven Ansible controller hosts # ----------------------------------------------------- [automationedacontroller] eda.example.org [all:vars] # Common variables # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-general-inventory-variables # ----------------------------------------------------- registry_username=<your RHN username> registry_password=<your RHN password> redis_mode=standalone # Platform gateway # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-gateway-variables # ----------------------------------------------------- automationgateway_admin_password=<set your own> automationgateway_pg_host=db.example.org automationgateway_pg_password=<set your own> # Event-Driven Ansible controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#event-driven-ansible-controller # ----------------------------------------------------- automationedacontroller_admin_password=<set your own> automationedacontroller_pg_host=db.example.org automationedacontroller_pg_password=<set your own> 2.3. RPM enterprise topology The enterprise topology is intended for organizations that require Ansible Automation Platform to be deployed with redundancy or higher compute for large volumes of automation. 2.3.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 2.3. Infrastructure topology diagram Each virtual machine (VM) has been tested with the following component requirements: 16 GB RAM, 4 CPUs, 60 GB local disk, and 3000 IOPS. Table 2.7. Infrastructure topology VM count Purpose Example VM group names 2 Platform gateway with colocated Redis automationgateway 2 Automation controller automationcontroller 2 Private automation hub with colocated Redis automationhub 2 Event-Driven Ansible with colocated Redis automationedacontroller 1 Automation mesh hop node execution_nodes 2 Automation mesh execution node execution_nodes 1 Externally managed database service N/A 1 HAProxy load balancer in front of platform gateway (externally managed) N/A Note 6 VMs are required for a Redis high availability (HA) compatible deployment. Redis can be colocated on each Ansible Automation Platform component VM except for automation controller, execution nodes, or the PostgreSQL database. 2.3.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 2.8. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Operating system Red Hat Enterprise Linux 8.8 or later minor versions of Red Hat Enterprise Linux 8. Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9. CPU architecture x86_64, AArch64, s390x (IBM Z), ppc64le (IBM Power) Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome Database PostgreSQL 15 2.3.3. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 2.9. Network ports and protocols Port number Protocol Service Source Destination 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation hub 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation controller 80/443 TCP HTTP/HTTPS Automation controller Automation hub 80/443 TCP HTTP/HTTPS HAProxy load balancer Platform gateway 80/443 TCP HTTP/HTTPS Platform gateway Automation controller 80/443 TCP HTTP/HTTPS Platform gateway Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible 5432 TCP PostgreSQL Event-Driven Ansible External database 5432 TCP PostgreSQL Platform gateway External database 5432 TCP PostgreSQL Automation hub External database 5432 TCP PostgreSQL Automation controller External database 6379 TCP Redis Event-Driven Ansible Redis node 6379 TCP Redis Platform gateway Redis node 8443 TCP HTTPS Platform gateway Platform gateway 16379 TCP Redis Redis node Redis node 27199 TCP Receptor Automation controller Hop node and execution node 27199 TCP Receptor Hop node Execution node 2.3.4. Example inventory file Use the example inventory file to perform an installation for this topology: # This is the Ansible Automation Platform enterprise installer inventory file # Consult the docs if you are unsure what to add # For all optional variables consult the Red Hat documentation: # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation # This section is for your platform gateway hosts # ----------------------------------------------------- [automationgateway] gateway1.example.org gateway2.example.org # This section is for your automation controller hosts # ----------------------------------------------------- [automationcontroller] controller1.example.org controller2.example.org [automationcontroller:vars] peers=execution_nodes # This section is for your Ansible Automation Platform execution hosts # ----------------------------------------------------- [execution_nodes] hop1.example.org node_type='hop' exec1.example.org exec2.example.org # This section is for your automation hub hosts # ----------------------------------------------------- [automationhub] hub1.example.org hub2.example.org # This section is for your Event-Driven Ansible controller hosts # ----------------------------------------------------- [automationedacontroller] eda1.example.org eda2.example.org [redis] gateway1.example.org gateway2.example.org hub1.example.org hub2.example.org eda1.example.org eda2.example.org [all:vars] # Common variables # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-general-inventory-variables # ----------------------------------------------------- registry_username=<your RHN username> registry_password=<your RHN password> # Platform gateway # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-gateway-variables # ----------------------------------------------------- automationgateway_admin_password=<set your own> automationgateway_pg_host=<set your own> automationgateway_pg_database=<set your own> automationgateway_pg_username=<set your own> automationgateway_pg_password=<set your own> # Automation controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-controller-variables # ----------------------------------------------------- admin_password=<set your own> pg_host=<set your own> pg_database=<set your own> pg_username=<set your own> pg_password=<set your own> # Automation hub # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-hub-variables # ----------------------------------------------------- automationhub_admin_password=<set your own> automationhub_pg_host=<set your own> automationhub_pg_database=<set your own> automationhub_pg_username=<set your own> automationhub_pg_password=<set your own> # Event-Driven Ansible controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#event-driven-ansible-controller # ----------------------------------------------------- automationedacontroller_admin_password=<set your own> automationedacontroller_pg_host=<set your own> automationedacontroller_pg_database=<set your own> automationedacontroller_pg_username=<set your own> automationedacontroller_pg_password=<set your own> 2.4. RPM mixed enterprise topology The enterprise topology is intended for organizations that require Ansible Automation Platform to be deployed with redundancy or higher compute for large volumes of automation. 2.4.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 2.4. Infrastructure topology diagram Note Here, automation controller and automation hub are at 2.4x while the Event-Driven Ansible and platform gateway components are at 2.5 Each VM has been tested with the following component requirements: 16 GB RAM, 4 CPUs, 60 GB local disk, and 3000 IOPS. Table 2.10. Infrastructure topology VM count Purpose Ansible Automation Platform version Example VM group names 3 Platform gateway with colocated Redis 2.5 automationgateway 2 Automation controller 2.4 automationcontroller 2 Private automation hub 2.4 automationhub 3 Event-Driven Ansible with colocated Redis 2.5 automationedacontroller 1 Automation mesh hop node 2.4 execution_nodes 2 Automation mesh execution node 2.4 execution_nodes 1 Externally managed database service N/A N/A 1 HAProxy load balancer in front of platform gateway (externally managed) N/A N/A Note 6 VMs are required for a Redis high availability (HA) compatible deployment. Redis can be colocated on each Ansible Automation Platform 2.5 component VM except for automation controller, execution nodes, or the PostgreSQL database. 2.4.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 2.11. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Operating system Red Hat Enterprise Linux 8.8 or later minor versions of Red Hat Enterprise Linux 8. Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9. CPU architecture x86_64, AArch64 Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome Database PostgreSQL 15 2.4.3. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 2.12. Network ports and protocols Port number Protocol Service Source Destination 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation hub 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation controller 80/443 TCP HTTP/HTTPS Automation controller Automation hub 80/443 TCP HTTP/HTTPS HAProxy load balancer Platform gateway 80/443 TCP HTTP/HTTPS Platform gateway Automation controller 80/443 TCP HTTP/HTTPS Platform gateway Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible 5432 TCP PostgreSQL Event-Driven Ansible External database 5432 TCP PostgreSQL Platform gateway External database 5432 TCP PostgreSQL Automation hub External database 5432 TCP PostgreSQL Automation controller External database 6379 TCP Redis Event-Driven Ansible Redis node 6379 TCP Redis Platform gateway Redis node 8443 TCP HTTPS Platform gateway Platform gateway 16379 TCP Redis Redis node Redis node 27199 TCP Receptor Automation controller Hop node and execution node 27199 TCP Receptor Hop node Execution node 2.4.4. Example inventory file Use the example inventory file to perform an installation for this topology: # This is the Ansible Automation Platform mixed enterprise installer inventory file # Consult the docs if you are unsure what to add # For all optional variables consult the Red Hat documentation: # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation # This section is for your platform gateway hosts # ----------------------------------------------------- [automationgateway] gateway1.example.org gateway2.example.org gateway3.example.org # This section is for your Event-Driven Ansible controller hosts # ----------------------------------------------------- [automationedacontroller] eda1.example.org eda2.example.org eda3.example.org [redis] gateway1.example.org gateway2.example.org gateway3.example.org eda1.example.org eda2.example.org eda3.example.org [all:vars] # Common variables # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-general-inventory-variables # ----------------------------------------------------- registry_username=<your RHN username> registry_password=<your RHN password> # Platform gateway # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#ref-gateway-variables # ----------------------------------------------------- automationgateway_admin_password=<set your own> automationgateway_pg_host=<set your own> automationgateway_pg_database=<set your own> automationgateway_pg_username=<set your own> automationgateway_pg_password=<set your own> # Event-Driven Ansible controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars#event-driven-ansible-controller # ----------------------------------------------------- automationedacontroller_admin_password=<set your own> automationedacontroller_pg_host=<set your own> automationedacontroller_pg_database=<set your own> automationedacontroller_pg_username=<set your own> automationedacontroller_pg_password=<set your own> Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/rpm-topologies"}
{"title": "Chapter 3. Container topologies", "content": "Chapter 3. Container topologies The containerized installer deploys Ansible Automation Platform on Red Hat Enterprise Linux by using Podman which runs the platform in containers on host machines. Customers manage the product and infrastructure lifecycle. 3.1. Container growth topology The growth topology is intended for organizations that are getting started with Ansible Automation Platform and do not require redundancy or higher compute for large volumes of automation. This topology allows for smaller footprint deployments. 3.1.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 3.1. Infrastructure topology diagram A single VM has been tested with the following component requirements: Table 3.1. Virtual machine requirements Requirement Minimum requirement RAM 16 GB CPUs 4 Local disk 60 GB Disk IOPS 3000 Resources, such as storage, can be increased based on the needs of the deployment. Note If performing a bundled installation of the growth topology with hub_seed_collections=true , then 32 GB RAM is recommended. Note that with this configuration the install time is going to increase and can take 45 or more minutes alone to complete seeding the collections. Table 3.2. Infrastructure topology Purpose Example group names All Ansible Automation Platform components automationgateway automationcontroller automationhub automationeda database 3.1.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 3.3. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Valid Red Hat Enterprise Linux subscription (to consume the BaseOS and AppStream repositories) Operating system Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9 CPU architecture x86_64, AArch64, s390x (IBM Z), ppc64le (IBM Power) Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome. Database PostgreSQL 15 3.1.3. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 3.4. Network ports and protocols Port number Protocol Service Source Destination 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation hub 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation controller 80/443 TCP HTTP/HTTPS Automation controller Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Automation controller 80/443 TCP HTTP/HTTPS Platform gateway Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible 5432 TCP PostgreSQL Event-Driven Ansible External database 5432 TCP PostgreSQL Platform gateway External database 5432 TCP PostgreSQL Automation hub External database 5432 TCP PostgreSQL Automation controller External database 6379 TCP Redis Event-Driven Ansible Redis container 6379 TCP Redis Platform gateway Redis container 8443 TCP HTTPS Platform gateway Platform gateway 27199 TCP Receptor Automation controller Execution container 3.1.4. Example inventory file Use the example inventory file to perform an installation for this topology: # This is the Ansible Automation Platform installer inventory file intended for the container growth deployment topology. # This inventory file expects to be run from the host where Ansible Automation Platform will be installed. # Consult the Ansible Automation Platform product documentation about this topology's tested hardware configuration. # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/container-topologies # # Consult the docs if you are unsure what to add # For all optional variables consult the included README.md # or the Ansible Automation Platform documentation: # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation # This section is for your platform gateway hosts # ----------------------------------------------------- [automationgateway] aap.example.org # This section is for your automation controller hosts # ------------------------------------------------- [automationcontroller] aap.example.org # This section is for your automation hub hosts # ----------------------------------------------------- [automationhub] aap.example.org # This section is for your Event-Driven Ansible controller hosts # ----------------------------------------------------- [automationeda] aap.example.org # This section is for the Ansible Automation Platform database # -------------------------------------- [database] aap.example.org [all:vars] # Ansible ansible_connection=local # Common variables # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-general-inventory-variables # ----------------------------------------------------- postgresql_admin_username=postgres postgresql_admin_password=<set your own> registry_username=<your RHN username> registry_password=<your RHN password> redis_mode=standalone # Platform gateway # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-gateway-variables # ----------------------------------------------------- gateway_admin_password=<set your own> gateway_pg_host=aap.example.org gateway_pg_password=<set your own> # Automation controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-controller-variables # ----------------------------------------------------- controller_admin_password=<set your own> controller_pg_host=aap.example.org controller_pg_password=<set your own> # Automation hub # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-hub-variables # ----------------------------------------------------- hub_admin_password=<set your own> hub_pg_host=aap.example.org hub_pg_password=<set your own> hub_seed_collections=false # Event-Driven Ansible controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#event-driven-ansible-controller # ----------------------------------------------------- eda_admin_password=<set your own> eda_pg_host=aap.example.org eda_pg_password=<set your own> SSH keys are only required when installing on remote hosts. If doing a self contained local VM based installation, you can use ansible_connection=local . 3.2. Container enterprise topology The enterprise topology is intended for organizations that require Ansible Automation Platform to be deployed with redundancy or higher compute for large volumes of automation. 3.2.1. Infrastructure topology The following diagram outlines the infrastructure topology that Red Hat has tested with this deployment model that customers can use when self-managing Ansible Automation Platform: Figure 3.2. Infrastructure topology diagram Each VM has been tested with the following component requirements: Table 3.5. Virtual machine requirements Requirement Minimum requirement RAM 16 GB CPUs 4 Local disk 60 GB Disk IOPS 3000 Table 3.6. Infrastructure topology VM count Purpose Example VM group names 2 Platform gateway with colocated Redis automationgateway 2 Automation controller automationcontroller 2 Private automation hub with colocated Redis automationhub 2 Event-Driven Ansible with colocated Redis automationeda 1 Automation mesh hop node execution_nodes 2 Automation mesh execution node execution_nodes 1 Externally managed database service N/A 1 HAProxy load balancer in front of platform gateway (externally managed) N/A Note 6 VMs are required for a Redis high availability (HA) compatible deployment. When installing Ansible Automation Platform with the containerized installer, Redis can be colocated on any Ansible Automation Platform component VMs of your choice except for execution nodes or the PostgreSQL database. They might also be assigned VMs specifically for Redis use. 3.2.2. Tested system configurations Red Hat has tested the following configurations to install and run Red Hat Ansible Automation Platform: Table 3.7. Tested system configurations Type Description Subscription Valid Red Hat Ansible Automation Platform subscription Valid Red Hat Enterprise Linux subscription (to consume the BaseOS and AppStream repositories) Operating system Red Hat Enterprise Linux 9.2 or later minor versions of Red Hat Enterprise Linux 9 CPU architecture x86_64, AArch64, s390x (IBM Z), ppc64le (IBM Power) Ansible-core Ansible-core version 2.16 or later Browser A currently supported version of Mozilla Firefox or Google Chrome. Database PostgreSQL 15 3.2.3. Network ports Red Hat Ansible Automation Platform uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 3.8. Network ports and protocols Port number Protocol Service Source Destination 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation hub 80/443 TCP HTTP/HTTPS Event-Driven Ansible Automation controller 80/443 TCP HTTP/HTTPS Automation controller Automation hub 80/443 TCP HTTP/HTTPS HAProxy load balancer Platform gateway 80/443 TCP HTTP/HTTPS Platform gateway Automation controller 80/443 TCP HTTP/HTTPS Platform gateway Automation hub 80/443 TCP HTTP/HTTPS Platform gateway Event-Driven Ansible 5432 TCP PostgreSQL Event-Driven Ansible External database 5432 TCP PostgreSQL Platform gateway External database 5432 TCP PostgreSQL Automation hub External database 5432 TCP PostgreSQL Automation controller External database 6379 TCP Redis Event-Driven Ansible Redis node 6379 TCP Redis Platform gateway Redis node 8443 TCP HTTPS Platform gateway Platform gateway 16379 TCP Redis Redis node Redis node 27199 TCP Receptor Automation controller Hop node and execution node 27199 TCP Receptor Hop node Execution node 3.2.4. Example inventory file Use the example inventory file to perform an installation for this topology: # This is the Ansible Automation Platform enterprise installer inventory file # Consult the docs if you are unsure what to add # For all optional variables consult the included README.md # or the Red Hat documentation: # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation # This section is for your platform gateway hosts # ----------------------------------------------------- [automationgateway] gateway1.example.org gateway2.example.org # This section is for your automation controller hosts # ----------------------------------------------------- [automationcontroller] controller1.example.org controller2.example.org # This section is for your Ansible Automation Platform execution hosts # ----------------------------------------------------- [execution_nodes] hop1.example.org receptor_type='hop' exec1.example.org exec2.example.org # This section is for your automation hub hosts # ----------------------------------------------------- [automationhub] hub1.example.org hub2.example.org # This section is for your Event-Driven Ansible controller hosts # ----------------------------------------------------- [automationeda] eda1.example.org eda2.example.org [redis] gateway1.example.org gateway2.example.org hub1.example.org hub2.example.org eda1.example.org eda2.example.org [all:vars] # Common variables # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-general-inventory-variables # ----------------------------------------------------- postgresql_admin_username=<set your own> postgresql_admin_password=<set your own> registry_username=<your RHN username> registry_password=<your RHN password> # Platform gateway # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-gateway-variables # ----------------------------------------------------- gateway_admin_password=<set your own> gateway_pg_host=externaldb.example.org gateway_pg_database=<set your own> gateway_pg_username=<set your own> gateway_pg_password=<set your own> # Automation controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-controller-variables # ----------------------------------------------------- controller_admin_password=<set your own> controller_pg_host=externaldb.example.org controller_pg_database=<set your own> controller_pg_username=<set your own> controller_pg_password=<set your own> # Automation hub # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#ref-hub-variables # ----------------------------------------------------- hub_admin_password=<set your own> hub_pg_host=externaldb.example.org hub_pg_database=<set your own> hub_pg_username=<set your own> hub_pg_password=<set your own> # Event-Driven Ansible controller # https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/containerized_installation/appendix-inventory-files-vars#event-driven-ansible-controller # ----------------------------------------------------- eda_admin_password=<set your own> eda_pg_host=externaldb.example.org eda_pg_database=<set your own> eda_pg_username=<set your own> eda_pg_password=<set your own> 3.2.5. Storage requirements Execution environments are pulled into automation controller hybrid nodes and execution nodes that run jobs. The size of these containers influences the storage requirements for $PATH_WHERE_PODMAN_PUTS_CONTAINER_IMAGES . The primary determining factors for the size of the database and its storage volume, which defaults to $POSTGRES_DEFAULT_DATA_DIR , are: The quantity of job events (lines of output from automation controller jobs) The quantity of days of job data that are retained On execution nodes and automation controller control and hybrid nodes, job output is buffered to the disk in $NAME_OF_RECEPTOR_DIR_VAR , which defaults to /tmp . The size and quantity of collections synced to automation hub influence the storage requirements of $PATH_WHERE_PULP_STORES_COLLECTIONS . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/container-topologies"}
{"title": "Chapter 19. Building container images with Buildah", "content": "Chapter 19. Building container images with Buildah Buildah facilitates building OCI container images that meet the OCI Runtime Specification . With Buildah, you can create a working container, either from scratch or using an image as a starting point. You can create an image either from a working container, using the instructions in a Containerfile , or by using a series of Buildah commands that emulate the commands found in a Containerfile . 19.1. The Buildah tool Buildah is a command-line tool for creating Open Container Initiative (OCI) container images and a working container from the image. With Buildah, you can create containers and container images in different ways: Container image from scratch You can create minimal container images from scratch with the buildah from scratch command. Minimal container images have the following benefits: Avoid including any unnecessary files or dependencies, enhanced security, and optimized performance. For more information, see Creating images from scratch with Buildah . Containers from a container image You can create a working container from the container image by using the buildah from <image> command. Then you can modify the container by using the buildah mount and buildah copy commands. For more information, see Working with containers using Buildah . Container image from an existing container You can create a new container image by using the bulidah commit command. Optionally, you can push the newly created container image to a container registry by using the buildah push command. For more information, see Working with containers using Buildah . Container image from instructions in a Containerfile You can build a container image from instructions in a Containerfile by using the buildah build or buildah bud commands. For more information, see Building and image from a Containerfile using Buildah . Using Buildah differs from building images with the docker command in the following ways: No Daemon Buildah requires no container runtime daemon. Base image or scratch You can build an image based on another container or start from scratch with an empty image . Reduced image size Buildah images do not include build tools, such as gcc , make , and dnf . As a result, the images are more secure and you can more easily transport the images. Compatibility You can easily migrate from Docker to Buildah because Buildah supports building container images with a Containerfile. You can use the same commands inside a Containerfile as in a Dockerfile . Interactive image building You can build images step-by-step interactively by creating and committing changes to containers. Simplified image creation You can create rootfs , generate JSON files, and build OCI-compliant images with Buildah. Flexibility You can script container builds directly in Bash. Additional resources Building with Buildah: Dockerfiles, command line, or scripts article How rootless Buildah works: Building containers in unprivileged environments article 19.2. Buildah and Podman relationship Buildah is a daemonless tool for building Open Container Initiative (OCI) images. Buildah’s commands replicate the commands of a Containerfile . Buildah provides a lower-level interface to build images without requiring a Containerfile . You can also use other scripting languages to build container images. Although you can create containers with Buildah, Buildah containers are primarily created temporarily for the purpose of defining the container image. Podman is a daemonless tool for maintaining and modifying OCI images, such as pulling and tagging. You can create, run, and maintain containers created from those images. Some of the Podman and Buildah commands have the same names but they differ in some aspects: run The podman run command runs a container. The buildah run command is similar to the RUN directive in a Containerfile . commit You can commit Podman containers only with Podman and Buildah containers only with Buildah. rm You can remove Podman containers only with Podman and Buildah containers only with Buildah. Note The default container storage for Buildah is /var/lib/containers/storage for root users and $HOME/.local/share/containers/storage for non-root users. This is the same as the location the CRI-O container engine uses for storing local copies of images. As a result, the images pulled from a registry by either CRI-O or Buildah, or committed by the buildah command, are stored in the same directory structure. However, even though CRI-O and Buildah currently can share images, they cannot share containers. 19.3. Installing Buildah Install the Buildah tool using the dnf command. Procedure Install the Buildah tool: # dnf -y install buildah Verification Display the help message: # buildah -h 19.4. Getting images with Buildah Use the buildah from command to create a new working container from scratch or based on a specified image as a starting point. Prerequisites The container-tools meta-package is installed. Procedure Create a new working container based on the registry.redhat.io/ubi9/ubi image: # buildah from registry.access.redhat.com/ubi9/ubi Getting image source signatures Copying blob… Writing manifest to image destination Storing signatures ubi-working-container Verification List all images in local storage: # buildah images REPOSITORY TAG IMAGE ID CREATED SIZE registry.access.redhat.com/ubi9/ubi latest 272209ff0ae5 2 weeks ago 234 MB List the working containers and their base images: # buildah containers CONTAINER ID BUILDER IMAGE ID IMAGE NAME CONTAINER NAME 01eab9588ae1 * 272209ff0ae5 registry.access.redhat.com/ub... ubi-working-container Additional resources buildah-from , buildah-images , and buildah-containers man pages on your system 19.5. Building an image from a Containerfile with Buildah Use the buildah bud command to build an image using instructions from a Containerfile . Note The buildah bud command uses a Containerfile if found in the context directory, if it is not found the buildah bud command uses a Dockerfile ; otherwise any file can be specified with the --file option. The available commands that are usable inside a Containerfile and a Dockerfile are equivalent. Prerequisites The container-tools meta-package is installed. Procedure Create a Containerfile : # cat Containerfile FROM registry.access.redhat.com/ubi9/ubi ADD myecho /usr/local/bin ENTRYPOINT \"/usr/local/bin/myecho\" Create a myecho script: # cat myecho echo \"This container works!\" Change the access permissions of myecho script: # chmod 755 myecho Build the myecho image using Containerfile in the current directory: # buildah bud -t myecho . STEP 1: FROM registry.access.redhat.com/ubi9/ubi STEP 2: ADD myecho /usr/local/bin STEP 3: ENTRYPOINT \"/usr/local/bin/myecho\" STEP 4: COMMIT myecho ... Storing signatures Verification List all images: # buildah images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/myecho latest b28cd00741b3 About a minute ago 234 MB Run the myecho container based on the localhost/myecho image: # podman run --name=myecho localhost/myecho This container works! List all containers: # podman ps -a 0d97517428d localhost/myecho 12 seconds ago Exited (0) 13 seconds ago myecho Note You can use the podman history command to display the information about each layer used in the image. Additional resources buildah-bud man page on your system 19.6. Creating images from scratch with Buildah Instead of starting with a base image, you can create a new container that holds only a minimal amount of container metadata. When creating an image from scratch container, consider: You can copy the executable with no dependencies into the scratch image and make a few configuration settings to get a minimal container to work. You must initialize an RPM database and add a release package in the container to use tools like dnf or rpm . If you add a lot of packages, consider using the standard UBI or minimal UBI images instead of scratch images. Prerequisites The container-tools meta-package is installed. Procedure You can adds a web service httpd to a container and configures it to run. Create an empty container: # buildah from scratch working-container Mount the working-container container and save the mount point path to the scratchmnt variable: # scratchmnt=$(buildah mount working-container) # echo $scratchmnt /var/lib/containers/storage/overlay/be2eaecf9f74b6acfe4d0017dd5534fde06b2fa8de9ed875691f6ccc791c1836/merged Initialize an RPM database within the scratch image and add the redhat-release package: # dnf install -y --releasever=8 --installroot=$scratchmnt redhat-release Install the httpd service to the scratch directory: # dnf install -y --setopt=reposdir=/etc/yum.repos.d \\ --installroot=$scratchmnt \\ --setopt=cachedir=/var/cache/dnf httpd Create the $scratchmnt/var/www/html/index.html file: # mkdir -p $scratchmnt/var/www/html # echo \"Your httpd container from scratch works!\" > $scratchmnt/var/www/html/index.html Configure working-container to run the httpd daemon directly from the container: # buildah config --cmd \"/usr/sbin/httpd -DFOREGROUND\" working-container # buildah config --port 80/tcp working-container # buildah commit working-container localhost/myhttpd:latest Verification List all images in local storage: # podman images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/myhttpd latest 08da72792f60 2 minutes ago 121 MB Run the localhost/myhttpd image and configure port mappings between the container and the host system: # podman run -p 8080:80 -d --name myhttpd 08da72792f60 Test the web server: # curl localhost:8080 Your httpd container from scratch works! Additional resources buildah-config and buildah-commit man pages on your system 19.7. Removing images with Buildah Use the buildah rmi command to remove locally stored container images. You can remove an image by its ID or name. Prerequisites The container-tools meta-package is installed. Procedure List all images on your local system: # buildah images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/johndoe/webserver latest dc5fcc610313 46 minutes ago 263 MB docker.io/library/mynewecho latest fa2091a7d8b6 17 hours ago 234 MB docker.io/library/myecho2 latest 4547d2c3e436 6 days ago 234 MB localhost/myecho latest b28cd00741b3 6 days ago 234 MB localhost/ubi-micro-httpd latest c6a7678c4139 12 days ago 152 MB registry.access.redhat.com/ubi9/ubi latest 272209ff0ae5 3 weeks ago 234 MB Remove the localhost/myecho image: # buildah rmi localhost/myecho To remove multiple images: # buildah rmi docker.io/library/mynewecho docker.io/library/myecho2 To remove all images from your system: # buildah rmi -a To remove images that have multiple names (tags) associated with them, add the -f option to remove them: # buildah rmi -f localhost/ubi-micro-httpd Verification Ensure that images were removed: # buildah images Additional resources buildah-rmi man page on your system Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_building-container-images-with-buildah"}
{"title": "Building, running, and managing containers", "content": "Building, running, and managing containers Red Hat Enterprise Linux 9 Using Podman, Buildah, and Skopeo on Red Hat Enterprise Linux 9 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/building_running_and_managing_containers/index"}
{"title": "Schedule and quota APIs", "content": "Schedule and quota APIs  Reference guide for schedule and quota APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/schedule_and_quota_apis/index"}
{"title": "Chapter 8. About the installer inventory file", "content": "Chapter 8. About the installer inventory file Red Hat Ansible Automation Platform works against a list of managed nodes or hosts in your infrastructure that are logically organized, using an inventory file. You can use the Red Hat Ansible Automation Platform installer inventory file to specify your installation scenario and describe host deployments to Ansible. By using an inventory file, Ansible can manage a large number of hosts with a single command. Inventories also help you use Ansible more efficiently by reducing the number of command line options you have to specify. The inventory file can be in one of many formats, depending on the inventory plugins that you have. The most common formats are INI and YAML . Inventory files listed in this document are shown in INI format. The location of the inventory file depends on the installer you used. The following table shows possible locations: Installer Location RPM /opt/ansible-automation-platform/installer RPM bundle tar /ansible-automation-platform-setup-bundle-<latest-version> RPM non-bundle tar /ansible-automation-platform-setup-<latest-version> Container bundle tar /ansible-automation-platform-containerized-setup-bundle-<latest-version> Container non-bundle tar /ansible-automation-platform-containerized-setup-<latest-version> You can verify the hosts in your inventory using the command: ansible all -i <path-to-inventory-file. --list-hosts Example inventory file [automationcontroller] controller.example.com [automationhub] automationhub.example.com [automationedacontroller] automationedacontroller.example.com [automationgateway] gateway.example.com [database] data.example.com [all:vars] admin_password='<password>' pg_host='' pg_port='' pg_database='awx' pg_username='awx' pg_password='<password>' registry_url='registry.redhat.io' registry_username='<registry username>' registry_password='<registry password>' automationgateway_admin_password=\"\" automationgateway_pg_host=\"\" automationgateway_pg_database=\"\" automationgateway_pg_username=\"\" automationgateway_pg_password=\"\" The first part of the inventory file specifies the hosts or groups that Ansible can work with. For more information on registry_username and registry_password , see Setting registry_username and registry_password . Platform gateway is the service that handles authentication and authorization for the Ansible Automation Platform. It provides a single entry into the Ansible Automation Platform and serves the platform user interface so you can authenticate and access all of the Ansible Automation Platform services from a single location. 8.1. Guidelines for hosts and groups Databases When using an external database, ensure the [database] sections of your inventory file are properly set up. To improve performance, do not colocate the database and the automation controller on the same server. Important When using an external database with Ansible Automation Platform, you must create and maintain that database. Ensure that you clear your external database when uninstalling the Ansible Automation Platform. Automation hub If there is an [automationhub] group, you must include the variables automationhub_pg_host and automationhub_pg_port . Add Ansible automation hub information in the [automationhub] group. Do not install Ansible automation hub and automation controller on the same node. Provide a reachable IP address or fully qualified domain name (FQDN) for the [automationhub] and [automationcontroller] hosts to ensure that users can synchronize and install content from Ansible automation hub and automation controller from a different node. The FQDN must not contain the _ symbol, as it will not be processed correctly in Skopeo. You may use the - symbol, as long as it is not at the start or the end of the host name. Do not use localhost . Private automation hub Do not install private automation hub and automation controller on the same node. You can use the same PostgreSQL (database) instance, but they must use a different (database) name. If you install private automation hub from an internal address, and have a certificate which only encompasses the external address, it can result in an installation you cannot use as a container registry without certificate issues. Important You must separate the installation of automation controller and Ansible automation hub because the [database] group does not distinguish between the two if both are installed at the same time. If you use one value in [database] and both automation controller and Ansible automation hub define it, they would use the same database. Automation controller Automation controller does not configure replication or failover for the database that it uses. Automation controller works with any replication that you have. Event-Driven Ansible controller Event-Driven Ansible controller must be installed on a separate server and cannot be installed on the same host as automation hub and automation controller. Platform gateway The platform gateway is the service that handles authentication and authorization for Ansible Automation Platform. It provides a single entry into the platform and serves the platform’s user interface. Clustered installations When upgrading an existing cluster, you can also reconfigure your cluster to omit existing instances or instance groups. Omitting the instance or the instance group from the inventory file is not enough to remove them from the cluster. In addition to omitting instances or instance groups from the inventory file, you must also deprovision instances or instance groups before starting the upgrade. For more information, see Deprovisioning nodes or groups . Otherwise, omitted instances or instance groups continue to communicate with the cluster, which can cause issues with automation controller services during the upgrade. If you are creating a clustered installation setup, you must replace [localhost] with the hostname or IP address of all instances. Installers for automation controller and automation hub do not accept [localhost] All nodes and instances must be able to reach any others by using this hostname or address. You cannot use the localhost ansible_connection=local on one of the nodes. Use the same format for the host names of all the nodes. Therefore, this does not work: [automationhub] localhost ansible_connection=local hostA hostB.example.com 172.27.0.4 Instead, use these formats: [automationhub] hostA hostB hostC or [automationhub] hostA.example.com hostB.example.com hostC.example.com 8.2. Deprovisioning nodes or groups You can deprovision nodes and instance groups using the Ansible Automation Platform installer. Running the installer will remove all configuration files and logs attached to the nodes in the group. Note You can deprovision any hosts in your inventory except for the first host specified in the [automationcontroller] group. To deprovision nodes, append node_state=deprovision to the node or group within the inventory file. For example: To remove a single node from a deployment: [automationcontroller] host1.example.com host2.example.com host4.example.com node_state=deprovision or To remove an entire instance group from a deployment: [instance_group_restrictedzone] host4.example.com host5.example.com [instance_group_restrictedzone:vars] node_state=deprovision 8.3. Inventory variables The second part of the example inventory file, following [all:vars] , is a list of variables used by the installer. Using all means the variables apply to all hosts. To apply variables to a particular host, use [hostname:vars] . For example, [automationhub:vars] . 8.4. Rules for declaring variables in inventory files The values of string variables are declared in quotes. For example: pg_database='awx' pg_username='awx' pg_password='<password>' When declared in a :vars section, INI values are interpreted as strings. For example, var=FALSE creates a string equal to FALSE . Unlike host lines, :vars sections accept only a single entry per line, so everything after the = must be the value for the entry. Host lines accept multiple key=value parameters per line. Therefore they need a way to indicate that a space is part of a value rather than a separator. Values that contain whitespace can be quoted (single or double). For more information, see Python shlex parsing rules . If a variable value set in an INI inventory must be a certain type (for example, a string or a boolean value), always specify the type with a filter in your task. Do not rely on types set in INI inventories when consuming variables. Note Consider using YAML format for inventory sources to avoid confusion on the actual type of a variable. The YAML inventory plugin processes variable values consistently and correctly. If a parameter value in the Ansible inventory file contains special characters, such as #, { or }, you must double-escape the value (that is enclose the value in both single and double quotation marks). For example, to use mypasswordwith#hashsigns as a value for the variable pg_password , declare it as pg_password='\"mypasswordwith#hashsigns\"' in the Ansible host inventory file. Disclaimer : Links contained in this information to external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content. 8.5. Securing secrets in the inventory file You can encrypt sensitive or secret variables with Ansible Vault. However, encrypting the variable names and the variable values makes it hard to find the source of the values. To circumvent this, you can encrypt the variables individually by using ansible-vault encrypt_string , or encrypt a file containing the variables. Procedure Create a file labeled credentials.yml to store the encrypted credentials. $ cat credentials.yml admin_password: my_long_admin_pw pg_password: my_long_pg_pw registry_password: my_long_registry_pw Encrypt the credentials.yml file using ansible-vault . $ ansible-vault encrypt credentials.yml New Vault password: Confirm New Vault password: Encryption successful Important Store your encrypted vault password in a safe place. Verify that the credentials.yml file is encrypted. $ cat credentials.yml $ANSIBLE_VAULT;1.1; AES256363836396535623865343163333339613833363064653364656138313534353135303764646165393765393063303065323466663330646232363065316666310a373062303133376339633831303033343135343839626136323037616366326239326530623438396136396536356433656162333133653636616639313864300a353239373433313339613465326339313035633565353464356538653631633464343835346432376638623533613666326136343332313163343639393964613265616433363430633534303935646264633034383966336232303365383763 Run setup.sh for installation of Ansible Automation Platform 2.5 and pass both credentials.yml and the --ask-vault-pass option . $ ANSIBLE_BECOME_METHOD='sudo' ANSIBLE_BECOME=True ANSIBLE_HOST_KEY_CHECKING=False ./setup.sh -e @credentials.yml -- --ask-vault-pass 8.6. Additional inventory file variables You can further configure your Red Hat Ansible Automation Platform installation by including additional variables in the inventory file. These configurations add optional features for managing your Red Hat Ansible Automation Platform. Add these variables by editing the inventory file using a text editor. A table of predefined values for inventory file variables can be found in Inventory file variables in the Red Hat Ansible Automation Platform Installation Guide . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/planning_your_installation/about_the_installer_inventory_file"}
{"title": "Chapter 26. Using the container-tools API", "content": "Chapter 26. Using the container-tools API The new REST based Podman 2.0 API replaces the old remote API for Podman that used the varlink library. The new API works in both a rootful and a rootless environment. The Podman v2.0 RESTful API consists of the Libpod API providing support for Podman, and Docker-compatible API. With this new REST API, you can call Podman from platforms such as cURL, Postman, Google’s Advanced REST client, and many others. Note As the podman service supports socket activation, unless connections on the socket are active, podman service will not run. Hence, to enable socket activation functionality, you need to manually start the podman.socket service. When a connection becomes active on the socket, it starts the podman service and runs the requested API action. Once the action is completed, the podman process ends, and the podman service returns to an inactive state. 26.1. Enabling the Podman API using systemd in root mode You can do the following: Use systemd to activate the Podman API socket. Use a Podman client to perform basic commands. Prerequisities The podman-remote package is installed. # dnf install podman-remote Procedure Start the service immediately: # systemctl enable --now podman.socket To enable the link to var/lib/docker.sock using the docker-podman package: # dnf install podman-docker Verification Display system information of Podman: # podman-remote info Verify the link: # ls -al /var/run/docker.sock lrwxrwxrwx. 1 root root 23 Nov 4 10:19 /var/run/docker.sock -> /run/podman/podman.sock Additional resources Podman v2.0 RESTful API A First Look At Podman 2.0 API Sneak peek: Podman’s new REST API 26.2. Enabling the Podman API using systemd in rootless mode You can use systemd to activate the Podman API socket and podman API service. Prerequisites The podman-remote package is installed. # dnf install podman-remote Procedure Enable and start the service immediately: $ systemctl --user enable --now podman.socket Optional: To enable programs using Docker to interact with the rootless Podman socket: $ export DOCKER_HOST=unix:///run/user/<uid>/podman//podman.sock Verification Check the status of the socket: $ systemctl --user status podman.socket ● podman.socket - Podman API Socket Loaded: loaded (/usr/lib/systemd/user/podman.socket; enabled; vendor preset: enabled) Active: active (listening) since Mon 2021-08-23 10:37:25 CEST; 9min ago Docs: man:podman-system-service(1) Listen: /run/user/1000/podman/podman.sock (Stream) CGroup: /user.slice/user-1000.slice/user@1000.service/podman.socket The podman.socket is active and is listening at /run/user/ <uid> /podman.podman.sock , where <uid> is the user’s ID. Display system information of Podman: $ podman-remote info Additional resources Podman v2.0 RESTful API A First Look At Podman 2.0 API Sneak peek: Podman’s new REST API Exploring Podman RESTful API using Python and Bash 26.3. Running the Podman API manually You can run the Podman API. This is useful for debugging API calls, especially when using the Docker compatibility layer. Prerequisities The podman-remote package is installed. # dnf install podman-remote Procedure Run the service for the REST API: # podman system service -t 0 --log-level=debug The value of 0 means no timeout. The default endpoint for a rootful service is unix:/run/podman/podman.sock . The --log-level <level> option sets the logging level. The standard logging levels are debug , info , warn , error , fatal , and panic . In another terminal, display system information of Podman. The podman-remote command, unlike the regular podman command, communicates through the Podman socket: # podman-remote info To troubleshoot the Podman API and display request and responses, use the curl comman. To get the information about the Podman installation on the Linux server in JSON format: # curl -s --unix-socket /run/podman/podman.sock http://d/v1.0.0/libpod/info | jq { \"host\": { \"arch\": \"amd64\", \"buildahVersion\": \"1.15.0\", \"cgroupVersion\": \"v1\", \"conmon\": { \"package\": \"conmon-2.0.18-1.module+el8.3.0+7084+c16098dd.x86_64\", \"path\": \"/usr/bin/conmon\", \"version\": \"conmon version 2.0.18, commit: 7fd3f71a218f8d3a7202e464252aeb1e942d17eb\" }, … \"version\": { \"APIVersion\": 1, \"Version\": \"2.0.0\", \"GoVersion\": \"go1.14.2\", \"GitCommit\": \"\", \"BuiltTime\": \"Thu Jan 1 01:00:00 1970\", \"Built\": 0, \"OsArch\": \"linux/amd64\" } } A jq utility is a command-line JSON processor. Pull the registry.access.redhat.com/ubi8/ubi container image: # curl -XPOST --unix-socket /run/podman/podman.sock -v 'http://d/v1.0.0/images/create?fromImage=registry.access.redhat.com%2Fubi8%2Fubi' * Trying /run/podman/podman.sock... * Connected to d (/run/podman/podman.sock) port 80 (#0) > POST /v1.0.0/images/create?fromImage=registry.access.redhat.com%2Fubi8%2Fubi HTTP/1.1 > Host: d > User-Agent: curl/7.61.1 > Accept: / > < HTTP/1.1 200 OK < Content-Type: application/json < Date: Tue, 20 Oct 2020 13:58:37 GMT < Content-Length: 231 < {\"status\":\"pulling image () from registry.access.redhat.com/ubi8/ubi:latest, registry.redhat.io/ubi8/ubi:latest\",\"error\":\"\",\"progress\":\"\",\"progressDetail\":{},\"id\":\"ecbc6f53bba0d1923ca9e92b3f747da8353a070fccbae93625bd8b47dbee772e\"} * Connection #0 to host d left intact Display the pulled image: # curl --unix-socket /run/podman/podman.sock -v 'http://d/v1.0.0/libpod/images/json' | jq * Trying /run/podman/podman.sock... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* Connected to d (/run/podman/podman.sock) port 80 ( 0) > GET /v1.0.0/libpod/images/json HTTP/1.1 > Host: d > User-Agent: curl/7.61.1 > Accept: / > < HTTP/1.1 200 OK < Content-Type: application/json < Date: Tue, 20 Oct 2020 13:59:55 GMT < Transfer-Encoding: chunked < { [12498 bytes data] 100 12485 0 12485 0 0 2032k 0 --:--:-- --:--:-- --:--:-- 2438k * Connection #0 to host d left intact [ { \"Id\": \"ecbc6f53bba0d1923ca9e92b3f747da8353a070fccbae93625bd8b47dbee772e\", \"RepoTags\": [ \"registry.access.redhat.com/ubi8/ubi:latest\", \"registry.redhat.io/ubi8/ubi:latest\" ], \"Created\": \"2020-09-01T19:44:12.470032Z\", \"Size\": 210838671, \"Labels\": { \"architecture\": \"x86_64\", \"build-date\": \"2020-09-01T19:43:46.041620\", \"com.redhat.build-host\": \"cpt-1008.osbs.prod.upshift.rdu2.redhat.com\", ... \"maintainer\": \"Red Hat, Inc.\", \"name\": \"ubi8\", ... \"summary\": \"Provides the latest release of Red Hat Universal Base Image 8.\", \"url\": \"https://access.redhat.com/containers/ /registry.access.redhat.com/ubi8/images/8.2-347\", ... }, \"Names\": [ \"registry.access.redhat.com/ubi8/ubi:latest\", \"registry.redhat.io/ubi8/ubi:latest\" ], ... ] } ] Additional resources Podman v2.0 RESTful API Sneak peek: Podman’s new REST API Exploring Podman RESTful API using Python and Bash podman-system-service man page on your system Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/using-the-container-tools-api"}
{"title": "Integrating OpenShift Container Platform data into cost management", "content": "Integrating OpenShift Container Platform data into cost management Cost Management Service 1-latest Learn how to add and configure your OpenShift Container Platform integrations Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html-single/integrating_openshift_container_platform_data_into_cost_management/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/legal-notice"}
{"title": "Getting started with cost management", "content": "Getting started with cost management Cost Management Service 1-latest Learn how you can track your OpenShift costs Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html-single/getting_started_with_cost_management/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_gitops/1.15/html/troubleshooting_issues/legal-notice"}
{"title": "Observability", "content": "Observability Red Hat OpenShift GitOps 1.15 Using observability features to view Argo CD logs and monitor the performance and health of Argo CD and application resources Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_gitops/1.15/html/observability/index"}
{"title": "Chapter 1. Issue: Auto-reboot during Argo CD sync with machine configurations", "content": "Chapter 1. Issue: Auto-reboot during Argo CD sync with machine configurations In the Red Hat OpenShift Container Platform, nodes are updated automatically through the Red Hat OpenShift Machine Config Operator (MCO). A Machine Config Operator (MCO) is a custom resource that is used by the cluster to manage the complete life cycle of its nodes. When an MCO resource is created or updated in a cluster, the MCO picks up the update, performs the necessary changes to the selected nodes, and restarts the nodes gracefully by cordoning, draining, and rebooting those nodes. It handles everything from the kernel to the kubelet. However, interactions between the MCO and the GitOps workflow can introduce major performance issues and other undesired behaviors. This section shows how to make the MCO and the Argo CD GitOps orchestration tool work well together. 1.1. Solution: Enhance performance in machine configurations and Argo CD When you are using a Machine Config Operator as part of a GitOps workflow, the following sequence can produce suboptimal performance: Argo CD starts an automated sync job after a commit to the Git repository that contains application resources. If Argo CD notices a new or an updated machine configuration while the sync operation is in process, MCO picks up the change to the machine configuration and starts rebooting the nodes to apply the change. If a rebooting node in the cluster contains the Argo CD application controller, the application controller terminates, and the application sync is aborted. As the MCO reboots the nodes in sequential order, and the Argo CD workloads can be rescheduled on each reboot, it can take some time for the sync to be completed. This results in an undefined behavior until the MCO has rebooted all nodes affected by the machine configurations within the sync. 1.2. Additional resources Preventing nodes from auto-rebooting during Argo CD sync with machine configs Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_gitops/1.15/html/troubleshooting_issues/auto-reboot-during-argo-cd-sync-with-machine-configurations"}
{"title": "Chapter 18. Running Skopeo, Buildah, and Podman in a container", "content": "Chapter 18. Running Skopeo, Buildah, and Podman in a container You can run Skopeo, Buildah, and Podman in a container. With Skopeo, you can inspect images on a remote registry without having to download the entire image with all its layers. You can also use Skopeo for copying images, signing images, syncing images, and converting images across different formats and layer compressions. Buildah facilitates building OCI container images. With Buildah, you can create a working container, either from scratch or using an image as a starting point. You can create an image either from a working container or using the instructions in a Containerfile . You can mount and unmount a working container’s root filesystem. With Podman, you can manage containers and images, volumes mounted into those containers, and pods made from groups of containers. Podman is based on a libpod library for container lifecycle management. The libpod library provides APIs for managing containers, pods, container images, and volumes. Reasons to run Buildah, Skopeo, and Podman in a container: CI/CD system : Podman and Skopeo : You can run a CI/CD system inside of Kubernetes or use OpenShift to build your container images, and possibly distribute those images across different container registries. To integrate Skopeo into a Kubernetes workflow, you must run it in a container. Buildah : You want to build OCI/container images within a Kubernetes or OpenShift CI/CD systems that are constantly building images. Previously, a Docker socket was used for connecting to the container engine and performing a docker build command. This was the equivalent of giving root access to the system without requiring a password which is not secure. For this reason, use Buildah in a container instead. Different versions : All : You are running an older operating system on the host but you want to run the latest version of Skopeo, Buildah, or Podman. The solution is to run the container tools in a container. For example, this is useful for running the latest version of the container tools provided in Red Hat Enterprise Linux 8 on a Red Hat Enterprise Linux 7 container host which does not have access to the newest versions natively. HPC environment : All : A common restriction in HPC environments is that non-root users are not allowed to install packages on the host. When you run Skopeo, Buildah, or Podman in a container, you can perform these specific tasks as a non-root user. 18.1. Running Skopeo in a container You can inspect a remote container image using Skopeo. Running Skopeo in a container means that the container root filesystem is isolated from the host root filesystem. To share or copy files between the host and container, you have to mount files and directories. Prerequisites The container-tools meta-package is installed. Procedure Log in to the registry.redhat.io registry: $ podman login registry.redhat.io Username: myuser@mycompany.com Password: <password> Login Succeeded! Get the registry.redhat.io/rhel9/skopeo container image: $ podman pull registry.redhat.io/rhel9/skopeo Inspect a remote container image registry.access.redhat.com/ubi9/ubi using Skopeo: $ podman run --rm registry.redhat.io/rhel9/skopeo \\ skopeo inspect docker://registry.access.redhat.com/ubi9/ubi { \"Name\": \"registry.access.redhat.com/ubi9/ubi\", ... \"Labels\": { \"architecture\": \"x86_64\", ... \"name\": \"ubi9\", ... \"summary\": \"Provides the latest release of Red Hat Universal Base Image 9.\", \"url\": \"https://access.redhat.com/containers/#/registry.access.redhat.com/ubi9/images/8.2-347\", ... }, \"Architecture\": \"amd64\", \"Os\": \"linux\", \"Layers\": [ ... ], \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"container=oci\" ] } The --rm option removes the registry.redhat.io/rhel9/skopeo image after the container exits. Additional resources How to run skopeo in a container 18.2. Running Skopeo in a container using credentials Working with container registries requires an authentication to access and alter data. Skopeo supports various ways to specify credentials. With this approach you can specify credentials on the command line using the --cred USERNAME[:PASSWORD] option. Prerequisites The container-tools meta-package is installed. Procedure Inspect a remote container image using Skopeo against a locked registry: $ podman run --rm registry.redhat.io/rhel9/skopeo inspect --creds $USER:$PASSWORD docker://$IMAGE Additional resources How to run skopeo in a container 18.3. Running Skopeo in a container using authfiles You can use an authentication file (authfile) to specify credentials. The skopeo login command logs into the specific registry and stores the authentication token in the authfile. The advantage of using authfiles is preventing the need to repeatedly enter credentials. When running on the same host, all container tools such as Skopeo, Buildah, and Podman share the same authfile. When running Skopeo in a container, you have to either share the authfile on the host by volume-mounting the authfile in the container, or you have to reauthenticate within the container. Prerequisites The container-tools meta-package is installed. Procedure Inspect a remote container image using Skopeo against a locked registry: $ podman run --rm -v $AUTHFILE:/auth.json registry.redhat.io/rhel9/skopeo inspect docker://$IMAGE The -v $AUTHFILE:/auth.json option volume-mounts an authfile at /auth.json within the container. Skopeo can now access the authentication tokens in the authfile on the host and get secure access to the registry. Other Skopeo commands work similarly, for example: Use the skopeo-copy command to specify credentials on the command line for the source and destination image using the --source-creds and --dest-creds options. It also reads the /auth.json authfile. If you want to specify separate authfiles for the source and destination image, use the --source-authfile and --dest-authfile options and volume-mount those authfiles from the host into the container. Additional resources How to run skopeo in a container 18.4. Copying container images to or from the host Skopeo, Buildah, and Podman share the same local container-image storage. If you want to copy containers to or from the host container storage, you need to mount it into the Skopeo container. Note The path to the host container storage differs between root ( /var/lib/containers/storage ) and non-root users ( $HOME/.local/share/containers/storage ). Prerequisites The container-tools meta-package is installed. Procedure Copy the registry.access.redhat.com/ubi9/ubi image into your local container storage: $ podman run --privileged --rm -v $HOME/.local/share/containers/storage:/var/lib/containers/storage \\ registry.redhat.io/rhel9/skopeo skopeo copy \\ docker://registry.access.redhat.com/ubi9/ubi containers-storage:registry.access.redhat.com/ubi9/ubi The --privileged option disables all security mechanisms. Red Hat recommends only using this option in trusted environments. To avoid disabling security mechanisms, export the images to a tarball or any other path-based image transport and mount them in the Skopeo container: $ podman save --format oci-archive -o oci.tar $IMAGE $ podman run --rm -v oci.tar:/oci.tar registry.redhat.io/rhel9/skopeo copy oci-archive:/oci.tar $DESTINATION Optional: List images in local storage: $ podman images REPOSITORY TAG IMAGE ID CREATED SIZE registry.access.redhat.com/ubi9/ubi latest ecbc6f53bba0 8 weeks ago 211 MB Additional resources How to run skopeo in a container 18.5. Running Buildah in a container The procedure demonstrates how to run Buildah in a container and create a working container based on an image. Prerequisites The container-tools meta-package is installed. Procedure Log in to the registry.redhat.io registry: $ podman login registry.redhat.io Username: myuser@mycompany.com Password: <password> Login Succeeded! Pull and run the registry.redhat.io/rhel9/buildah image: # podman run --rm --device /dev/fuse -it \\ registry.redhat.io/rhel9/buildah /bin/bash The --rm option removes the registry.redhat.io/rhel9/buildah image after the container exits. The --device option adds a host device to the container. The sys_chroot - capability to change to a different root directory. It is not included in the default capabilities of a container. Create a new container using a registry.access.redhat.com/ubi9 image: # buildah from registry.access.redhat.com/ubi9 ... ubi9-working-container Run the ls / command inside the ubi9-working-container container: # buildah run --isolation=chroot ubi9-working-container ls / bin boot dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv Optional: List all images in a local storage: # buildah images REPOSITORY TAG IMAGE ID CREATED SIZE registry.access.redhat.com/ubi9 latest ecbc6f53bba0 5 weeks ago 211 MB Optional: List the working containers and their base images: # buildah containers CONTAINER ID BUILDER IMAGE ID IMAGE NAME CONTAINER NAME 0aaba7192762 * ecbc6f53bba0 registry.access.redhat.com/ub... ubi9-working-container Optional: Push the registry.access.redhat.com/ubi9 image to the a local registry located on registry.example.com : # buildah push ecbc6f53bba0 registry.example.com:5000/ubi9/ubi Additional resources Best practices for running Buildah in a container 18.6. Privileged and unprivileged Podman containers By default, Podman containers are unprivileged and cannot, for example, modify parts of the operating system on the host. This is because by default a container is only allowed limited access to devices. The following list emphasizes important properties of privileged containers. You can run the privileged container using the podman run --privileged <image_name> command. A privileged container is given the same access to devices as the user launching the container. A privileged container disables the security features that isolate the container from the host. Dropped Capabilities, limited devices, read-only mount points, Apparmor/SELinux separation, and Seccomp filters are all disabled. A privileged container cannot have more privileges than the account that launched them. Additional resources How to use the --privileged flag with container engines podman-run man page on your system 18.7. Running Podman with extended privileges If you cannot run your workloads in a rootless environment, you need to run these workloads as a root user. Running a container with extended privileges should be done judiciously, because it disables all security features. Prerequisites The container-tools meta-package is installed. Procedure Run the Podman container in the Podman container: $ podman run --privileged --name=privileged_podman \\ registry.access.redhat.com//podman podman run ubi9 echo hello Resolved \"ubi9\" as an alias (/etc/containers/registries.conf.d/001-rhel-shortnames.conf) Trying to pull registry.access.redhat.com/ubi9:latest... ... Storing signatures hello Run the outer container named privileged_podman based on the registry.access.redhat.com/ubi9/podman image. The --privileged option disables the security features that isolate the container from the host. Run podman run ubi9 echo hello command to create the inner container based on the ubi9 image. Notice that the ubi9 short image name was resolved as an alias. As a result, the registry.access.redhat.com/ubi9:latest image is pulled. Verification List all containers: $ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 52537876caf4 registry.access.redhat.com/ubi9/podman podman run ubi9 e... 30 seconds ago Exited (0) 13 seconds ago privileged_podman Additional resources How to use Podman inside of a container podman-run man page on your system 18.8. Running Podman with less privileges You can run two nested Podman containers without the --privileged option. Running the container without the --privileged option is a more secure option. This can be useful when you want to try out different versions of Podman in the most secure way possible. Prerequisites The container-tools meta-package is installed. Procedure Run two nested containers: $ podman run --name=unprivileged_podman --security-opt label=disable \\ --user podman --device /dev/fuse \\ registry.access.redhat.com/ubi9/podman \\ podman run ubi9 echo hello Run the outer container named unprivileged_podman based on the registry.access.redhat.com/ubi9/podman image. The --security-opt label=disable option disables SELinux separation on the host Podman. SELinux does not allow containerized processes to mount all of the file systems required to run inside a container. The --user podman option automatically causes the Podman inside the outer container to run within the user namespace. The --device /dev/fuse option uses the fuse-overlayfs package inside the container. This option adds /dev/fuse to the outer container, so that Podman inside the container can use it. Run podman run ubi9 echo hello command to create the inner container based on the ubi9 image. Notice that the ubi9 short image name was resolved as an alias. As a result, the registry.access.redhat.com/ubi9:latest image is pulled. Verification List all containers: $ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a47b26290f43 podman run ubi9 e... 30 seconds ago Exited (0) 13 seconds ago unprivileged_podman 18.9. Building a container inside a Podman container You can run a container in a container using Podman. This example shows how to use Podman to build and run another container from within this container. The container will run \"Moon-buggy\", a simple text-based game. Prerequisites The container-tools meta-package is installed. You are logged in to the registry.redhat.io registry: # podman login registry.redhat.io Procedure Run the container based on registry.redhat.io/rhel9/podman image: # podman run --privileged --name podman_container -it \\ registry.redhat.io/rhel9/podman /bin/bash Run the outer container named podman_container based on the registry.redhat.io/rhel9/podman image. The --it option specifies that you want to run an interactive bash shell within a container. The --privileged option disables the security features that isolate the container from the host. Create a Containerfile inside the podman_container container: # vi Containerfile FROM registry.access.redhat.com/ubi9/ubi RUN dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm RUN dnf -y install moon-buggy && dnf clean all CMD [\"/usr/bin/moon-buggy\"] The commands in the Containerfile cause the following build command to: Build a container from the registry.access.redhat.com/ubi9/ubi image. Install the epel-release-latest-8.noarch.rpm package. Install the moon-buggy package. Set the container command. Build a new container image named moon-buggy using the Containerfile : # podman build -t moon-buggy . Optional: List all images: # podman images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/moon-buggy latest c97c58abb564 13 seconds ago 1.67 GB registry.access.redhat.com/ubi9/ubi latest 4199acc83c6a 132seconds ago 213 MB Run a new container based on a moon-buggy container: # podman run -it --name moon moon-buggy Optional: Tag the moon-buggy image: # podman tag moon-buggy registry.example.com/moon-buggy Optional: Push the moon-buggy image to the registry: # podman push registry.example.com/moon-buggy Additional resources Technology preview: Running a container inside a container Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_running-skopeo-buildah-and-podman-in-a-container"}
{"title": "Security APIs", "content": "Security APIs  Reference guide for security APIs Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/security_apis/index"}
{"title": "Chapter 2. Distribution of content in RHEL 8", "content": "Chapter 2. Distribution of content in RHEL 8 In the following sections, learn how the software is distributed in Red Hat Enterprise Linux 8. 2.1. Repositories Red Hat Enterprise Linux (RHEL) distributes content through different repositories, for example: BaseOS Content in the BaseOS repository consists of the core set of the underlying operating system functionality that provides the foundation for all installations. This content is available in the RPM format and is subject to support terms similar to those in earlier releases of RHEL. AppStream Content in the AppStream repository includes additional user-space applications, runtime languages, and databases in support of the varied workloads and use cases. Content in AppStream is available in the following formats: RPM packages Modules, which are an extension to the RPM format Important Both the BaseOS and AppStream content sets are required by RHEL and are available in all RHEL subscriptions. For installation instructions, see Interactively installing RHEL from installation media . CodeReady Linux Builder The CodeReady Linux Builder repository is available with all RHEL subscriptions. It provides additional packages for use by developers. Red Hat does not support packages included in the CodeReady Linux Builder repository. Additional resources Package manifest 2.2. Application Streams Red Hat Enterprise Linux (RHEL) 8 introduces the concept of multiple versions of user-space components named Application Streams. These components are delivered and updated more frequently than the core operating system packages. This provides more flexibility to customize Red Hat Enterprise Linux RHEL without impacting the underlying stability of the platform or specific deployments. Application Streams are delivered through the AppStream repository. Application Streams are available in the following formats: RPM packages Modules, which are an extension to the RPM format Important Each Application Stream has its own life cycle, and it can be the same or shorter than the life cycle of RHEL 8.For more information, see Red Hat Enterprise Linux Application Streams Life Cycle . Always determine which version of an Application Stream you want to install, and make sure to review the RHEL Application Stream life cycle first. Additional Resources Red Hat Enterprise Linux Life Cycle Red Hat Enterprise Linux Application Streams Life Cycle 2.3. Introduction to modules A module is a set of RPM packages that represent a component. A typical module contains the following package types: Packages with an application Packages with the application-specific dependency libraries Packages with documentation for the application Packages with helper utilities 2.3.1. Module streams Module streams are filters that can be imagined as virtual repositories in the AppStream physical repository. Module streams versions of the AppStream components. Each of the streams receives updates independently, and they can depend on other module streams. Module streams can be active or inactive. Active streams give the system access to the RPM packages within the particular module stream, allowing the installation of the respective component version. A stream is active in the following cases: If an administrator explicitly enables it. If the stream is a dependency of an enabled module. If the stream is the default stream. Each module can have a default stream. Default streams make it easy to consume RHEL packages the usual way without the need to learn about modules. The default stream is active unless the whole module has been disabled or another stream of that module has been enabled. Only one stream of a particular module can be active at a given point in time. Therefore, only one version of a component can be installed on a system. Different versions can be used in separate containers. Important The default stream does not change throughout the RHEL major release. Always consider each stream’s life cycle . Do not rely on the default stream for instances in which the default stream reaches the End of Life status prior to the end of the RHEL major release. Certain module streams depend on other module streams. For example, the following module streams depend on certain perl module streams: perl-App-cpanminus perl-DBD-MySQL perl-DBD-Pg perl-DBD-SQLite perl-DBI perl-YAML freeradius Prior to selecting a particular stream for a runtime user application or a developer application, consider the following: Required functionality and which component versions support it. Compatibility with your application or use case. The life cycle of the Application Stream and your update plan. For a list of all available modules and streams, see the Package manifest . For per-component changes, see the Release Notes . Additional resources Modular dependencies and stream changes 2.3.2. Module profiles A module profile is a list of recommended packages to be installed together for a particular use case such as for a server, client, development, minimal install, or other. These package lists can contain packages outside the module stream, usually from the BaseOS repository or the dependencies of the stream. Installing packages by using a profile is a one-time action provided for the user’s convenience. It does not prevent installing or uninstalling any of the packages provided by the module. It is also possible to install packages by using multiple profiles of the same module stream without any further preparatory steps. Each module stream can have any number of profiles, including none. For any given module stream, some of its profiles can be marked as default and are then used for profile installation actions if you did not explicitly specify a profile. However, the existence of a default profile for a module stream is not required. Example 2.1. httpd module profiles The httpd module, which provides the Apache web server, offers the following profiles for installation: # yum module list httpd Name Stream Profiles Summary httpd 2.4 [d] common [d], devel, minimal Apache HTTP Server Hint: [d]efault, [e]nabled, [x]disabled, [i]nstalled In this example, the following profiles are available: common : The production-ready packages. This is the default profile ( [d] ). devel : The packages necessary for making modifications to httpd . minimal : The smallest set of packages that provides a running Apache web server. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/installing_managing_and_removing_user-space_components/distribution-of-content-in-rhel-8_using-appstream"}
{"title": "", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_openshift_gitops/1.15/"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/legal-notice"}
{"title": "Chapter 5. Automation mesh nodes", "content": "Chapter 5. Automation mesh nodes Automation mesh is an overlay network intended to ease the distribution of work across a large and dispersed collection of workers. This is done through nodes that establish peer-to-peer connections with each other by using existing networks. 5.1. Tested system configurations Each automation mesh VM has been tested with the following component requirements: 16 GB RAM, 4 CPUs, 60 GB local disk, and 3000 IOPS. 5.2. Network ports Automation mesh uses several ports to communicate with its services. These ports must be open and available for incoming connections to the Red Hat Ansible Automation Platform server for it to work. Ensure that these ports are available and are not blocked by the server firewall. Table 5.1. Network ports and protocols Port number Protocol Service Source Destination 80/443 HTTP/HTTPS Receptor Execution node OpenShift Container Platform mesh ingress 80/443 HTTP/HTTPS Receptor Hop node OpenShift Container Platform mesh ingress 27199 TCP Receptor OpenShift Container Platform cluster Execution node 27199 TCP Receptor OpenShift Container Platform cluster Hop node Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/mesh-nodes"}
{"title": "Chapter 15. Managing containers using the Ansible playbook", "content": "Chapter 15. Managing containers using the Ansible playbook With Podman 4.2, you can use the Podman RHEL system role to manage Podman configuration, containers, and systemd services which run Podman containers. RHEL system roles provide a configuration interface to remotely manage multiple RHEL systems. You can use the interface to manage system configurations across multiple versions of RHEL, as well as adopting new major releases. For more information, see the Automating system administration by using RHEL system roles . 15.1. Creating a rootless container with bind mount by using the podman RHEL system role You can use the podman RHEL system role to create rootless containers with bind mount by running an Ansible playbook and with that, manage your application configuration. The example Ansible playbook starts two Kubernetes pods: one for a database and another for a web application. The database pod configuration is specified in the playbook, while the web application pod is defined in an external YAML file. Prerequisites You have prepared the control node and the managed nodes You are logged in to the control node as a user who can run playbooks on the managed nodes. The account you use to connect to the managed nodes has sudo permissions on them. The user and group webapp exist, and must be listed in the /etc/subuid and /etc/subgid files on the host. Procedure Create a playbook file, for example ~/playbook.yml , with the following content: - name: Configure Podman hosts: managed-node-01.example.com tasks: - name: Create a web application and a database ansible.builtin.include_role: name: rhel-system-roles.podman vars: podman_create_host_directories: true podman_firewall: - port: 8080-8081/tcp state: enabled - port: 12340/tcp state: enabled podman_selinux_ports: - ports: 8080-8081 setype: http_port_t podman_kube_specs: - state: started run_as_user: dbuser run_as_group: dbgroup kube_file_content: apiVersion: v1 kind: Pod metadata: name: db spec: containers: - name: db image: quay.io/linux-system-roles/mysql:5.6 ports: - containerPort: 1234 hostPort: 12340 volumeMounts: - mountPath: /var/lib/db:Z name: db volumes: - name: db hostPath: path: /var/lib/db - state: started run_as_user: webapp run_as_group: webapp kube_file_src: /path/to/webapp.yml The settings specified in the example playbook include the following: run_as_user and run_as_group Specify that containers are rootless. kube_file_content Contains a Kubernetes YAML file defining the first container named db . You can generate the Kubernetes YAML file by using the podman kube generate command. The db container is based on the quay.io/db/db:stable container image. The db bind mount maps the /var/lib/db directory on the host to the /var/lib/db directory in the container. The Z flag labels the content with a private unshared label, therefore, only the db container can access the content. kube_file_src: <path> Defines the second container. The content of the /path/to/webapp.yml file on the controller node will be copied to the kube_file field on the managed node. volumes: <list> A YAML list to define the source of the data to provide in one or more containers. For example, a local disk on the host ( hostPath ) or other disk device. volumeMounts: <list> A YAML list to define the destination where the individual container will mount a given volume. podman_create_host_directories: true Creates the directory on the host. This instructs the role to check the kube specification for hostPath volumes and create those directories on the host. If you need more control over the ownership and permissions, use podman_host_directories . For details about all variables used in the playbook, see the /usr/share/ansible/roles/rhel-system-roles.podman/README.md file on the control node. Validate the playbook syntax: $ ansible-playbook --syntax-check --ask-vault-pass ~/playbook.yml Note that this command only validates the syntax and does not protect against a wrong but valid configuration. Run the playbook: $ ansible-playbook --ask-vault-pass ~/playbook.yml Additional resources /usr/share/ansible/roles/rhel-system-roles.podman/README.md file /usr/share/doc/rhel-system-roles/podman/ directory 15.2. Creating a rootful container with Podman volume by using the podman RHEL system role You can use the podman RHEL system role to create a rootful container with a Podman volume by running an Ansible playbook and with that, manage your application configuration. The example Ansible playbook deploys a Kubernetes pod named ubi8-httpd running an HTTP server container from the registry.access.redhat.com/ubi8/httpd-24 image. The container’s web content is mounted from a persistent volume named ubi8-html-volume . By default, the podman role creates rootful containers. Prerequisites You have prepared the control node and the managed nodes You are logged in to the control node as a user who can run playbooks on the managed nodes. The account you use to connect to the managed nodes has sudo permissions on them. Procedure Create a playbook file, for example ~/playbook.yml , with the following content: - name: Configure Podman hosts: managed-node-01.example.com tasks: - name: Start Apache server on port 8080 ansible.builtin.include_role: name: rhel-system-roles.podman vars: podman_firewall: - port: 8080/tcp state: enabled podman_kube_specs: - state: started kube_file_content: apiVersion: v1 kind: Pod metadata: name: ubi8-httpd spec: containers: - name: ubi8-httpd image: registry.access.redhat.com/ubi8/httpd-24 ports: - containerPort: 8080 hostPort: 8080 volumeMounts: - mountPath: /var/www/html:Z name: ubi8-html volumes: - name: ubi8-html persistentVolumeClaim: claimName: ubi8-html-volume The settings specified in the example playbook include the following: kube_file_content Contains a Kubernetes YAML file defining the first container named db . You can generate the Kubernetes YAML file by using the podman kube generate command. The ubi8-httpd container is based on the registry.access.redhat.com/ubi8/httpd-24 container image. The ubi8-html-volume maps the /var/www/html directory on the host to the container. The Z flag labels the content with a private unshared label, therefore, only the ubi8-httpd container can access the content. The pod mounts the existing persistent volume named ubi8-html-volume with the mount path /var/www/html . For details about all variables used in the playbook, see the /usr/share/ansible/roles/rhel-system-roles.podman/README.md file on the control node. Validate the playbook syntax: $ ansible-playbook --syntax-check ~/playbook.yml Note that this command only validates the syntax and does not protect against a wrong but valid configuration. Run the playbook: $ ansible-playbook ~/playbook.yml Additional resources /usr/share/ansible/roles/rhel-system-roles.podman/README.md file /usr/share/doc/rhel-system-roles/podman/ directory 15.3. Creating a Quadlet application with secrets by using the podman RHEL system role You can use the podman RHEL system role to create a Quadlet application with secrets by running an Ansible playbook. Prerequisites You have prepared the control node and the managed nodes You are logged in to the control node as a user who can run playbooks on the managed nodes. The account you use to connect to the managed nodes has sudo permissions on them. The certificate and the corresponding private key that the web server in the container should use are stored in the ~/certificate.pem and ~/key.pem files. Procedure Display the contents of the certificate and private key files: $ cat ~/certificate.pem -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- $ cat ~/key.pem -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- You require this information in a later step. Store your sensitive variables in an encrypted file: Create the vault: $ ansible-vault create vault.yml New Vault password: <vault_password> Confirm New Vault password: <vault_password> After the ansible-vault create command opens an editor, enter the sensitive data in the <key> : <value> format: root_password: <root_password> certificate: |- -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- key: |- -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- Ensure that all lines in the certificate and key variables start with two spaces. Save the changes, and close the editor. Ansible encrypts the data in the vault. Create a playbook file, for example ~/playbook.yml , with the following content: - name: Deploy a wordpress CMS with MySQL database hosts: managed-node-01.example.com vars_files: - vault.yml tasks: - name: Create and run the container ansible.builtin.include_role: name: rhel-system-roles.podman vars: podman_create_host_directories: true podman_activate_systemd_unit: false podman_quadlet_specs: - name: quadlet-demo type: network file_content: | [Network] Subnet=192.168.30.0/24 Gateway=192.168.30.1 Label=app=wordpress - file_src: quadlet-demo-mysql.volume - template_src: quadlet-demo-mysql.container.j2 - file_src: envoy-proxy-configmap.yml - file_src: quadlet-demo.yml - file_src: quadlet-demo.kube activate_systemd_unit: true podman_firewall: - port: 8000/tcp state: enabled - port: 9000/tcp state: enabled podman_secrets: - name: mysql-root-password-container state: present skip_existing: true data: \"{{ root_password }}\" - name: mysql-root-password-kube state: present skip_existing: true data: | apiVersion: v1 data: password: \"{{ root_password | b64encode }}\" kind: Secret metadata: name: mysql-root-password-kube - name: envoy-certificates state: present skip_existing: true data: | apiVersion: v1 data: certificate.key: {{ key | b64encode }} certificate.pem: {{ certificate | b64encode }} kind: Secret metadata: name: envoy-certificates The procedure creates a WordPress content management system paired with a MySQL database. The podman_quadlet_specs role variable defines a set of configurations for the Quadlet, which refers to a group of containers or services that work together in a certain way. It includes the following specifications: The Wordpress network is defined by the quadlet-demo network unit. The volume configuration for MySQL container is defined by the file_src: quadlet-demo-mysql.volume field. The template_src: quadlet-demo-mysql.container.j2 field is used to generate a configuration for the MySQL container. Two YAML files follow: file_src: envoy-proxy-configmap.yml and file_src: quadlet-demo.yml . Note that .yml is not a valid Quadlet unit type, therefore these files will just be copied and not processed as a Quadlet specification. The Wordpress and envoy proxy containers and configuration are defined by the file_src: quadlet-demo.kube field. The kube unit refers to the previous YAML files in the [Kube] section as Yaml=quadlet-demo.yml and ConfigMap=envoy-proxy-configmap.yml . Validate the playbook syntax: $ ansible-playbook --syntax-check --ask-vault-pass ~/playbook.yml Note that this command only validates the syntax and does not protect against a wrong but valid configuration. Run the playbook: $ ansible-playbook --ask-vault-pass ~/playbook.yml Additional resources /usr/share/ansible/roles/rhel-system-roles.podman/README.md file /usr/share/doc/rhel-system-roles/podman/ directory Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/managing-containers-using-the-ansible-playbook_building-running-and-managing-containers"}
{"title": "Appendix A. Additional resources for tested deployment models", "content": "Appendix A. Additional resources for tested deployment models This appendix provides a reference for the additional resources relevant to the tested deployment models outlined in Tested deployment models. For additional information about each of the tested topologies described in this document, see the test-topologies GitHub repo . For questions around IBM cloud specific configurations or issues, see IBM support . Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/appendix-topology-resources"}
{"title": "Developing C and C++ applications in RHEL 9", "content": "Developing C and C++ applications in RHEL 9 Red Hat Enterprise Linux 9 Setting up a developer workstation, and developing and debugging C and C++ applications in Red Hat Enterprise Linux 9 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/developing_c_and_cpp_applications_in_rhel_9/index"}
{"title": "Chapter 1. Overview of tested deployment models", "content": "Chapter 1. Overview of tested deployment models Red Hat tests Ansible Automation Platform 2.5 with a defined set of topologies to give you opinionated deployment options. Deploy all components of Ansible Automation Platform so that all features and capabilities are available for use without the need to take further action. Red Hat tests the installation of Ansible Automation Platform 2.5 based on a defined set of infrastructure topologies or reference architectures. Enterprise organizations can use one of the enterprise topologies for production deployments to ensure the highest level of uptime, performance, and continued scalability. Organizations or deployments that are resource constrained can use a growth topology. It is possible to install the Ansible Automation Platform on different infrastructure topologies and with different environment configurations. Red Hat does not fully test topologies outside of published reference architectures. Use a tested topology for all new deployments. 1.1. Installation and deployment models The following table outlines the different ways to install or deploy Ansible Automation Platform: Table 1.1. Ansible Automation Platform installation and deployment models Mode Infrastructure Description Tested topologies RPM Virtual machines and bare metal The RPM installer deploys Ansible Automation Platform on Red Hat Enterprise Linux by using RPMs to install the platform on host machines. Customers manage the product and infrastructure lifecycle. RPM growth topology RPM mixed growth topology RPM enterprise topology RPM mixed enterprise topology Containers Virtual machines and bare metal The containerized installer deploys Ansible Automation Platform on Red Hat Enterprise Linux by using Podman which runs the platform in containers on host machines. Customers manage the product and infrastructure lifecycle. Container growth topology Container enterprise topology Operator Red Hat OpenShift The Operator uses Red Hat OpenShift Operators to deploy Ansible Automation Platform within Red Hat OpenShift. Customers manage the product and infrastructure lifecycle. Operator growth topology Operator enterprise topology Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/overview-tested-deployment-models"}
{"title": "Chapter 4. Administrator tasks", "content": "Chapter 4. Administrator tasks 4.1. Adding Operators to a cluster Cluster administrators can install Operators to an OpenShift Container Platform cluster by subscribing Operators to namespaces with OperatorHub. Note For information on how OLM handles updates for installed Operators colocated in the same namespace, as well as an alternative method for installing Operators with custom global Operator groups, see Multitenancy and Operator colocation . 4.1.1. About Operator installation with OperatorHub OperatorHub is a user interface for discovering Operators; it works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster. As a user with the proper permissions, you can install an Operator from OperatorHub using the OpenShift Container Platform web console or CLI. During installation, you must determine the following initial settings for the Operator: Installation Mode Choose a specific namespace in which to install the Operator. Update Channel If an Operator is available through multiple channels, you can choose which channel you want to subscribe to. For example, to deploy from the stable channel, if available, select it from the list. Approval Strategy You can choose automatic or manual updates. If you choose automatic updates for an installed Operator, when a new version of that Operator is available in the selected channel, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without human intervention. If you select manual updates, when a newer version of an Operator is available, OLM creates an update request. As a cluster administrator, you must then manually approve that update request to have the Operator updated to the new version. Additional resources Understanding OperatorHub 4.1.2. Installing from OperatorHub using the web console You can install and subscribe to an Operator from OperatorHub using the OpenShift Container Platform web console. Prerequisites Access to an OpenShift Container Platform cluster using an account with cluster-admin permissions. Access to an OpenShift Container Platform cluster using an account with Operator installation permissions. Procedure Navigate in the web console to the Operators OperatorHub page. Scroll or type a keyword into the Filter by keyword box to find the Operator you want. For example, type advanced to find the Advanced Cluster Management for Kubernetes Operator. You can also filter options by Infrastructure Features . For example, select Disconnected if you want to see Operators that work in disconnected environments, also known as restricted network environments. Select the Operator to display additional information. Note Choosing a Community Operator warns that Red Hat does not certify Community Operators; you must acknowledge the warning before continuing. Read the information about the Operator and click Install . On the Install Operator page: Select one of the following: All namespaces on the cluster (default) installs the Operator in the default openshift-operators namespace to watch and be made available to all namespaces in the cluster. This option is not always available. A specific namespace on the cluster allows you to choose a specific, single namespace in which to install the Operator. The Operator will only watch and be made available for use in this single namespace. Choose a specific, single namespace in which to install the Operator. The Operator will only watch and be made available for use in this single namespace. Select an Update Channel (if more than one is available). Select Automatic or Manual approval strategy, as described earlier. Click Install to make the Operator available to the selected namespaces on this OpenShift Container Platform cluster. If you selected a Manual approval strategy, the upgrade status of the subscription remains Upgrading until you review and approve the install plan. After approving on the Install Plan page, the subscription upgrade status moves to Up to date . If you selected an Automatic approval strategy, the upgrade status should resolve to Up to date without intervention. After the upgrade status of the subscription is Up to date , select Operators Installed Operators to verify that the cluster service version (CSV) of the installed Operator eventually shows up. The Status should ultimately resolve to InstallSucceeded in the relevant namespace. Note For the All namespaces…​ installation mode, the status resolves to InstallSucceeded in the openshift-operators namespace, but the status is Copied if you check in other namespaces. If it does not: Check the logs in any pods in the openshift-operators project (or other relevant namespace if A specific namespace…​ installation mode was selected) on the Workloads Pods page that are reporting issues to troubleshoot further. 4.1.3. Installing from OperatorHub using the CLI Instead of using the OpenShift Container Platform web console, you can install an Operator from OperatorHub using the CLI. Use the oc command to create or update a Subscription object. Prerequisites Access to an OpenShift Container Platform cluster using an account with Operator installation permissions. Install the oc command to your local system. Procedure View the list of Operators available to the cluster from OperatorHub: $ oc get packagemanifests -n openshift-marketplace Example output NAME CATALOG AGE 3scale-operator Red Hat Operators 91m advanced-cluster-management Red Hat Operators 91m amq7-cert-manager Red Hat Operators 91m ... couchbase-enterprise-certified Certified Operators 91m crunchy-postgres-operator Certified Operators 91m mongodb-enterprise Certified Operators 91m ... etcd Community Operators 91m jaeger Community Operators 91m kubefed Community Operators 91m ... Note the catalog for your desired Operator. Inspect your desired Operator to verify its supported install modes and available channels: $ oc describe packagemanifests <operator_name> -n openshift-marketplace An Operator group, defined by an OperatorGroup object, selects target namespaces in which to generate required RBAC access for all Operators in the same namespace as the Operator group. The namespace to which you subscribe the Operator must have an Operator group that matches the install mode of the Operator, either the AllNamespaces or SingleNamespace mode. If the Operator you intend to install uses the AllNamespaces , then the openshift-operators namespace already has an appropriate Operator group in place. However, if the Operator uses the SingleNamespace mode and you do not already have an appropriate Operator group in place, you must create one. Note The web console version of this procedure handles the creation of the OperatorGroup and Subscription objects automatically behind the scenes for you when choosing SingleNamespace mode. Create an OperatorGroup object YAML file, for example operatorgroup.yaml : Example OperatorGroup object apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: <operatorgroup_name> namespace: <namespace> spec: targetNamespaces: - <namespace> Warning Operator Lifecycle Manager (OLM) creates the following cluster roles for each Operator group: <operatorgroup_name>-admin <operatorgroup_name>-edit <operatorgroup_name>-view When you manually create an Operator group, you must specify a unique name that does not conflict with the existing cluster roles or other Operator groups on the cluster. Create the OperatorGroup object: $ oc apply -f operatorgroup.yaml Create a Subscription object YAML file to subscribe a namespace to an Operator, for example sub.yaml : Example Subscription object apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: <subscription_name> namespace: openshift-operators 1 spec: channel: <channel_name> 2 name: <operator_name> 3 source: redhat-operators 4 sourceNamespace: openshift-marketplace 5 config: env: 6 - name: ARGS value: \"-v=10\" envFrom: 7 - secretRef: name: license-secret volumes: 8 - name: <volume_name> configMap: name: <configmap_name> volumeMounts: 9 - mountPath: <directory_name> name: <volume_name> tolerations: 10 - operator: \"Exists\" resources: 11 requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" nodeSelector: 12 foo: bar 1 For default AllNamespaces install mode usage, specify the openshift-operators namespace. Alternatively, you can specify a custom global namespace, if you have created one. Otherwise, specify the relevant single namespace for SingleNamespace install mode usage. 2 Name of the channel to subscribe to. 3 Name of the Operator to subscribe to. 4 Name of the catalog source that provides the Operator. 5 Namespace of the catalog source. Use openshift-marketplace for the default OperatorHub catalog sources. 6 The env parameter defines a list of Environment Variables that must exist in all containers in the pod created by OLM. 7 The envFrom parameter defines a list of sources to populate Environment Variables in the container. 8 The volumes parameter defines a list of Volumes that must exist on the pod created by OLM. 9 The volumeMounts parameter defines a list of VolumeMounts that must exist in all containers in the pod created by OLM. If a volumeMount references a volume that does not exist, OLM fails to deploy the Operator. 10 The tolerations parameter defines a list of Tolerations for the pod created by OLM. 11 The resources parameter defines resource constraints for all the containers in the pod created by OLM. 12 The nodeSelector parameter defines a NodeSelector for the pod created by OLM. Create the Subscription object: $ oc apply -f sub.yaml At this point, OLM is now aware of the selected Operator. A cluster service version (CSV) for the Operator should appear in the target namespace, and APIs provided by the Operator should be available for creation. Additional resources About Operator groups 4.1.4. Installing a specific version of an Operator You can install a specific version of an Operator by setting the cluster service version (CSV) in a Subscription object. Prerequisites Access to an OpenShift Container Platform cluster using an account with Operator installation permissions OpenShift CLI ( oc ) installed Procedure Create a Subscription object YAML file that subscribes a namespace to an Operator with a specific version by setting the startingCSV field. Set the installPlanApproval field to Manual to prevent the Operator from automatically upgrading if a later version exists in the catalog. For example, the following sub.yaml file can be used to install the Red Hat Quay Operator specifically to version 3.4.0: Subscription with a specific starting Operator version apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: quay-operator namespace: quay spec: channel: quay-v3.4 installPlanApproval: Manual 1 name: quay-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: quay-operator.v3.4.0 2 1 Set the approval strategy to Manual in case your specified version is superseded by a later version in the catalog. This plan prevents an automatic upgrade to a later version and requires manual approval before the starting CSV can complete the installation. 2 Set a specific version of an Operator CSV. Create the Subscription object: $ oc apply -f sub.yaml Manually approve the pending install plan to complete the Operator installation. Additional resources Manually approving a pending Operator update 4.1.5. Preparing for multiple instances of an Operator for multitenant clusters As a cluster administrator, you can add multiple instances of an Operator for use in multitenant clusters. This is an alternative solution to either using the standard All namespaces install mode, which can be considered to violate the principle of least privilege, or the Multinamespace mode, which is not widely adopted. For more information, see \"Operators in multitenant clusters\". In the following procedure, the tenant is a user or group of users that share common access and privileges for a set of deployed workloads. The tenant Operator is the instance of an Operator that is intended for use by only that tenant. Prerequisites All instances of the Operator you want to install must be the same version across a given cluster. Important For more information on this and other limitations, see \"Operators in multitenant clusters\". Procedure Before installing the Operator, create a namespace for the tenant Operator that is separate from the tenant’s namespace. For example, if the tenant’s namespace is team1 , you might create a team1-operator namespace: Define a Namespace resource and save the YAML file, for example, team1-operator.yaml : apiVersion: v1 kind: Namespace metadata: name: team1-operator Create the namespace by running the following command: $ oc create -f team1-operator.yaml Create an Operator group for the tenant Operator scoped to the tenant’s namespace, with only that one namespace entry in the spec.targetNamespaces list: Define an OperatorGroup resource and save the YAML file, for example, team1-operatorgroup.yaml : apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: team1-operatorgroup namespace: team1-operator spec: targetNamespaces: - team1 1 1 Define only the tenant’s namespace in the spec.targetNamespaces list. Create the Operator group by running the following command: $ oc create -f team1-operatorgroup.yaml Next steps Install the Operator in the tenant Operator namespace. This task is more easily performed by using the OperatorHub in the web console instead of the CLI; for a detailed procedure, see Installing from OperatorHub using the web console . Note After completing the Operator installation, the Operator resides in the tenant Operator namespace and watches the tenant namespace, but neither the Operator’s pod nor its service account are visible or usable by the tenant. Additional resources Operators in multitenant clusters 4.1.6. Installing global Operators in custom namespaces When installing Operators with the OpenShift Container Platform web console, the default behavior installs Operators that support the All namespaces install mode into the default openshift-operators global namespace. This can cause issues related to shared install plans and update policies between all Operators in the namespace. For more details on these limitations, see \"Multitenancy and Operator colocation\". As a cluster administrator, you can bypass this default behavior manually by creating a custom global namespace and using that namespace to install your individual or scoped set of Operators and their dependencies. Procedure Before installing the Operator, create a namespace for the installation of your desired Operator. This installation namespace will become the custom global namespace: Define a Namespace resource and save the YAML file, for example, global-operators.yaml : apiVersion: v1 kind: Namespace metadata: name: global-operators Create the namespace by running the following command: $ oc create -f global-operators.yaml Create a custom global Operator group , which is an Operator group that watches all namespaces: Define an OperatorGroup resource and save the YAML file, for example, global-operatorgroup.yaml . Omit both the spec.selector and spec.targetNamespaces fields to make it a global Operator group , which selects all namespaces: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: global-operatorgroup namespace: global-operators Note The status.namespaces of a created global Operator group contains the empty string ( \"\" ), which signals to a consuming Operator that it should watch all namespaces. Create the Operator group by running the following command: $ oc create -f global-operatorgroup.yaml Next steps Install the desired Operator in your custom global namespace. Because the web console does not populate the Installed Namespace menu during Operator installation with custom global namespaces, this task can only be performed with the OpenShift CLI ( oc ). For a detailed procedure, see Installing from OperatorHub using the CLI . Note When you initiate the Operator installation, if the Operator has dependencies, the dependencies are also automatically installed in the custom global namespace. As a result, it is then valid for the dependency Operators to have the same update policy and shared install plans. Additional resources Multitenancy and Operator colocation 4.1.7. Pod placement of Operator workloads By default, Operator Lifecycle Manager (OLM) places pods on arbitrary worker nodes when installing an Operator or deploying Operand workloads. As an administrator, you can use projects with a combination of node selectors, taints, and tolerations to control the placement of Operators and Operands to specific nodes. Controlling pod placement of Operator and Operand workloads has the following prerequisites: Determine a node or set of nodes to target for the pods per your requirements. If available, note an existing label, such as node-role.kubernetes.io/app , that identifies the node or nodes. Otherwise, add a label, such as myoperator , by using a machine set or editing the node directly. You will use this label in a later step as the node selector on your project. If you want to ensure that only pods with a certain label are allowed to run on the nodes, while steering unrelated workloads to other nodes, add a taint to the node or nodes by using a machine set or editing the node directly. Use an effect that ensures that new pods that do not match the taint cannot be scheduled on the nodes. For example, a myoperator:NoSchedule taint ensures that new pods that do not match the taint are not scheduled onto that node, but existing pods on the node are allowed to remain. Create a project that is configured with a default node selector and, if you added a taint, a matching toleration. At this point, the project you created can be used to steer pods towards the specified nodes in the following scenarios: For Operator pods Administrators can create a Subscription object in the project. As a result, the Operator pods are placed on the specified nodes. For Operand pods Using an installed Operator, users can create an application in the project, which places the custom resource (CR) owned by the Operator in the project. As a result, the Operand pods are placed on the specified nodes, unless the Operator is deploying cluster-wide objects or resources in other namespaces, in which case this customized pod placement does not apply. Additional resources Adding taints and tolerations manually to nodes or with machine sets Creating project-wide node selectors Creating a project with a node selector and toleration 4.2. Updating installed Operators As a cluster administrator, you can update Operators that have been previously installed using Operator Lifecycle Manager (OLM) on your OpenShift Container Platform cluster. Note For information on how OLM handles updates for installed Operators colocated in the same namespace, as well as an alternative method for installing Operators with custom global Operator groups, see Multitenancy and Operator colocation . 4.2.1. Preparing for an Operator update The subscription of an installed Operator specifies an update channel that tracks and receives updates for the Operator. You can change the update channel to start tracking and receiving updates from a newer channel. The names of update channels in a subscription can differ between Operators, but the naming scheme typically follows a common convention within a given Operator. For example, channel names might follow a minor release update stream for the application provided by the Operator ( 1.2 , 1.3 ) or a release frequency ( stable , fast ). Note You cannot change installed Operators to a channel that is older than the current channel. Red Hat Customer Portal Labs include the following application that helps administrators prepare to update their Operators: Red Hat OpenShift Container Platform Operator Update Information Checker You can use the application to search for Operator Lifecycle Manager-based Operators and verify the available Operator version per update channel across different versions of OpenShift Container Platform. Cluster Version Operator-based Operators are not included. 4.2.2. Changing the update channel for an Operator You can change the update channel for an Operator by using the OpenShift Container Platform web console. Tip If the approval strategy in the subscription is set to Automatic , the update process initiates as soon as a new Operator version is available in the selected channel. If the approval strategy is set to Manual , you must manually approve pending updates. Prerequisites An Operator previously installed using Operator Lifecycle Manager (OLM). Procedure In the Administrator perspective of the web console, navigate to Operators Installed Operators . Click the name of the Operator you want to change the update channel for. Click the Subscription tab. Click the name of the update channel under Channel . Click the newer update channel that you want to change to, then click Save . For subscriptions with an Automatic approval strategy, the update begins automatically. Navigate back to the Operators Installed Operators page to monitor the progress of the update. When complete, the status changes to Succeeded and Up to date . For subscriptions with a Manual approval strategy, you can manually approve the update from the Subscription tab. 4.2.3. Manually approving a pending Operator update If an installed Operator has the approval strategy in its subscription set to Manual , when new updates are released in its current update channel, the update must be manually approved before installation can begin. Prerequisites An Operator previously installed using Operator Lifecycle Manager (OLM). Procedure In the Administrator perspective of the OpenShift Container Platform web console, navigate to Operators Installed Operators . Operators that have a pending update display a status with Upgrade available . Click the name of the Operator you want to update. Click the Subscription tab. Any update requiring approval are displayed next to Upgrade Status . For example, it might display 1 requires approval . Click 1 requires approval , then click Preview Install Plan . Review the resources that are listed as available for update. When satisfied, click Approve . Navigate back to the Operators Installed Operators page to monitor the progress of the update. When complete, the status changes to Succeeded and Up to date . 4.3. Deleting Operators from a cluster The following describes how to delete, or uninstall, Operators that were previously installed using Operator Lifecycle Manager (OLM) on your OpenShift Container Platform cluster. Important You must successfully and completely uninstall an Operator prior to attempting to reinstall the same Operator. Failure to fully uninstall the Operator properly can leave resources, such as a project or namespace, stuck in a \"Terminating\" state and cause \"error resolving resource\" messages to be observed when trying to reinstall the Operator. For more information, see Reinstalling Operators after failed uninstallation . 4.3.1. Deleting Operators from a cluster using the web console Cluster administrators can delete installed Operators from a selected namespace by using the web console. Prerequisites You have access to an OpenShift Container Platform cluster web console using an account with cluster-admin permissions. Procedure Navigate to the Operators Installed Operators page. Scroll or enter a keyword into the Filter by name field to find the Operator that you want to remove. Then, click on it. On the right side of the Operator Details page, select Uninstall Operator from the Actions list. An Uninstall Operator? dialog box is displayed. Select Uninstall to remove the Operator, Operator deployments, and pods. Following this action, the Operator stops running and no longer receives updates. Note This action does not remove resources managed by the Operator, including custom resource definitions (CRDs) and custom resources (CRs). Dashboards and navigation items enabled by the web console and off-cluster resources that continue to run might need manual clean up. To remove these after uninstalling the Operator, you might need to manually delete the Operator CRDs. 4.3.2. Deleting Operators from a cluster using the CLI Cluster administrators can delete installed Operators from a selected namespace by using the CLI. Prerequisites Access to an OpenShift Container Platform cluster using an account with cluster-admin permissions. oc command installed on workstation. Procedure Ensure the latest version of the subscribed operator (for example, serverless-operator ) is identified in the currentCSV field. $ oc get subscription.operators.coreos.com serverless-operator -n openshift-serverless -o yaml | grep currentCSV Example output currentCSV: serverless-operator.v1.28.0 Delete the subscription (for example, serverless-operator ): $ oc delete subscription.operators.coreos.com serverless-operator -n openshift-serverless Example output subscription.operators.coreos.com \"serverless-operator\" deleted Delete the CSV for the Operator in the target namespace using the currentCSV value from the previous step: $ oc delete clusterserviceversion serverless-operator.v1.28.0 -n openshift-serverless Example output clusterserviceversion.operators.coreos.com \"serverless-operator.v1.28.0\" deleted 4.3.3. Refreshing failing subscriptions In Operator Lifecycle Manager (OLM), if you subscribe to an Operator that references images that are not accessible on your network, you can find jobs in the openshift-marketplace namespace that are failing with the following errors: Example output ImagePullBackOff for Back-off pulling image \"example.com/openshift4/ose-elasticsearch-operator-bundle@sha256:6d2587129c846ec28d384540322b40b05833e7e00b25cca584e004af9a1d292e\" Example output rpc error: code = Unknown desc = error pinging docker registry example.com: Get \"https://example.com/v2/\": dial tcp: lookup example.com on 10.0.0.1:53: no such host As a result, the subscription is stuck in this failing state and the Operator is unable to install or upgrade. You can refresh a failing subscription by deleting the subscription, cluster service version (CSV), and other related objects. After recreating the subscription, OLM then reinstalls the correct version of the Operator. Prerequisites You have a failing subscription that is unable to pull an inaccessible bundle image. You have confirmed that the correct bundle image is accessible. Procedure Get the names of the Subscription and ClusterServiceVersion objects from the namespace where the Operator is installed: $ oc get sub,csv -n <namespace> Example output NAME PACKAGE SOURCE CHANNEL subscription.operators.coreos.com/elasticsearch-operator elasticsearch-operator redhat-operators 5.0 NAME DISPLAY VERSION REPLACES PHASE clusterserviceversion.operators.coreos.com/elasticsearch-operator.5.0.0-65 OpenShift Elasticsearch Operator 5.0.0-65 Succeeded Delete the subscription: $ oc delete subscription <subscription_name> -n <namespace> Delete the cluster service version: $ oc delete csv <csv_name> -n <namespace> Get the names of any failing jobs and related config maps in the openshift-marketplace namespace: $ oc get job,configmap -n openshift-marketplace Example output NAME COMPLETIONS DURATION AGE job.batch/1de9443b6324e629ddf31fed0a853a121275806170e34c926d69e53a7fcbccb 1/1 26s 9m30s NAME DATA AGE configmap/1de9443b6324e629ddf31fed0a853a121275806170e34c926d69e53a7fcbccb 3 9m30s Delete the job: $ oc delete job <job_name> -n openshift-marketplace This ensures pods that try to pull the inaccessible image are not recreated. Delete the config map: $ oc delete configmap <configmap_name> -n openshift-marketplace Reinstall the Operator using OperatorHub in the web console. Verification Check that the Operator has been reinstalled successfully: $ oc get sub,csv,installplan -n <namespace> 4.4. Configuring Operator Lifecycle Manager features The Operator Lifecycle Manager (OLM) controller is configured by an OLMConfig custom resource (CR) named cluster . Cluster administrators can modify this resource to enable or disable certain features. This document outlines the features currently supported by OLM that are configured by the OLMConfig resource. 4.4.1. Disabling copied CSVs When an Operator is installed by Operator Lifecycle Manager (OLM), a simplified copy of its cluster service version (CSV) is created in every namespace that the Operator is configured to watch. These CSVs are known as copied CSVs and communicate to users which controllers are actively reconciling resource events in a given namespace. When Operators are configured to use the AllNamespaces install mode, versus targeting a single or specified set of namespaces, a copied CSV is created in every namespace on the cluster. On especially large clusters, with namespaces and installed Operators potentially in the hundreds or thousands, copied CSVs consume an untenable amount of resources, such as OLM’s memory usage, cluster etcd limits, and networking. To support these larger clusters, cluster administrators can disable copied CSVs for Operators installed with the AllNamespaces mode. Warning If you disable copied CSVs, a user’s ability to discover Operators in the OperatorHub and CLI is limited to Operators installed directly in the user’s namespace. If an Operator is configured to reconcile events in the user’s namespace but is installed in a different namespace, the user cannot view the Operator in the OperatorHub or CLI. Operators affected by this limitation are still available and continue to reconcile events in the user’s namespace. This behavior occurs for the following reasons: Copied CSVs identify the Operators available for a given namespace. Role-based access control (RBAC) scopes the user’s ability to view and discover Operators in the OperatorHub and CLI. Procedure Edit the OLMConfig object named cluster and set the spec.features.disableCopiedCSVs field to true : $ oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OLMConfig metadata: name: cluster spec: features: disableCopiedCSVs: true 1 EOF 1 Disabled copied CSVs for AllNamespaces install mode Operators Verification When copied CSVs are disabled, OLM captures this information in an event in the Operator’s namespace: $ oc get events Example output LAST SEEN TYPE REASON OBJECT MESSAGE 85s Warning DisabledCopiedCSVs clusterserviceversion/my-csv.v1.0.0 CSV copying disabled for operators/my-csv.v1.0.0 When the spec.features.disableCopiedCSVs field is missing or set to false , OLM recreates the copied CSVs for all Operators installed with the AllNamespaces mode and deletes the previously mentioned events. Additional resources Install modes 4.5. Configuring proxy support in Operator Lifecycle Manager If a global proxy is configured on the OpenShift Container Platform cluster, Operator Lifecycle Manager (OLM) automatically configures Operators that it manages with the cluster-wide proxy. However, you can also configure installed Operators to override the global proxy or inject a custom CA certificate. Additional resources Configuring the cluster-wide proxy Configuring a custom PKI (custom CA certificate) Developing Operators that support proxy settings for Go , Ansible , and Helm 4.5.1. Overriding proxy settings of an Operator If a cluster-wide egress proxy is configured, Operators running with Operator Lifecycle Manager (OLM) inherit the cluster-wide proxy settings on their deployments. Cluster administrators can also override these proxy settings by configuring the subscription of an Operator. Important Operators must handle setting environment variables for proxy settings in the pods for any managed Operands. Prerequisites Access to an OpenShift Container Platform cluster using an account with cluster-admin permissions. Procedure Navigate in the web console to the Operators OperatorHub page. Select the Operator and click Install . On the Install Operator page, modify the Subscription object to include one or more of the following environment variables in the spec section: HTTP_PROXY HTTPS_PROXY NO_PROXY For example: Subscription object with proxy setting overrides apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: etcd-config-test namespace: openshift-operators spec: config: env: - name: HTTP_PROXY value: test_http - name: HTTPS_PROXY value: test_https - name: NO_PROXY value: test channel: clusterwide-alpha installPlanApproval: Automatic name: etcd source: community-operators sourceNamespace: openshift-marketplace startingCSV: etcdoperator.v0.9.4-clusterwide Note These environment variables can also be unset using an empty value to remove any previously set cluster-wide or custom proxy settings. OLM handles these environment variables as a unit; if at least one of them is set, all three are considered overridden and the cluster-wide defaults are not used for the deployments of the subscribed Operator. Click Install to make the Operator available to the selected namespaces. After the CSV for the Operator appears in the relevant namespace, you can verify that custom proxy environment variables are set in the deployment. For example, using the CLI: $ oc get deployment -n openshift-operators \\ etcd-operator -o yaml \\ | grep -i \"PROXY\" -A 2 Example output - name: HTTP_PROXY value: test_http - name: HTTPS_PROXY value: test_https - name: NO_PROXY value: test image: quay.io/coreos/etcd-operator@sha256:66a37fd61a06a43969854ee6d3e21088a98b93838e284a6086b13917f96b0d9c ... 4.5.2. Injecting a custom CA certificate When a cluster administrator adds a custom CA certificate to a cluster using a config map, the Cluster Network Operator merges the user-provided certificates and system CA certificates into a single bundle. You can inject this merged bundle into your Operator running on Operator Lifecycle Manager (OLM), which is useful if you have a man-in-the-middle HTTPS proxy. Prerequisites Access to an OpenShift Container Platform cluster using an account with cluster-admin permissions. Custom CA certificate added to the cluster using a config map. Desired Operator installed and running on OLM. Procedure Create an empty config map in the namespace where the subscription for your Operator exists and include the following label: apiVersion: v1 kind: ConfigMap metadata: name: trusted-ca 1 labels: config.openshift.io/inject-trusted-cabundle: \"true\" 2 1 Name of the config map. 2 Requests the Cluster Network Operator to inject the merged bundle. After creating this config map, it is immediately populated with the certificate contents of the merged bundle. Update your the Subscription object to include a spec.config section that mounts the trusted-ca config map as a volume to each container within a pod that requires a custom CA: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: my-operator spec: package: etcd channel: alpha config: 1 selector: matchLabels: <labels_for_pods> 2 volumes: 3 - name: trusted-ca configMap: name: trusted-ca items: - key: ca-bundle.crt 4 path: tls-ca-bundle.pem 5 volumeMounts: 6 - name: trusted-ca mountPath: /etc/pki/ca-trust/extracted/pem readOnly: true 1 Add a config section if it does not exist. 2 Specify labels to match pods that are owned by the Operator. 3 Create a trusted-ca volume. 4 ca-bundle.crt is required as the config map key. 5 tls-ca-bundle.pem is required as the config map path. 6 Create a trusted-ca volume mount. Note Deployments of an Operator can fail to validate the authority and display a x509 certificate signed by unknown authority error. This error can occur even after injecting a custom CA when using the subscription of an Operator. In this case, you can set the mountPath as /etc/ssl/certs for trusted-ca by using the subscription of an Operator. 4.6. Viewing Operator status Understanding the state of the system in Operator Lifecycle Manager (OLM) is important for making decisions about and debugging problems with installed Operators. OLM provides insight into subscriptions and related catalog sources regarding their state and actions performed. This helps users better understand the healthiness of their Operators. 4.6.1. Operator subscription condition types Subscriptions can report the following condition types: Table 4.1. Subscription condition types Condition Description CatalogSourcesUnhealthy Some or all of the catalog sources to be used in resolution are unhealthy. InstallPlanMissing An install plan for a subscription is missing. InstallPlanPending An install plan for a subscription is pending installation. InstallPlanFailed An install plan for a subscription has failed. ResolutionFailed The dependency resolution for a subscription has failed. Note Default OpenShift Container Platform cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a Subscription object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a Subscription object. Additional resources Refreshing failing subscriptions 4.6.2. Viewing Operator subscription status by using the CLI You can view Operator subscription status by using the CLI. Prerequisites You have access to the cluster as a user with the cluster-admin role. You have installed the OpenShift CLI ( oc ). Procedure List Operator subscriptions: $ oc get subs -n <operator_namespace> Use the oc describe command to inspect a Subscription resource: $ oc describe sub <subscription_name> -n <operator_namespace> In the command output, find the Conditions section for the status of Operator subscription condition types. In the following example, the CatalogSourcesUnhealthy condition type has a status of false because all available catalog sources are healthy: Example output Name: cluster-logging Namespace: openshift-logging Labels: operators.coreos.com/cluster-logging.openshift-logging= Annotations: <none> API Version: operators.coreos.com/v1alpha1 Kind: Subscription # ... Conditions: Last Transition Time: 2019-07-29T13:42:57Z Message: all available catalogsources are healthy Reason: AllCatalogSourcesHealthy Status: False Type: CatalogSourcesUnhealthy # ... Note Default OpenShift Container Platform cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a Subscription object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a Subscription object. 4.6.3. Viewing Operator catalog source status by using the CLI You can view the status of an Operator catalog source by using the CLI. Prerequisites You have access to the cluster as a user with the cluster-admin role. You have installed the OpenShift CLI ( oc ). Procedure List the catalog sources in a namespace. For example, you can check the openshift-marketplace namespace, which is used for cluster-wide catalog sources: $ oc get catalogsources -n openshift-marketplace Example output NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 55m community-operators Community Operators grpc Red Hat 55m example-catalog Example Catalog grpc Example Org 2m25s redhat-marketplace Red Hat Marketplace grpc Red Hat 55m redhat-operators Red Hat Operators grpc Red Hat 55m Use the oc describe command to get more details and status about a catalog source: $ oc describe catalogsource example-catalog -n openshift-marketplace Example output Name: example-catalog Namespace: openshift-marketplace Labels: <none> Annotations: operatorframework.io/managed-by: marketplace-operator target.workload.openshift.io/management: {\"effect\": \"PreferredDuringScheduling\"} API Version: operators.coreos.com/v1alpha1 Kind: CatalogSource # ... Status: Connection State: Address: example-catalog.openshift-marketplace.svc:50051 Last Connect: 2021-09-09T17:07:35Z Last Observed State: TRANSIENT_FAILURE Registry Service: Created At: 2021-09-09T17:05:45Z Port: 50051 Protocol: grpc Service Name: example-catalog Service Namespace: openshift-marketplace # ... In the preceding example output, the last observed state is TRANSIENT_FAILURE . This state indicates that there is a problem establishing a connection for the catalog source. List the pods in the namespace where your catalog source was created: $ oc get pods -n openshift-marketplace Example output NAME READY STATUS RESTARTS AGE certified-operators-cv9nn 1/1 Running 0 36m community-operators-6v8lp 1/1 Running 0 36m marketplace-operator-86bfc75f9b-jkgbc 1/1 Running 0 42m example-catalog-bwt8z 0/1 ImagePullBackOff 0 3m55s redhat-marketplace-57p8c 1/1 Running 0 36m redhat-operators-smxx8 1/1 Running 0 36m When a catalog source is created in a namespace, a pod for the catalog source is created in that namespace. In the preceding example output, the status for the example-catalog-bwt8z pod is ImagePullBackOff . This status indicates that there is an issue pulling the catalog source’s index image. Use the oc describe command to inspect a pod for more detailed information: $ oc describe pod example-catalog-bwt8z -n openshift-marketplace Example output Name: example-catalog-bwt8z Namespace: openshift-marketplace Priority: 0 Node: ci-ln-jyryyg2-f76d1-ggdbq-worker-b-vsxjd/10.0.128.2 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 48s default-scheduler Successfully assigned openshift-marketplace/example-catalog-bwt8z to ci-ln-jyryyf2-f76d1-fgdbq-worker-b-vsxjd Normal AddedInterface 47s multus Add eth0 [10.131.0.40/23] from openshift-sdn Normal BackOff 20s (x2 over 46s) kubelet Back-off pulling image \"quay.io/example-org/example-catalog:v1\" Warning Failed 20s (x2 over 46s) kubelet Error: ImagePullBackOff Normal Pulling 8s (x3 over 47s) kubelet Pulling image \"quay.io/example-org/example-catalog:v1\" Warning Failed 8s (x3 over 47s) kubelet Failed to pull image \"quay.io/example-org/example-catalog:v1\": rpc error: code = Unknown desc = reading manifest v1 in quay.io/example-org/example-catalog: unauthorized: access to the requested resource is not authorized Warning Failed 8s (x3 over 47s) kubelet Error: ErrImagePull In the preceding example output, the error messages indicate that the catalog source’s index image is failing to pull successfully because of an authorization issue. For example, the index image might be stored in a registry that requires login credentials. Additional resources Operator Lifecycle Manager concepts and resources Catalog source gRPC documentation: States of Connectivity Accessing images for Operators from private registries 4.7. Managing Operator conditions As a cluster administrator, you can manage Operator conditions by using Operator Lifecycle Manager (OLM). 4.7.1. Overriding Operator conditions As a cluster administrator, you might want to ignore a supported Operator condition reported by an Operator. When present, Operator conditions in the Spec.Overrides array override the conditions in the Spec.Conditions array, allowing cluster administrators to deal with situations where an Operator is incorrectly reporting a state to Operator Lifecycle Manager (OLM). Note By default, the Spec.Overrides array is not present in an OperatorCondition object until it is added by a cluster administrator. The Spec.Conditions array is also not present until it is either added by a user or as a result of custom Operator logic. For example, consider a known version of an Operator that always communicates that it is not upgradeable. In this instance, you might want to upgrade the Operator despite the Operator communicating that it is not upgradeable. This could be accomplished by overriding the Operator condition by adding the condition type and status to the Spec.Overrides array in the OperatorCondition object. Prerequisites An Operator with an OperatorCondition object, installed using OLM. Procedure Edit the OperatorCondition object for the Operator: $ oc edit operatorcondition <name> Add a Spec.Overrides array to the object: Example Operator condition override apiVersion: operators.coreos.com/v1 kind: OperatorCondition metadata: name: my-operator namespace: operators spec: overrides: - type: Upgradeable 1 status: \"True\" reason: \"upgradeIsSafe\" message: \"This is a known issue with the Operator where it always reports that it cannot be upgraded.\" conditions: - type: Upgradeable status: \"False\" reason: \"migration\" message: \"The operator is performing a migration.\" lastTransitionTime: \"2020-08-24T23:15:55Z\" 1 Allows the cluster administrator to change the upgrade readiness to True . 4.7.2. Updating your Operator to use Operator conditions Operator Lifecycle Manager (OLM) automatically creates an OperatorCondition resource for each ClusterServiceVersion resource that it reconciles. All service accounts in the CSV are granted the RBAC to interact with the OperatorCondition owned by the Operator. An Operator author can develop their Operator to use the operator-lib library such that, after the Operator has been deployed by OLM, it can set its own conditions. For more resources about setting Operator conditions as an Operator author, see the Enabling Operator conditions page. 4.7.2.1. Setting defaults In an effort to remain backwards compatible, OLM treats the absence of an OperatorCondition resource as opting out of the condition. Therefore, an Operator that opts in to using Operator conditions should set default conditions before the ready probe for the pod is set to true . This provides the Operator with a grace period to update the condition to the correct state. 4.7.3. Additional resources Operator conditions 4.8. Allowing non-cluster administrators to install Operators Cluster administrators can use Operator groups to allow regular users to install Operators. Additional resources Operator groups 4.8.1. Understanding Operator installation policy Operators can require wide privileges to run, and the required privileges can change between versions. Operator Lifecycle Manager (OLM) runs with cluster-admin privileges. By default, Operator authors can specify any set of permissions in the cluster service version (CSV), and OLM consequently grants it to the Operator. To ensure that an Operator cannot achieve cluster-scoped privileges and that users cannot escalate privileges using OLM, Cluster administrators can manually audit Operators before they are added to the cluster. Cluster administrators are also provided tools for determining and constraining which actions are allowed during an Operator installation or upgrade using service accounts. Cluster administrators can associate an Operator group with a service account that has a set of privileges granted to it. The service account sets policy on Operators to ensure they only run within predetermined boundaries by using role-based access control (RBAC) rules. As a result, the Operator is unable to do anything that is not explicitly permitted by those rules. By employing Operator groups, users with enough privileges can install Operators with a limited scope. As a result, more of the Operator Framework tools can safely be made available to more users, providing a richer experience for building applications with Operators. Note Role-based access control (RBAC) for Subscription objects is automatically granted to every user with the edit or admin role in a namespace. However, RBAC does not exist on OperatorGroup objects; this absence is what prevents regular users from installing Operators. Preinstalling Operator groups is effectively what gives installation privileges. Keep the following points in mind when associating an Operator group with a service account: The APIService and CustomResourceDefinition resources are always created by OLM using the cluster-admin role. A service account associated with an Operator group should never be granted privileges to write these resources. Any Operator tied to this Operator group is now confined to the permissions granted to the specified service account. If the Operator asks for permissions that are outside the scope of the service account, the install fails with appropriate errors so the cluster administrator can troubleshoot and resolve the issue. 4.8.1.1. Installation scenarios When determining whether an Operator can be installed or upgraded on a cluster, Operator Lifecycle Manager (OLM) considers the following scenarios: A cluster administrator creates a new Operator group and specifies a service account. All Operator(s) associated with this Operator group are installed and run against the privileges granted to the service account. A cluster administrator creates a new Operator group and does not specify any service account. OpenShift Container Platform maintains backward compatibility, so the default behavior remains and Operator installs and upgrades are permitted. For existing Operator groups that do not specify a service account, the default behavior remains and Operator installs and upgrades are permitted. A cluster administrator updates an existing Operator group and specifies a service account. OLM allows the existing Operator to continue to run with their current privileges. When such an existing Operator is going through an upgrade, it is reinstalled and run against the privileges granted to the service account like any new Operator. A service account specified by an Operator group changes by adding or removing permissions, or the existing service account is swapped with a new one. When existing Operators go through an upgrade, it is reinstalled and run against the privileges granted to the updated service account like any new Operator. A cluster administrator removes the service account from an Operator group. The default behavior remains and Operator installs and upgrades are permitted. 4.8.1.2. Installation workflow When an Operator group is tied to a service account and an Operator is installed or upgraded, Operator Lifecycle Manager (OLM) uses the following workflow: The given Subscription object is picked up by OLM. OLM fetches the Operator group tied to this subscription. OLM determines that the Operator group has a service account specified. OLM creates a client scoped to the service account and uses the scoped client to install the Operator. This ensures that any permission requested by the Operator is always confined to that of the service account in the Operator group. OLM creates a new service account with the set of permissions specified in the CSV and assigns it to the Operator. The Operator runs as the assigned service account. 4.8.2. Scoping Operator installations To provide scoping rules to Operator installations and upgrades on Operator Lifecycle Manager (OLM), associate a service account with an Operator group. Using this example, a cluster administrator can confine a set of Operators to a designated namespace. Procedure Create a new namespace: $ cat <<EOF | oc create -f - apiVersion: v1 kind: Namespace metadata: name: scoped EOF Allocate permissions that you want the Operator(s) to be confined to. This involves creating a new service account, relevant role(s), and role binding(s). $ cat <<EOF | oc create -f - apiVersion: v1 kind: ServiceAccount metadata: name: scoped namespace: scoped EOF The following example grants the service account permissions to do anything in the designated namespace for simplicity. In a production environment, you should create a more fine-grained set of permissions: $ cat <<EOF | oc create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: scoped namespace: scoped rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"*\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: scoped-bindings namespace: scoped roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: scoped subjects: - kind: ServiceAccount name: scoped namespace: scoped EOF Create an OperatorGroup object in the designated namespace. This Operator group targets the designated namespace to ensure that its tenancy is confined to it. In addition, Operator groups allow a user to specify a service account. Specify the service account created in the previous step: $ cat <<EOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: scoped namespace: scoped spec: serviceAccountName: scoped targetNamespaces: - scoped EOF Any Operator installed in the designated namespace is tied to this Operator group and therefore to the service account specified. Warning Operator Lifecycle Manager (OLM) creates the following cluster roles for each Operator group: <operatorgroup_name>-admin <operatorgroup_name>-edit <operatorgroup_name>-view When you manually create an Operator group, you must specify a unique name that does not conflict with the existing cluster roles or other Operator groups on the cluster. Create a Subscription object in the designated namespace to install an Operator: $ cat <<EOF | oc create -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: etcd namespace: scoped spec: channel: singlenamespace-alpha name: etcd source: <catalog_source_name> 1 sourceNamespace: <catalog_source_namespace> 2 EOF 1 Specify a catalog source that already exists in the designated namespace or one that is in the global catalog namespace. 2 Specify a namespace where the catalog source was created. Any Operator tied to this Operator group is confined to the permissions granted to the specified service account. If the Operator requests permissions that are outside the scope of the service account, the installation fails with relevant errors. 4.8.2.1. Fine-grained permissions Operator Lifecycle Manager (OLM) uses the service account specified in an Operator group to create or update the following resources related to the Operator being installed: ClusterServiceVersion Subscription Secret ServiceAccount Service ClusterRole and ClusterRoleBinding Role and RoleBinding To confine Operators to a designated namespace, cluster administrators can start by granting the following permissions to the service account: Note The following role is a generic example and additional rules might be required based on the specific Operator. kind: Role rules: - apiGroups: [\"operators.coreos.com\"] resources: [\"subscriptions\", \"clusterserviceversions\"] verbs: [\"get\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"serviceaccounts\"] verbs: [\"get\", \"create\", \"update\", \"patch\"] - apiGroups: [\"rbac.authorization.k8s.io\"] resources: [\"roles\", \"rolebindings\"] verbs: [\"get\", \"create\", \"update\", \"patch\"] - apiGroups: [\"apps\"] 1 resources: [\"deployments\"] verbs: [\"list\", \"watch\", \"get\", \"create\", \"update\", \"patch\", \"delete\"] - apiGroups: [\"\"] 2 resources: [\"pods\"] verbs: [\"list\", \"watch\", \"get\", \"create\", \"update\", \"patch\", \"delete\"] 1 2 Add permissions to create other resources, such as deployments and pods shown here. In addition, if any Operator specifies a pull secret, the following permissions must also be added: kind: ClusterRole 1 rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] --- kind: Role rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"update\", \"patch\"] 1 Required to get the secret from the OLM namespace. 4.8.3. Operator catalog access control When an Operator catalog is created in the global catalog namespace openshift-marketplace , the catalog’s Operators are made available cluster-wide to all namespaces. A catalog created in other namespaces only makes its Operators available in that same namespace of the catalog. On clusters where non-cluster administrator users have been delegated Operator installation privileges, cluster administrators might want to further control or restrict the set of Operators those users are allowed to install. This can be achieved with the following actions: Disable all of the default global catalogs. Enable custom, curated catalogs in the same namespace where the relevant Operator groups have been preinstalled. Additional resources Disabling the default OperatorHub sources Adding a catalog source to a cluster 4.8.4. Troubleshooting permission failures If an Operator installation fails due to lack of permissions, identify the errors using the following procedure. Procedure Review the Subscription object. Its status has an object reference installPlanRef that points to the InstallPlan object that attempted to create the necessary [Cluster]Role[Binding] object(s) for the Operator: apiVersion: operators.coreos.com/v1 kind: Subscription metadata: name: etcd namespace: scoped status: installPlanRef: apiVersion: operators.coreos.com/v1 kind: InstallPlan name: install-4plp8 namespace: scoped resourceVersion: \"117359\" uid: 2c1df80e-afea-11e9-bce3-5254009c9c23 Check the status of the InstallPlan object for any errors: apiVersion: operators.coreos.com/v1 kind: InstallPlan status: conditions: - lastTransitionTime: \"2019-07-26T21:13:10Z\" lastUpdateTime: \"2019-07-26T21:13:10Z\" message: 'error creating clusterrole etcdoperator.v0.9.4-clusterwide-dsfx4: clusterroles.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:scoped:scoped\" cannot create resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope' reason: InstallComponentFailed status: \"False\" type: Installed phase: Failed The error message tells you: The type of resource it failed to create, including the API group of the resource. In this case, it was clusterroles in the rbac.authorization.k8s.io group. The name of the resource. The type of error: is forbidden tells you that the user does not have enough permission to do the operation. The name of the user who attempted to create or update the resource. In this case, it refers to the service account specified in the Operator group. The scope of the operation: cluster scope or not. The user can add the missing permission to the service account and then iterate. Note Operator Lifecycle Manager (OLM) does not currently provide the complete list of errors on the first try. 4.9. Managing custom catalogs Cluster administrators and Operator catalog maintainers can create and manage custom catalogs packaged using the bundle format on Operator Lifecycle Manager (OLM) in OpenShift Container Platform. Important Kubernetes periodically deprecates certain APIs that are removed in subsequent releases. As a result, Operators are unable to use removed APIs starting with the version of OpenShift Container Platform that uses the Kubernetes version that removed the API. If your cluster is using custom catalogs, see Controlling Operator compatibility with OpenShift Container Platform versions for more details about how Operator authors can update their projects to help avoid workload issues and prevent incompatible upgrades. Additional resources Red Hat-provided Operator catalogs 4.9.1. Prerequisites Install the opm CLI . 4.9.2. File-based catalogs File-based catalogs are the latest iteration of the catalog format in Operator Lifecycle Manager (OLM). It is a plain text-based (JSON or YAML) and declarative config evolution of the earlier SQLite database format, and it is fully backwards compatible. Important As of , the default Red Hat-provided Operator catalog releases in the file-based catalog format. The default Red Hat-provided Operator catalogs for  through 4.10 released in the deprecated SQLite database format. The opm subcommands, flags, and functionality related to the SQLite database format are also deprecated and will be removed in a future release. The features are still supported and must be used for catalogs that use the deprecated SQLite database format. Many of the opm subcommands and flags for working with the SQLite database format, such as opm index prune , do not work with the file-based catalog format. For more information about working with file-based catalogs, see Operator Framework packaging format and Mirroring images for a disconnected installation using the oc-mirror plugin . 4.9.2.1. Creating a file-based catalog image You can use the opm CLI to create a catalog image that uses the plain text file-based catalog format (JSON or YAML), which replaces the deprecated SQLite database format. Prerequisites opm podman version 1.9.3+ A bundle image built and pushed to a registry that supports Docker v2-2 Procedure Initialize the catalog: Create a directory for the catalog by running the following command: $ mkdir <catalog_dir> Generate a Dockerfile that can build a catalog image by running the opm generate dockerfile command: $ opm generate dockerfile <catalog_dir> \\ -i registry.redhat.io/openshift4/ose-operator-registry:v4.11 1 1 Specify the official Red Hat base image by using the -i flag, otherwise the Dockerfile uses the default upstream image. The Dockerfile must be in the same parent directory as the catalog directory that you created in the previous step: Example directory structure . 1 ├── <catalog_dir> 2 └── <catalog_dir>.Dockerfile 3 1 Parent directory 2 Catalog directory 3 Dockerfile generated by the opm generate dockerfile command Populate the catalog with the package definition for your Operator by running the opm init command: $ opm init <operator_name> \\ 1 --default-channel=preview \\ 2 --description=./README.md \\ 3 --icon=./operator-icon.svg \\ 4 --output yaml \\ 5 > <catalog_dir>/index.yaml 6 1 Operator, or package, name 2 Channel that subscriptions default to if unspecified 3 Path to the Operator’s README.md or other documentation 4 Path to the Operator’s icon 5 Output format: JSON or YAML 6 Path for creating the catalog configuration file This command generates an olm.package declarative config blob in the specified catalog configuration file. Add a bundle to the catalog by running the opm render command: $ opm render <registry>/<namespace>/<bundle_image_name>:<tag> \\ 1 --output=yaml \\ >> <catalog_dir>/index.yaml 2 1 Pull spec for the bundle image 2 Path to the catalog configuration file Note Channels must contain at least one bundle. Add a channel entry for the bundle. For example, modify the following example to your specifications, and add it to your <catalog_dir>/index.yaml file: Example channel entry --- schema: olm.channel package: <operator_name> name: preview entries: - name: <operator_name>.v0.1.0 1 1 Ensure that you include the period ( . ) after <operator_name> but before the v in the version. Otherwise, the entry fails to pass the opm validate command. Validate the file-based catalog: Run the opm validate command against the catalog directory: $ opm validate <catalog_dir> Check that the error code is 0 : $ echo $? Example output 0 Build the catalog image by running the podman build command: $ podman build . \\ -f <catalog_dir>.Dockerfile \\ -t <registry>/<namespace>/<catalog_image_name>:<tag> Push the catalog image to a registry: If required, authenticate with your target registry by running the podman login command: $ podman login <registry> Push the catalog image by running the podman push command: $ podman push <registry>/<namespace>/<catalog_image_name>:<tag> Additional resources opm CLI reference 4.9.2.2. Updating or filtering a file-based catalog image You can use the opm CLI to update or filter (also known as prune) a catalog image that uses the file-based catalog format. By extracting and modifying the contents of an existing catalog image, you can update, add, or remove one or more Operator package entries from the catalog. You can then rebuild the image as an updated version of the catalog. Note Alternatively, if you already have a catalog image on a mirror registry, you can use the oc-mirror CLI plugin to automatically prune any removed images from an updated source version of that catalog image while mirroring it to the target registry. For more information about the oc-mirror plugin and this use case, see the \"Keeping your mirror registry content updated\" section, and specifically the \"Pruning images\" subsection, of \"Mirroring images for a disconnected installation using the oc-mirror plugin\". Prerequisites opm CLI. podman version 1.9.3+. A file-based catalog image. A catalog directory structure recently initialized on your workstation related to this catalog. If you do not have an initialized catalog directory, create the directory and generate the Dockerfile. For more information, see the \"Initialize the catalog\" step from the \"Creating a file-based catalog image\" procedure. Procedure Extract the contents of the catalog image in YAML format to an index.yaml file in your catalog directory: $ opm render <registry>/<namespace>/<catalog_image_name>:<tag> \\ -o yaml > <catalog_dir>/index.yaml Note Alternatively, you can use the -o json flag to output in JSON format. Modify the contents of the resulting index.yaml file to your specifications by updating, adding, or removing one or more Operator package entries. Important After a bundle has been published in a catalog, assume that one of your users has installed it. Ensure that all previously published bundles in a catalog have an update path to the current or newer channel head to avoid stranding users that have that version installed. For example, if you wanted to remove an Operator package, the following example lists a set of olm.package , olm.channel , and olm.bundle blobs which must be deleted to remove the package from the catalog: Example 4.1. Example removed entries --- defaultChannel: release-2.7 icon: base64data: <base64_string> mediatype: image/svg+xml name: example-operator schema: olm.package --- entries: - name: example-operator.v2.7.0 skipRange: '>=2.6.0 <2.7.0' - name: example-operator.v2.7.1 replaces: example-operator.v2.7.0 skipRange: '>=2.6.0 <2.7.1' - name: example-operator.v2.7.2 replaces: example-operator.v2.7.1 skipRange: '>=2.6.0 <2.7.2' - name: example-operator.v2.7.3 replaces: example-operator.v2.7.2 skipRange: '>=2.6.0 <2.7.3' - name: example-operator.v2.7.4 replaces: example-operator.v2.7.3 skipRange: '>=2.6.0 <2.7.4' name: release-2.7 package: example-operator schema: olm.channel --- image: example.com/example-inc/example-operator-bundle@sha256:<digest> name: example-operator.v2.7.0 package: example-operator properties: - type: olm.gvk value: group: example-group.example.io kind: MyObject version: v1alpha1 - type: olm.gvk value: group: example-group.example.io kind: MyOtherObject version: v1beta1 - type: olm.package value: packageName: example-operator version: 2.7.0 - type: olm.bundle.object value: data: <base64_string> - type: olm.bundle.object value: data: <base64_string> relatedImages: - image: example.com/example-inc/example-related-image@sha256:<digest> name: example-related-image schema: olm.bundle --- Save your changes to the index.yaml file. Validate the catalog: $ opm validate <catalog_dir> Rebuild the catalog: $ podman build . \\ -f <catalog_dir>.Dockerfile \\ -t <registry>/<namespace>/<catalog_image_name>:<tag> Push the updated catalog image to a registry: $ podman push <registry>/<namespace>/<catalog_image_name>:<tag> Verification In the web console, navigate to the OperatorHub configuration resource in the Administration Cluster Settings Configuration page. Add the catalog source or update the existing catalog source to use the pull spec for your updated catalog image. For more information, see \"Adding a catalog source to a cluster\" in the \"Additional resources\" of this section. After the catalog source is in a READY state, navigate to the Operators OperatorHub page and check that the changes you made are reflected in the list of Operators. Additional resources Mirroring images for a disconnected installation using the oc-mirror plugin Keeping your mirror registry content updated Adding a catalog source to a cluster 4.9.3. SQLite-based catalogs Important The SQLite database format for Operator catalogs is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the Deprecated and removed features section of the OpenShift Container Platform release notes. 4.9.3.1. Creating a SQLite-based index image You can create an index image based on the SQLite database format by using the opm CLI. Prerequisites opm podman version 1.9.3+ A bundle image built and pushed to a registry that supports Docker v2-2 Procedure Start a new index: $ opm index add \\ --bundles <registry>/<namespace>/<bundle_image_name>:<tag> \\ 1 --tag <registry>/<namespace>/<index_image_name>:<tag> \\ 2 [--binary-image <registry_base_image>] 3 1 Comma-separated list of bundle images to add to the index. 2 The image tag that you want the index image to have. 3 Optional: An alternative registry base image to use for serving the catalog. Push the index image to a registry. If required, authenticate with your target registry: $ podman login <registry> Push the index image: $ podman push <registry>/<namespace>/<index_image_name>:<tag> 4.9.3.2. Updating a SQLite-based index image After configuring OperatorHub to use a catalog source that references a custom index image, cluster administrators can keep the available Operators on their cluster up to date by adding bundle images to the index image. You can update an existing index image using the opm index add command. Prerequisites opm podman version 1.9.3+ An index image built and pushed to a registry. An existing catalog source referencing the index image. Procedure Update the existing index by adding bundle images: $ opm index add \\ --bundles <registry>/<namespace>/<new_bundle_image>@sha256:<digest> \\ 1 --from-index <registry>/<namespace>/<existing_index_image>:<existing_tag> \\ 2 --tag <registry>/<namespace>/<existing_index_image>:<updated_tag> \\ 3 --pull-tool podman 4 1 The --bundles flag specifies a comma-separated list of additional bundle images to add to the index. 2 The --from-index flag specifies the previously pushed index. 3 The --tag flag specifies the image tag to apply to the updated index image. 4 The --pull-tool flag specifies the tool used to pull container images. where: <registry> Specifies the hostname of the registry, such as quay.io or mirror.example.com . <namespace> Specifies the namespace of the registry, such as ocs-dev or abc . <new_bundle_image> Specifies the new bundle image to add to the registry, such as ocs-operator . <digest> Specifies the SHA image ID, or digest, of the bundle image, such as c7f11097a628f092d8bad148406aa0e0951094a03445fd4bc0775431ef683a41 . <existing_index_image> Specifies the previously pushed image, such as abc-redhat-operator-index . <existing_tag> Specifies a previously pushed image tag, such as 4.11 . <updated_tag> Specifies the image tag to apply to the updated index image, such as 4.11.1 . Example command $ opm index add \\ --bundles quay.io/ocs-dev/ocs-operator@sha256:c7f11097a628f092d8bad148406aa0e0951094a03445fd4bc0775431ef683a41 \\ --from-index mirror.example.com/abc/abc-redhat-operator-index:4.11 \\ --tag mirror.example.com/abc/abc-redhat-operator-index:4.11.1 \\ --pull-tool podman Push the updated index image: $ podman push <registry>/<namespace>/<existing_index_image>:<updated_tag> After Operator Lifecycle Manager (OLM) automatically polls the index image referenced in the catalog source at its regular interval, verify that the new packages are successfully added: $ oc get packagemanifests -n openshift-marketplace 4.9.3.3. Filtering a SQLite-based index image An index image, based on the Operator bundle format, is a containerized snapshot of an Operator catalog. You can filter, or prune , an index of all but a specified list of packages, which creates a copy of the source index containing only the Operators that you want. Prerequisites podman version 1.9.3+ grpcurl (third-party command-line tool) opm Access to a registry that supports Docker v2-2 Procedure Authenticate with your target registry: $ podman login <target_registry> Determine the list of packages you want to include in your pruned index. Run the source index image that you want to prune in a container. For example: $ podman run -p50051:50051 \\ -it registry.redhat.io/redhat/redhat-operator-index:v4.11 Example output Trying to pull registry.redhat.io/redhat/redhat-operator-index:v4.11... Getting image source signatures Copying blob ae8a0c23f5b1 done ... INFO[0000] serving registry database=/database/index.db port=50051 In a separate terminal session, use the grpcurl command to get a list of the packages provided by the index: $ grpcurl -plaintext localhost:50051 api.Registry/ListPackages > packages.out Inspect the packages.out file and identify which package names from this list you want to keep in your pruned index. For example: Example snippets of packages list ... { \"name\": \"advanced-cluster-management\" } ... { \"name\": \"jaeger-product\" } ... { { \"name\": \"quay-operator\" } ... In the terminal session where you executed the podman run command, press Ctrl and C to stop the container process. Run the following command to prune the source index of all but the specified packages: $ opm index prune \\ -f registry.redhat.io/redhat/redhat-operator-index:v4.11 \\ 1 -p advanced-cluster-management,jaeger-product,quay-operator \\ 2 [-i registry.redhat.io/openshift4/ose-operator-registry:v4.9] \\ 3 -t <target_registry>:<port>/<namespace>/redhat-operator-index:v4.11 4 1 Index to prune. 2 Comma-separated list of packages to keep. 3 Required only for IBM Power and IBM Z images: Operator Registry base image with the tag that matches the target OpenShift Container Platform cluster major and minor version. 4 Custom tag for new index image being built. Run the following command to push the new index image to your target registry: $ podman push <target_registry>:<port>/<namespace>/redhat-operator-index:v4.11 where <namespace> is any existing namespace on the registry. 4.9.4. Adding a catalog source to a cluster Adding a catalog source to an OpenShift Container Platform cluster enables the discovery and installation of Operators for users. Cluster administrators can create a CatalogSource object that references an index image. OperatorHub uses catalog sources to populate the user interface. Tip Alternatively, you can use the web console to manage catalog sources. From the Administration Cluster Settings Configuration OperatorHub page, click the Sources tab, where you can create, update, delete, disable, and enable individual sources. Prerequisites An index image built and pushed to a registry. Procedure Create a CatalogSource object that references your index image. Modify the following to your specifications and save it as a catalogSource.yaml file: apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: my-operator-catalog namespace: openshift-marketplace 1 annotations: olm.catalogImageTemplate: 2 \"<registry>/<namespace>/<index_image_name>:v{kube_major_version}.{kube_minor_version}.{kube_patch_version}\" spec: sourceType: grpc image: <registry>/<namespace>/<index_image_name>:<tag> 3 displayName: My Operator Catalog publisher: <publisher_name> 4 updateStrategy: registryPoll: 5 interval: 30m 1 If you want the catalog source to be available globally to users in all namespaces, specify the openshift-marketplace namespace. Otherwise, you can specify a different namespace for the catalog to be scoped and available only for that namespace. 2 Optional: Set the olm.catalogImageTemplate annotation to your index image name and use one or more of the Kubernetes cluster version variables as shown when constructing the template for the image tag. 3 Specify your index image. If you specify a tag after the image name, for example :v4.11 , the catalog source pod uses an image pull policy of Always , meaning the pod always pulls the image prior to starting the container. If you specify a digest, for example @sha256:<id> , the image pull policy is IfNotPresent , meaning the pod pulls the image only if it does not already exist on the node. 4 Specify your name or an organization name publishing the catalog. 5 Catalog sources can automatically check for new versions to keep up to date. Use the file to create the CatalogSource object: $ oc apply -f catalogSource.yaml Verify the following resources are created successfully. Check the pods: $ oc get pods -n openshift-marketplace Example output NAME READY STATUS RESTARTS AGE my-operator-catalog-6njx6 1/1 Running 0 28s marketplace-operator-d9f549946-96sgr 1/1 Running 0 26h Check the catalog source: $ oc get catalogsource -n openshift-marketplace Example output NAME DISPLAY TYPE PUBLISHER AGE my-operator-catalog My Operator Catalog grpc 5s Check the package manifest: $ oc get packagemanifest -n openshift-marketplace Example output NAME CATALOG AGE jaeger-product My Operator Catalog 93s You can now install the Operators from the OperatorHub page on your OpenShift Container Platform web console. Additional resources Operator Lifecycle Manager concepts and resources Catalog source Accessing images for Operators from private registries Image pull policy 4.9.5. Accessing images for Operators from private registries If certain images relevant to Operators managed by Operator Lifecycle Manager (OLM) are hosted in an authenticated container image registry, also known as a private registry, OLM and OperatorHub are unable to pull the images by default. To enable access, you can create a pull secret that contains the authentication credentials for the registry. By referencing one or more pull secrets in a catalog source, OLM can handle placing the secrets in the Operator and catalog namespace to allow installation. Other images required by an Operator or its Operands might require access to private registries as well. OLM does not handle placing the secrets in target tenant namespaces for this scenario, but authentication credentials can be added to the global cluster pull secret or individual namespace service accounts to enable the required access. The following types of images should be considered when determining whether Operators managed by OLM have appropriate pull access: Index images A CatalogSource object can reference an index image, which use the Operator bundle format and are catalog sources packaged as container images hosted in images registries. If an index image is hosted in a private registry, a secret can be used to enable pull access. Bundle images Operator bundle images are metadata and manifests packaged as container images that represent a unique version of an Operator. If any bundle images referenced in a catalog source are hosted in one or more private registries, a secret can be used to enable pull access. Operator and Operand images If an Operator installed from a catalog source uses a private image, either for the Operator image itself or one of the Operand images it watches, the Operator will fail to install because the deployment will not have access to the required registry authentication. Referencing secrets in a catalog source does not enable OLM to place the secrets in target tenant namespaces in which Operands are installed. Instead, the authentication details can be added to the global cluster pull secret in the openshift-config namespace, which provides access to all namespaces on the cluster. Alternatively, if providing access to the entire cluster is not permissible, the pull secret can be added to the default service accounts of the target tenant namespaces. Prerequisites At least one of the following hosted in a private registry: An index image or catalog image. An Operator bundle image. An Operator or Operand image. Procedure Create a secret for each required private registry. Log in to the private registry to create or update your registry credentials file: $ podman login <registry>:<port> Note The file path of your registry credentials can be different depending on the container tool used to log in to the registry. For the podman CLI, the default location is ${XDG_RUNTIME_DIR}/containers/auth.json . For the docker CLI, the default location is /root/.docker/config.json . It is recommended to include credentials for only one registry per secret, and manage credentials for multiple registries in separate secrets. Multiple secrets can be included in a CatalogSource object in later steps, and OpenShift Container Platform will merge the secrets into a single virtual credentials file for use during an image pull. A registry credentials file can, by default, store details for more than one registry or for multiple repositories in one registry. Verify the current contents of your file. For example: File storing credentials for multiple registries { \"auths\": { \"registry.redhat.io\": { \"auth\": \"FrNHNydQXdzclNqdg==\" }, \"quay.io\": { \"auth\": \"fegdsRib21iMQ==\" }, \"https://quay.io/my-namespace/my-user/my-image\": { \"auth\": \"eWfjwsDdfsa221==\" }, \"https://quay.io/my-namespace/my-user\": { \"auth\": \"feFweDdscw34rR==\" }, \"https://quay.io/my-namespace\": { \"auth\": \"frwEews4fescyq==\" } } } Because this file is used to create secrets in later steps, ensure that you are storing details for only one registry per file. This can be accomplished by using either of the following methods: Use the podman logout <registry> command to remove credentials for additional registries until only the one registry you want remains. Edit your registry credentials file and separate the registry details to be stored in multiple files. For example: File storing credentials for one registry { \"auths\": { \"registry.redhat.io\": { \"auth\": \"FrNHNydQXdzclNqdg==\" } } } File storing credentials for another registry { \"auths\": { \"quay.io\": { \"auth\": \"Xd2lhdsbnRib21iMQ==\" } } } Create a secret in the openshift-marketplace namespace that contains the authentication credentials for a private registry: $ oc create secret generic <secret_name> \\ -n openshift-marketplace \\ --from-file=.dockerconfigjson=<path/to/registry/credentials> \\ --type=kubernetes.io/dockerconfigjson Repeat this step to create additional secrets for any other required private registries, updating the --from-file flag to specify another registry credentials file path. Create or update an existing CatalogSource object to reference one or more secrets: apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: my-operator-catalog namespace: openshift-marketplace spec: sourceType: grpc secrets: 1 - \"<secret_name_1>\" - \"<secret_name_2>\" image: <registry>:<port>/<namespace>/<image>:<tag> displayName: My Operator Catalog publisher: <publisher_name> updateStrategy: registryPoll: interval: 30m 1 Add a spec.secrets section and specify any required secrets. If any Operator or Operand images that are referenced by a subscribed Operator require access to a private registry, you can either provide access to all namespaces in the cluster, or individual target tenant namespaces. To provide access to all namespaces in the cluster, add authentication details to the global cluster pull secret in the openshift-config namespace. Warning Cluster resources must adjust to the new global pull secret, which can temporarily limit the usability of the cluster. Extract the .dockerconfigjson file from the global pull secret: $ oc extract secret/pull-secret -n openshift-config --confirm Update the .dockerconfigjson file with your authentication credentials for the required private registry or registries and save it as a new file: $ cat .dockerconfigjson | \\ jq --compact-output '.auths[\"<registry>:<port>/<namespace>/\"] |= . + {\"auth\":\"<token>\"}' \\ 1 > new_dockerconfigjson 1 Replace <registry>:<port>/<namespace> with the private registry details and <token> with your authentication credentials. Update the global pull secret with the new file: $ oc set data secret/pull-secret -n openshift-config \\ --from-file=.dockerconfigjson=new_dockerconfigjson To update an individual namespace, add a pull secret to the service account for the Operator that requires access in the target tenant namespace. Recreate the secret that you created for the openshift-marketplace in the tenant namespace: $ oc create secret generic <secret_name> \\ -n <tenant_namespace> \\ --from-file=.dockerconfigjson=<path/to/registry/credentials> \\ --type=kubernetes.io/dockerconfigjson Verify the name of the service account for the Operator by searching the tenant namespace: $ oc get sa -n <tenant_namespace> 1 1 If the Operator was installed in an individual namespace, search that namespace. If the Operator was installed for all namespaces, search the openshift-operators namespace. Example output NAME SECRETS AGE builder 2 6m1s default 2 6m1s deployer 2 6m1s etcd-operator 2 5m18s 1 1 Service account for an installed etcd Operator. Link the secret to the service account for the Operator: $ oc secrets link <operator_sa> \\ -n <tenant_namespace> \\ <secret_name> \\ --for=pull Additional resources See What is a secret? for more information on the types of secrets, including those used for registry credentials. See Updating the global cluster pull secret for more details on the impact of changing this secret. See Allowing pods to reference images from other secured registries for more details on linking pull secrets to service accounts per namespace. 4.9.6. Disabling the default OperatorHub sources Operator catalogs that source content provided by Red Hat and community projects are configured for OperatorHub by default during an OpenShift Container Platform installation. As a cluster administrator, you can disable the set of default catalogs. Procedure Disable the sources for the default catalogs by adding disableAllDefaultSources: true to the OperatorHub object: $ oc patch OperatorHub cluster --type json \\ -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]' Tip Alternatively, you can use the web console to manage catalog sources. From the Administration Cluster Settings Configuration OperatorHub page, click the Sources tab, where you can create, update, delete, disable, and enable individual sources. 4.9.7. Removing custom catalogs As a cluster administrator, you can remove custom Operator catalogs that have been previously added to your cluster by deleting the related catalog source. Procedure In the Administrator perspective of the web console, navigate to Administration Cluster Settings . Click the Configuration tab, and then click OperatorHub . Click the Sources tab. Select the Options menu for the catalog that you want to remove, and then click Delete CatalogSource . 4.10. Using Operator Lifecycle Manager on restricted networks For OpenShift Container Platform clusters that are installed on restricted networks, also known as disconnected clusters , Operator Lifecycle Manager (OLM) by default cannot access the Red Hat-provided OperatorHub sources hosted on remote registries because those remote sources require full internet connectivity. However, as a cluster administrator you can still enable your cluster to use OLM in a restricted network if you have a workstation that has full internet access. The workstation, which requires full internet access to pull the remote OperatorHub content, is used to prepare local mirrors of the remote sources, and push the content to a mirror registry. The mirror registry can be located on a bastion host, which requires connectivity to both your workstation and the disconnected cluster, or a completely disconnected, or airgapped , host, which requires removable media to physically move the mirrored content to the disconnected environment. This guide describes the following process that is required to enable OLM in restricted networks: Disable the default remote OperatorHub sources for OLM. Use a workstation with full internet access to create and push local mirrors of the OperatorHub content to a mirror registry. Configure OLM to install and manage Operators from local sources on the mirror registry instead of the default remote sources. After enabling OLM in a restricted network, you can continue to use your unrestricted workstation to keep your local OperatorHub sources updated as newer versions of Operators are released. Important While OLM can manage Operators from local sources, the ability for a given Operator to run successfully in a restricted network still depends on the Operator itself meeting the following criteria: List any related images, or other container images that the Operator might require to perform their functions, in the relatedImages parameter of its ClusterServiceVersion (CSV) object. Reference all specified images by a digest (SHA) and not by a tag. You can search software on the Red Hat Ecosystem Catalog for a list of Red Hat Operators that support running in disconnected mode by filtering with the following selections: Type Containerized application Deployment method Operator Infrastructure features Disconnected Additional resources Red Hat-provided Operator catalogs Enabling your Operator for restricted network environments 4.10.1. Prerequisites Log in to your OpenShift Container Platform cluster as a user with cluster-admin privileges. Note If you are using OLM in a restricted network on IBM Z, you must have at least 12 GB allocated to the directory where you place your registry. 4.10.2. Disabling the default OperatorHub sources Operator catalogs that source content provided by Red Hat and community projects are configured for OperatorHub by default during an OpenShift Container Platform installation. In a restricted network environment, you must disable the default catalogs as a cluster administrator. You can then configure OperatorHub to use local catalog sources. Procedure Disable the sources for the default catalogs by adding disableAllDefaultSources: true to the OperatorHub object: $ oc patch OperatorHub cluster --type json \\ -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]' Tip Alternatively, you can use the web console to manage catalog sources. From the Administration Cluster Settings Configuration OperatorHub page, click the Sources tab, where you can create, update, delete, disable, and enable individual sources. 4.10.3. Mirroring an Operator catalog For instructions about mirroring Operator catalogs for use with disconnected clusters, see Installing Mirroring images for a disconnected installation . Important As of , the default Red Hat-provided Operator catalog releases in the file-based catalog format. The default Red Hat-provided Operator catalogs for  through 4.10 released in the deprecated SQLite database format. The opm subcommands, flags, and functionality related to the SQLite database format are also deprecated and will be removed in a future release. The features are still supported and must be used for catalogs that use the deprecated SQLite database format. Many of the opm subcommands and flags for working with the SQLite database format, such as opm index prune , do not work with the file-based catalog format. For more information about working with file-based catalogs, see Operator Framework packaging format , Managing custom catalogs , and Mirroring images for a disconnected installation using the oc-mirror plugin . 4.10.4. Adding a catalog source to a cluster Adding a catalog source to an OpenShift Container Platform cluster enables the discovery and installation of Operators for users. Cluster administrators can create a CatalogSource object that references an index image. OperatorHub uses catalog sources to populate the user interface. Tip Alternatively, you can use the web console to manage catalog sources. From the Administration Cluster Settings Configuration OperatorHub page, click the Sources tab, where you can create, update, delete, disable, and enable individual sources. Prerequisites An index image built and pushed to a registry. Procedure Create a CatalogSource object that references your index image. If you used the oc adm catalog mirror command to mirror your catalog to a target registry, you can use the generated catalogSource.yaml file in your manifests directory as a starting point. Modify the following to your specifications and save it as a catalogSource.yaml file: apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: my-operator-catalog 1 namespace: openshift-marketplace 2 spec: sourceType: grpc image: <registry>/<namespace>/redhat-operator-index:v4.11 3 displayName: My Operator Catalog publisher: <publisher_name> 4 updateStrategy: registryPoll: 5 interval: 30m 1 If you mirrored content to local files before uploading to a registry, remove any backslash ( / ) characters from the metadata.name field to avoid an \"invalid resource name\" error when you create the object. 2 If you want the catalog source to be available globally to users in all namespaces, specify the openshift-marketplace namespace. Otherwise, you can specify a different namespace for the catalog to be scoped and available only for that namespace. 3 Specify your index image. If you specify a tag after the image name, for example :v4.11 , the catalog source pod uses an image pull policy of Always , meaning the pod always pulls the image prior to starting the container. If you specify a digest, for example @sha256:<id> , the image pull policy is IfNotPresent , meaning the pod pulls the image only if it does not already exist on the node. 4 Specify your name or an organization name publishing the catalog. 5 Catalog sources can automatically check for new versions to keep up to date. Use the file to create the CatalogSource object: $ oc apply -f catalogSource.yaml Verify the following resources are created successfully. Check the pods: $ oc get pods -n openshift-marketplace Example output NAME READY STATUS RESTARTS AGE my-operator-catalog-6njx6 1/1 Running 0 28s marketplace-operator-d9f549946-96sgr 1/1 Running 0 26h Check the catalog source: $ oc get catalogsource -n openshift-marketplace Example output NAME DISPLAY TYPE PUBLISHER AGE my-operator-catalog My Operator Catalog grpc 5s Check the package manifest: $ oc get packagemanifest -n openshift-marketplace Example output NAME CATALOG AGE jaeger-product My Operator Catalog 93s You can now install the Operators from the OperatorHub page on your OpenShift Container Platform web console. Additional resources Accessing images for Operators from private registries Image template for custom catalog sources Image pull policy 4.11. Catalog source pod scheduling When an Operator Lifecycle Manager (OLM) catalog source of source type grpc defines a spec.image , the Catalog Operator creates a pod that serves the defined image content. By default, this pod defines the following in its spec: Only the kubernetes.io/os=linux node selector No priority class name No tolerations As an administrator, you can override these values by modifying fields in the CatalogSource object’s optional spec.grpcPodConfig section. Additional resources OLM concepts and resources Catalog source 4.11.1. Overriding the node selector for catalog source pods Prequisites CatalogSource object of source type grpc with spec.image defined Procedure Edit the CatalogSource object and add or modify the spec.grpcPodConfig section to include the following: grpcPodConfig: nodeSelector: custom_label: <label> where <label> is the label for the node selector that you want catalog source pods to use for scheduling. Additional resources Placing pods on specific nodes using node selectors 4.11.2. Overriding the priority class name for catalog source pods Prequisites CatalogSource object of source type grpc with spec.image defined Procedure Edit the CatalogSource object and add or modify the spec.grpcPodConfig section to include the following: grpcPodConfig: priorityClassName: <priority_class> where <priority_class> is one of the following: One of the default priority classes provided by Kubernetes: system-cluster-critical or system-node-critical An empty set ( \"\" ) to assign the default priority A pre-existing and custom defined priority class Note Previously, the only pod scheduling parameter that could be overriden was priorityClassName . This was done by adding the operatorframework.io/priorityclass annotation to the CatalogSource object. For example: apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: example-catalog namespace: openshift-marketplace annotations: operatorframework.io/priorityclass: system-cluster-critical If a CatalogSource object defines both the annotation and spec.grpcPodConfig.priorityClassName , the annotation takes precedence over the configuration parameter. Additional resources Pod priority classes 4.11.3. Overriding tolerations for catalog source pods Prequisites CatalogSource object of source type grpc with spec.image defined Procedure Edit the CatalogSource object and add or modify the spec.grpcPodConfig section to include the following: grpcPodConfig: tolerations: - key: \"<key_name>\" operator: \"<operator_type>\" value: \"<value>\" effect: \"<effect>\" Additional resources Understanding taints and tolerations Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/operators/administrator-tasks"}
{"title": "Providing feedback on Red Hat documentation", "content": "Providing feedback on Red Hat documentation If you have a suggestion to improve this documentation, or find an error, you can contact technical support at https://access.redhat.com to open a request. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/providing-feedback"}
{"title": "Visualizing your costs using cost explorer", "content": "Visualizing your costs using cost explorer Cost Management Service 1-latest Use Cost Explorer to visualize and understand your costs Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html-single/visualizing_your_costs_using_cost_explorer/index"}
{"title": "Appendix A. Inventory file variables", "content": "Appendix A. Inventory file variables The following tables contain information about the variables used in Ansible Automation Platform’s installation inventory files. The tables include the variables that you can use for RPM-based installation and container-based installation. A.1. Ansible variables The following variables control how Ansible Automation Platform interacts with remote hosts. For more information about variables specific to certain plugins, see the documentation for Ansible.Builtin . For a list of global configuration options, see Ansible Configuration Settings . Variable Description ansible_connection The connection plugin used for the task on the target host. This can be the name of any of Ansible connection plugins. SSH protocol types are smart , ssh , or paramiko . Default = smart ansible_host The IP address or name of the target host to use instead of inventory_hostname . ansible_password The password to authenticate to the host. Do not store this variable in plain text. Always use a vault. For more information, see Keep vaulted variables safely visible . ansible_port The connection port number. The default for SSH is 22 . ansible_scp_extra_args This setting is always appended to the default scp command line. ansible_sftp_extra_args This setting is always appended to the default sftp command line. ansible_shell_executable This sets the shell that the Ansible controller uses on the target machine and overrides the executable in ansible.cfg which defaults to /bin/sh . ansible_shell_type The shell type of the target system. Do not use this setting unless you have set the ansible_shell_executable to a non-Bourne (sh) compatible shell. By default commands are formatted using sh-style syntax. Setting this to csh or fish causes commands executed on target systems to follow the syntax of those shells instead. ansible_ssh_common_args This setting is always appended to the default command line for sftp , scp , and ssh . Useful to configure a ProxyCommand for a certain host or group. ansible_ssh_executable This setting overrides the default behavior to use the system ssh . This can override the ssh_executable setting in ansible.cfg . ansible_ssh_extra_args This setting is always appended to the default ssh command line. ansible_ssh_pipelining Determines if SSH pipelining is used. This can override the pipelining setting in ansible.cfg . If using SSH key-based authentication, the key must be managed by an SSH agent. ansible_ssh_private_key_file Private key file used by SSH. Useful if using multiple keys and you do not want to use an SSH agent. ansible_user The user name to use when connecting to the host. Do not change this variable unless /bin/sh is not installed on the target machine or cannot be run from sudo. inventory_hostname This variable takes the hostname of the machine from the inventory script or the Ansible configuration file. You cannot set the value of this variable. Because the value is taken from the configuration file, the actual runtime hostname value can vary from what is returned by this variable. A.2. Automation hub variables RPM variable name Container variable name Description Required or optional Default automationhub_admin_password hub_admin_password Automation hub administrator password. Use of special characters for this variable is limited. The password can include any printable ASCII character except / , ” , or @ . Required automationhub_api_token Set the existing token for the installation program. For example, a regenerated token in the automation hub UI will invalidate an existing token. Use this variable to set that token in the installation program the next time you run the installation program. Optional automationhub_auto_sign_collections hub_collection_auto_sign If a collection signing service is enabled, collections are not signed automatically by default. Set this variable to true to sign collections by default. Optional false automationhub_backup_collections Ansible automation hub provides artifacts in /var/lib/pulp . These artifacts are automatically backed up by default. Set this variable to false to prevent backup or restore of /var/lib/pulp . Optional true automationhub_client_max_body_size hub_nginx_client_max_body_size Maximum allowed size for data sent to automation hub through NGINX. Optional 20m automationhub_collection_download_count Denote whether or not the collection download count should be displayed in the UI. Optional false automationhub_collection_seed_repository Controls the type of content to upload when hub_seed_collections is set to true . Valid options include: certified , validated Optional Both certified and validated are enabled by default. automationhub_collection_signing_service_key hub_collection_signing_key Path to the collection signing key file. Required if a collection signing service is enabled. automationhub_container_repair_media_type Denote whether or not to run the command pulpcore-manager container-repair-media-type . Valid options include: true , false , auto Optional auto automationhub_container_signing_service_key hub_container_signing_key Path to the container signing key file. Required if a container signing service is enabled. automationhub_create_default_collection_signing_service hub_collection_signing Set this variable to true to enable a collection signing service. Optional false automationhub_create_default_container_signing_service hub_container_signing Set this variable to true to enable a container signing service. Optional false automationhub_disable_hsts hub_nginx_disable_hsts Controls whether HTTP Strict Transport Security (HSTS) is enabled or disabled for automation hub. Set this variable to true to disable HSTS. Optional false automationhub_disable_https hub_nginx_disable_https Controls whether HTTPS is enabled or disabled for automation hub. Set this variable to true to disable HTTPS. Optional false automationhub_enable_api_access_log Controls whether logging is enabled or disabled at /var/log/galaxy_api_access.log . The file logs all user actions made to the platform, including username and IP address. Set this variable to true to enable this logging. Optional false automationhub_enable_unauthenticated_collection_access Controls whether read-only access is enabled or disabled for unauthorized users viewing collections or namespaces for automation hub. Set this variable to true to enable read-only access. Optional false automationhub_enable_unauthenticated_collection_download Controls whether or not unauthorized users can download read-only collections from automation hub. Set this variable to true to enable download of read-only collections. Optional false automationhub_force_change_admin_password Denote whether or not to require the change of the default administrator password for automation hub during installation. Set to true to require the user to change the default administrator password during installation. Optional false automationhub_importer_settings hub_galaxy_importer Dictionary of settings to pass to the galaxy-importer.cfg configuration file. These settings control how the galaxy-importer service processes and validates Ansible content. Example values include: ansible-doc , ansible-lint , and flake8 . Optional automationhub_nginx_tls_files_remote Denote whether the web certificate sources are local to the installation program ( false ) or on the remote component server ( true ). Optional The value defined in automationhub_tls_files_remote . automationhub_pg_cert_auth hub_pg_cert_auth Controls whether client certificate authentication is enabled or disabled on the automation hub PostgreSQL database. Set this variable to true to enable client certificate authentication. Optional false automationhub_pg_database hub_pg_database Name of the PostgreSQL database used by automation hub. Optional RPM = automationhub Container = pulp automationhub_pg_host hub_pg_host Hostname of the PostgreSQL database used by automation hub. Required RPM = 127.0.0.1 Container = automationhub_pg_password hub_pg_password Password for the automation hub PostgreSQL database user. Use of special characters for this variable is limited. The ! , # , 0 and @ characters are supported. Use of other special characters can cause the setup to fail. Optional automationhub_pg_port hub_pg_port Port number for the PostgreSQL database used by automation hub. Optional 5432 automationhub_pg_sslmode hub_pg_sslmode Controls the SSL/TLS mode to use when automation hub connects to the PostgreSQL database. Valid options include verify-full , verify-ca , require , prefer , allow , disable . Optional prefer automationhub_pg_username hub_pg_username Username for the automation hub PostgreSQL database user. Optional RPM = automationhub Container = pulp automationhub_pgclient_sslcert hub_pg_tls_cert Path to the PostgreSQL SSL/TLS certificate file for automation hub. Required if using client certificate authentication. automationhub_pgclient_sslkey hub_pg_tls_key Path to the PostgreSQL SSL/TLS key file for automation hub. Required if using client certificate authentication. automationhub_pgclient_tls_files_remote Denote whether the PostgreSQL client certificate sources are local to the installation program ( false ) or on the remote component server ( true ). Optional The value defined in automationhub_tls_files_remote . automationhub_require_content_approval Controls whether content signing is enabled or disabled for automation hub. By default when you upload collections to automation hub, an administrator must approve it before they are made available to users. To disable the content approval flow, set the variable to false . Optional true automationhub_restore_signing_keys Controls whether or not existing signing keys should be restored from a backup. Set to false to disable restoration of existing signing keys. Optional true automationhub_seed_collections hub_seed_collections Controls whether or not pre-loading of collections is enabled. When you run the bundle installer, validated content is uploaded to the validated repository, and certified content is uploaded to the rh-certified repository. By default, certified content and validated content are both uploaded. If you do not want to pre-load content, set this variable to false . For the RPM-based installer, if you only want one type of content, set this variable to true and set the automationhub_collection_seed_repository variable to the type of content you want to include. Optional true automationhub_ssl_cert hub_tls_cert Path to the SSL/TLS certificate file for automation hub. Optional automationhub_ssl_key hub_tls_key Path to the SSL/TLS key file for automation hub. Optional automationhub_tls_files_remote hub_tls_remote Denote whether the automation hub provided certificate files are local to the installation program ( false ) or on the remote component server ( true ). Optional false automationhub_user_headers hub_nginx_user_headers List of additional NGINX headers to add to automation hub’s NGINX configuration. Optional [] generate_automationhub_token Controls whether or not a token is generated for automation hub during installation. By default, a token is automatically generated during a fresh installation. If set to true , a token is regenerated during installation. Optional false nginx_hsts_max_age hub_nginx_hsts_max_age Maximum duration (in seconds) that HTTP Strict Transport Security (HSTS) is enforced for automation hub. Optional 63072000 pulp_secret hub_secret_key Secret key value used by automation hub to sign and encrypt data. Optional hub_azure_account_key Azure blob storage account key. Required if using an Azure blob storage backend. hub_azure_account_name Account name associated with the Azure blob storage. Required when using an Azure blob storage backend. hub_azure_container Name of the Azure blob storage container. Optional pulp hub_azure_extra_settings Defines extra parameters for the Azure blob storage backend. For more information about the list of parameters, see django-storages documentation - Azure Storage . Optional {} hub_collection_signing_pass Password for the automation content collection signing service. Required if the collection signing service is protected by a passphrase. hub_collection_signing_service Service for signing collections. Optional ansible-default hub_container_signing_pass Password for the automation content container signing service. Required if the container signing service is protected by a passphrase. hub_container_signing_service Service for signing containers. Optional container-default hub_nginx_http_port Port number that automation hub listens on for HTTP requests. Optional 8081 hub_nginx_https_port Port number that automation hub listens on for HTTPS requests. Optional 8444 hub_nginx_https_protocols Protocols that automation hub will support when handling HTTPS traffic. Optional [TLSv1.2, TLSv1.3] hub_pg_socket UNIX socket used by automation hub to connect to the PostgreSQL database. Optional hub_s3_access_key AWS S3 access key. Required if using an AWS S3 storage backend. hub_s3_bucket_name Name of the AWS S3 storage bucket. Optional pulp hub_s3_extra_settings Used to define extra parameters for the AWS S3 storage backend. For more information about the list of parameters, see django-storages documentation - Amazon S3 . Optional {} hub_s3_secret_key AWS S3 secret key. Required if using an AWS S3 storage backend. hub_shared_data_mount_opts Mount options for the Network File System (NFS) share. Optional rw,sync,hard hub_shared_data_path Path to the Network File System (NFS) share with read, write, and execute (RWX) access. Required if installing more than one instance of automation hub with a file storage backend. When installing a single instance of automation hub, it is optional. hub_storage_backend Automation hub storage backend type. Possible values include: azure , file , s3 . Optional file hub_workers Number of automation hub workers. Optional 2 A.3. Automation controller variables RPM variable name Container variable name Description admin_email The email address used for the admin user for automation controller. admin_password controller_admin_password Required Automation controller admin password. Passwords must be enclosed in quotes when they are provided in plain text in the inventory file. Use of special characters for this variable is limited. The password can include any printable ASCII character except / , ” , or @ . admin_username controller_admin_user Automation controller admin user. Default = admin automation_controller_main_url Automation controller main URL. awx_pg_cert_auth controller_pg_cert_auth Set this variable to true to enable client certificate authentication. Default = false controller_tls_files_remote controller_tls_remote Automation controller TLS remote files. Default = false nginx_disable_hsts controller_nginx_disable_hsts Disable NGINX HTTP Strict Transport Security (HSTS). Default = false nginx_disable_https controller_nginx_disable_https Disable NGINX HTTPS. Default = false nginx_hsts_max_age controller_nginx_hsts_max_age This variable specifies how long, in seconds, the system must be considered as an HTTP Strict Transport Security (HSTS) host. That is, how long HTTPS is used only for communication. Default = 63072000 seconds, or two years. nginx_http_port controller_nginx_http_port The NGINX HTTP server listens for inbound connections. RPM default = 80 Container default = 8080 nginx_https_port controller_nginx_https_port The NGINX HTTPS server listens for secure connections. RPM Default = 443 Container default = 8443 nginx_user_headers controller_nginx_user_headers List of NGINX headers for the automation controller web server. Each element in the list is provided to the web server’s NGINX configuration as a separate line. Default = empty list node_state Optional The status of a node or group of nodes. Valid options are active , deprovision to remove a node from a cluster, or iso_migrate to migrate a legacy isolated node to an execution node. Default = active node_type See receptor_type for the container equivalent variable. For the [automationcontroller] group the two options are: node_type=control - The node only runs project and inventory updates, but not regular jobs. node_type=hybrid - The node runs everything. Default for this group = hybrid . For the [execution_nodes] group the two options are: node_type=hop - The node forwards jobs to an execution node. node_type=execution - The node can run jobs. Default for this group = execution peers See receptor_peers for the container equivalent variable. Used to indicate which nodes a specific host or group connects to. Wherever this variable is defined, an outbound connection to the specific host or group is established. The peers variable can be a comma-separated list of hosts and groups from the inventory. This is resolved into a set of hosts that is used to construct the receptor.conf file. pg_database controller_pg_database The name of the PostgreSQL database used by automation controller. Default = awx pg_host controller_pg_host Required The hostname of the PostgreSQL database used by automation controller. Default = 127.0.0.1 pg_password controller_pg_password Required if not using client certificate authentication. The password for the automation controller PostgreSQL database. Use of special characters for this variable is limited. The ! , # , 0 and @ characters are supported. Use of other special characters can cause the setup to fail. pg_port controller_pg_port Required if not using an internal database. The port number of the PostgreSQL database used by automation controller. Default = 5432 pg_sslmode controller_pg_sslmode Determines the level of encryption and authentication for client server connections. Valid options include verify-full , verify-ca , require , prefer , allow , disable . Default = prefer pg_username controller_pg_username The username for the automation controller PostgreSQL database. Default = awx pgclient_sslcert controller_pg_tls_cert Required if using client certificate authentication. Path to the PostgreSQL SSL/TLS certificate file for automation controller. pgclient_sslkey controller_pg_tls_key Required if using client certificate authentication. Path to the PostgreSQL SSL/TLS key file for automation controller. web_server_ssl_cert controller_tls_cert Optional Path to the SSL/TLS certificate file for automation controller. web_server_ssl_key controller_tls_key Optional Path to the SSL/TLS key file for automation controller. controller_event_workers Automation controller event workers. Default = 4 controller_license_file The location of your automation controller license file. For example: controller_license_file=/path/to/license.zip If you are defining this variable as part of the postinstall process ( controller_postinstall = true ), then you need to also set the controller_postinstall_dir variable. controller_nginx_client_max_body_size NGINX maximum body size. Default = 5m controller_nginx_https_protocols NGINX HTTPS protocols. Default = [TLSv1.2, TLSv1.3] controller_pg_socket PostgreSQL Controller UNIX socket. controller_secret_key The secret key value used by automation controller to sign and encrypt data, ensuring secure communication and data integrity between services. controller_uwsgi_listen_queue_size Automation controller uWSGI listen queue size. Default = 2048 controller_postinstall Enable or disable the postinstall feature of the containerized installer. If set to true , then you also need to set controller_license_file and controller_postinstall_dir . Default = false controller_postinstall_dir The location of your automation controller postinstall directory. controller_postinstall_async_delay Postinstall delay between retries. Default = 1 controller_postinstall_async_retries Postinstall number of tries to attempt. Default = 30 controller_postinstall_ignore_files Automation controller ignore files. controller_postinstall_repo_ref Automation controller repository branch or tag. Default = main controller_postinstall_repo_url Automation controller repository URL. A.4. Database variables RPM variable name Container variable name Description pg_ssl_mode Choose one of the two available modes: prefer and verify-full . Set to verify-full for client-side enforced SSL/TLS. Default = prefer postgres_max_connections postgresql_max_connections Maximum database connections setting to apply if you are using an installer-managed database. See PostgreSQL database configuration and maintenance for automation controller for help selecting a value. Default = 1024 postgres_ssl_cert postgresql_tls_cert Path to the PostgreSQL SSL/TLS certificate file. postgres_ssl_key postgresql_tls_key Path to the PostgreSQL SSL/TLS key file. postgres_use_cert Location of the PostgreSQL user certificate file. /path/to/pgsql.crt postgres_use_ssl postgresql_disable_tls Determines if the connection between Ansible Automation Platform and the PostgreSQL database should use SSL/TLS. The default for this variable is false which means SSL/TLS is not used for PostgreSQL connections. When set to true , the platform connects to PostgreSQL by using SSL/TLS. postgresql_admin_username The username for the PostgreSQL admin user. When used, the installation program creates each component’s database and credentials. The PostgreSQL admin user must have SUPERUSER privileges. Default = postgres postgresql_admin_password Required when using postgresql_admin_username . The password for the PostgreSQL admin user. When used, the installation program creates each component’s database and credentials. The PostgreSQL admin user must have SUPERUSER privileges. postgresql_admin_database The name of the PostgreSQL admin database. Default = postgres postgresql_effective_cache_size This defines the total memory available for caching data. The format should be <int>MB. postgresql_keep_databases Determines whether or not to keep databases during uninstall. When set to true databases will be kept during uninstall. This variable applies to databases managed by the installation program only, and not external customer-created databases. Default = false postgresql_log_destination The location of the PostgreSQL log file. Default = /dev/stderr postgresql_password_encryption The type of PostgreSQL password encryption to use. Default = scram-sha-256 postgresql_shared_buffers The amount of memory allocated for caching data within the database. The format should be <int>MB. postgresql_tls_remote PostgreSQL TLS remote files. Default = false postgresql_port PostgreSQL port number. Default = 5432 A.5. Event-Driven Ansible controller variables RPM variable name Container variable name Description automationedacontroller_activation_workers eda_activation_workers Optional Number of workers for ansible-rulebook activation pods in Event-Driven Ansible. Default = (# of cores or threads) * 2 + 1 automationedacontroller_admin_email eda_admin_email Optional Email address used by Django for the admin user for Event-Driven Ansible controller. Default = admin@example.com automationedacontroller_admin_password eda_admin_password Required The admin password used by the Event-Driven Ansible controller instance. Passwords must be enclosed in quotes when they are provided in plain text in the inventory file. Use of special characters for this variable is limited. The password can include any printable ASCII character except / , ” , or @ . automationedacontroller_admin_username eda_admin_user Username used by Django to identify and create the admin superuser in Event-Driven Ansible controller. Default = admin automationedacontroller_allowed_hostnames List of additional addresses to enable for user access to Event-Driven Ansible controller. Default = empty list automationedacontroller_controller_verify_ssl Boolean flag used to verify automation controller’s web certificates when making calls from Event-Driven Ansible controller. Verified is true and not verified is false . Default = false automationedacontroller_disable_hsts eda_nginx_disable_hsts Optional Boolean flag to disable HSTS for Event-Driven Ansible controller. Default = false automationedacontroller_disable_https eda_nginx_disable_https Optional Boolean flag to disable HTTPS for Event-Driven Ansible controller. Default = false automationedacontroller_event_stream_path eda_event_stream_prefix_path API prefix path used for Event-Driven Ansible event-stream through platform gateway. Default = /eda-event-streams automationedacontroller_gunicorn_workers eda_gunicorn_workers Number of workers for the API served through Gunicorn. Default = (# of cores or threads) * 2 + 1 automationedacontroller_max_running_activations eda_max_running_activations Optional The number of maximum activations running concurrently per node. This is an integer that must be greater than 0. Default = 12 automationedacontroller_nginx_tls_files_remote eda_tls_remote Boolean flag to specify whether cert sources are on the remote host (true) or local (false). Default = false automationedacontroller_pg_cert_auth eda_pg_cert_auth Set this variable to true to enable client certificate authentication. Default = false automationedacontroller_pg_database eda_pg_database The name of the PostgreSQL database used by Event-Driven Ansible. RPM default = automationedacontroller Container default = eda automationedacontroller_pg_host eda_pg_host Required The hostname of the PostgreSQL database used by Event-Driven Ansible. Default = 127.0.0.1 automationedacontroller_pg_password eda_pg_password Required if not using client certificate authentication. The password for the Event-Driven Ansible PostgreSQL database. Use of special characters for this variable is limited. The ! , # , 0 and @ characters are supported. Use of other special characters can cause the setup to fail. automationedacontroller_pg_port eda_pg_port Required if not using an internal database. The port number of the PostgreSQL database used by Event-Driven Ansible. Default = 5432 automationedacontroller_pg_sslmode eda_pg_sslmode Determines the level of encryption and authentication for client server connections. Valid options include verify-full , verify-ca , require , prefer , allow , disable . Default = prefer automationedacontroller_pg_username eda_pg_username The username for the Event-Driven Ansible PostgreSQL database. RPM default = automationedacontroller Container default = eda automationedacontroller_pgclient_sslcert eda_pg_tls_cert Required if using client certificate authentication. Path to the PostgreSQL SSL/TLS certificate file for Event-Driven Ansible. automationedacontroller_pgclient_sslkey eda_pg_tls_key Required if using client certificate authentication. Path to the PostgreSQL SSL/TLS key file for Event-Driven Ansible. automationedacontroller_redis_host eda_redis_host The Redis hostname used by Event-Driven Ansible controller. automationedacontroller_redis_port eda_redis_port The port used for the Redis host defined by automationedacontroller_redis_host for Event-Driven Ansible controller. automationedacontroller_rq_workers Number of Redis Queue (RQ) workers used by Event-Driven Ansible controller. RQ workers are Python processes that run in the background. Default = (# of cores or threads) * 2 + 1 automationedacontroller_ssl_cert eda_tls_cert Optional Path to the SSL/TLS certificate file for Event-Driven Ansible. automationedacontroller_ssl_key eda_tls_key Optional Path to the SSL/TLS key file for Event-Driven Ansible. automationedacontroller_user_headers eda_nginx_user_headers List of additional NGINX headers to add to Event-Driven Ansible controller’s NGINX configuration. Default = empty list eda_node_type eda_type Optional Event-Driven Ansible controller node type. Default = hybrid eda_debug Event-Driven Ansible controller debug. Default = false eda_event_stream_url Event-Driven Ansible controller event stream URL. eda_main_url Event-Driven Ansible controller main URL. eda_nginx_client_max_body_size NGINX maximum body size. Default = 1m eda_nginx_hsts_max_age NGINX HSTS maximum age. Default = 63072000 eda_nginx_http_port NGINX HTTP port. Default = 8082 eda_nginx_https_port NGINX HTTPS port. Default = 8445 eda_nginx_https_protocols NGINX HTTPS protocols. Default = [TLSv1.2, TLSv1.3] eda_pg_socket PostgreSQL Event-Driven Ansible UNIX socket. eda_redis_disable_tls Disable TLS Redis (for many nodes). Default = false eda_redis_password Redis Event-Driven Ansible controller password (for many nodes). eda_redis_tls_cert Optional Path to the Event-Driven Ansible Redis certificate file. eda_redis_tls_key Optional Path to the Event-Driven Ansible Redis key file. eda_redis_username Redis Event-Driven Ansible controller username (for many nodes). eda_safe_plugins Event-Driven Ansible controller safe plugins. eda_secret_key The secret key value used by Event-Driven Ansible controller to sign and encrypt data, ensuring secure communication and data integrity between services. eda_workers Event-Driven Ansible controller workers count. Default = 2 A.6. General variables RPM variable name Container variable name Description aap_ca_cert_file ca_tls_cert Define a Certification Authority certificate along with a matching key when you want the installation program to create leaf certificates for each product for you. aap_ca_cert_files_remote ca_tls_remote Denote whether the CA provided certificate files are local to the installation program ( false ) or on the remote component server ( true ). Default = false aap_ca_key_file ca_tls_key Define the key for the matching certificate when you want the installation program to create leaf certificates for each product for you. bundle_install bundle_install Set to true to enable a bundled installation. Default = false bundle_install_folder bundle_dir Specify the path to the bundle directory when performing a bundle install. Container default = false RPM Default = /var/lib/ansible-automation-platform-bundle custom_ca_cert custom_ca_cert The path to the custom CA certificate file. If set, this installs a custom CA certificate to the system truststore. enable_insights_collection The default install registers the node to the Red Hat Insights for Red Hat Ansible Automation Platform for the Red Hat Ansible Automation Platform Service if the node is registered with Subscription Manager. Set to false to disable. Default = true nginx_tls_protocols Defines support for ssl_protocols in NGINX. Values available TLSv1 , TLSv1.1 , TLSv1.2 , TLSv1.3 . The TLSv1.1 and TLSv1.2 parameters only work when OpenSSL 1.0.1 or higher is used. The TLSv1.3 parameter only works when OpenSSL 1.1.1 or higher is used. If nginx_tls-protocols = ['TLSv1.3'] only TLSv1.3 is enabled. To set more than one protocol use nginx_tls_protocols = ['TLSv1.2', 'TLSv.1.3'] . Default = TLSv1.2 nginx_user_http_config List of NGINX configurations for /etc/nginx/nginx.conf under the http section. Each element in the list is provided into http nginx config as a separate line. Default = {} redis_cluster_ip redis_cluster_ip The IPv4 address used by the Redis cluster to identify each host in the cluster. Redis clusters cannot use hostnames or IPv6 addresses. When defining hosts in the [redis] group, use this variable to identify the IPv4 address if the default is not what you want. redis_mode redis_mode The Redis mode to use for your Ansible Automation Platform installation. Possible values are: standalone and cluster . For more information about Redis, see Caching and queueing system in Planning your installation . Default = cluster registry_password registry_password Required if performing an online non-bundled installation. The password credential for access to the registry source defined in registry_url . For more information, see Setting registry_username and registry_password . registry_url registry_url URL for the registry source. Default = registry.redhat.io registry_username registry_username Required if performing an online non-bundled installation. The username credential for access to the registry source defined in registry_url . For more information, see Setting registry_username and registry_password . registry_verify_ssl registry_tls_verify Controls whether SSL/TLS certificate verification should be enabled or disabled when making HTTPS requests. Default = true routable_hostname routable_hostname This variable is used if the machine running the installation program can only route to the target host through a specific URL. For example, if you use short names in your inventory, but the node running the installation program can only resolve that host by using a FQDN. If routable_hostname is not set, it should default to ansible_host . If you do not set ansible_host , inventory_hostname is used as a last resort. This variable is used as a host variable for particular hosts and not under the [all:vars] section. For further information, see Assigning a variable to one machine: host variables . backup_dir The location of the backup directory on the Ansible host. Used when performing backup and restore. Default = ~/backups container_compress Container compression software. Default = gzip container_keep_images Keep container images. Default = false container_pull_images Pull newer container images. Default = true registry_auth Use registry authentication. Default = true registry_ns_aap Ansible Automation Platform registry namespace. Default = ansible-automation-platform-25 registry_ns_rhel RHEL registry namespace. Default = rhel8 A.7. Image variables RPM variable name Container variable name Description controller_image Automation controller image. Default = controller-rhel8:latest de_extra_images Decision environment extra images. de_supported_image Decision environment supported image. Default = de-supported-rhel8:latest eda_image Event-Driven Ansible image. Default = eda-controller-rhel8:latest eda_web_image Event-Driven Ansible web image. Default = eda-controller-ui-rhel8:latest ee_29_enabled Enable execution environment 29. Default = false ee_29_image Execution environment 29 image. Default = ee-29-rhel8:latest ee_extra_images Execution environment extra images. ee_minimal_image Execution environment minimal image. Default = ee-minimal-rhel8:latest ee_supported_image Execution environment supported image. Default = ee-supported-rhel8:latest hub_image Automation hub image. Default = hub-rhel8:latest hub_web_image Automation hub web image. Default = hub-web-rhel8:latest postgresql_image PostgreSQL image. Default = postgresql-15:latest receptor_image Receptor image. Default = receptor-rhel8:latest redis_image Redis image. Default = redis-6:latest pcp_image Performance Co-Pilot image. Default = rhel8-pcp:latest A.8. Platform gateway variables RPM variable name Container variable name Description automationgateway_admin_email gateway_admin_email The email address used for the admin user for platform gateway. automationgateway_admin_password gateway_admin_password Required The admin password used to connect to the platform gateway instance. Passwords must be enclosed in quotes when they are provided in plain text in the inventory file. Use of special characters for this variable is limited. The password can include any printable ASCII character except / , ” , or @ . automationgateway_admin_username gateway_admin_user Optional The username used to identify and create the admin superuser in platform gateway. Default = admin automationgateway_disable_hsts gateway_nginx_disable_hsts Optional Disable NGINX HSTS. Default = false automationgateway_disable_https gateway_nginx_disable_https Optional Disable NGINX HTTPS. Default = false automationgateway_grpc_auth_service_timeout gateway_grpc_auth_service_timeout Platform gateway auth server timeout. Default = 30s automationgateway_grpc_server_max_threads_per_process gateway_grpc_server_max_threads_per_process Platform gateway auth server threads per process. Default = 10 automationgateway_grpc_server_processes gateway_grpc_server_processes Platform gateway auth server processes Default = 5 automationgateway_main_url gateway_main_url Optional The main platform gateway URL that clients will connect to (e.g. https://<gateway_node> ). If not specified, the first the first node in the [automationgateway] group will be used when needed. automationgateway_pg_cert_auth gateway_pg_cert_auth Set this variable to true to enable client certificate authentication. Default = false automationgateway_pg_database gateway_pg_database The name of the PostgreSQL database used by platform gateway. RPM default = automationgateway Container default = gateway automationgateway_pg_host gateway_pg_host Required The hostname of the PostgreSQL database used by platform gateway. Default = 127.0.0.1 automationgateway_pg_password gateway_pg_password Required if not using client certificate authentication. The password for the platform gateway PostgreSQL database. Use of special characters for this variable is limited. The ! , # , 0 and @ characters are supported. Use of other special characters can cause the setup to fail. automationgateway_pg_port gateway_pg_port Required if not using an internal database. The port number of the PostgreSQL database used by platform gateway. Default = 5432 automationgateway_pg_sslmode gateway_pg_sslmode Determines the level of encryption and authentication for client server connections. Valid options include verify-full , verify-ca , require , prefer , allow , disable . Default = prefer automationgateway_pg_username gateway_pg_username The username for the platform gateway PostgreSQL database. RPM default = automationgateway Container default = gateway automationgateway_pgclient_sslcert gateway_pg_tls_cert Required if using client certificate authentication. Path to the PostgreSQL SSL/TLS certificate file for platform gateway. automationgateway_pgclient_sslkey gateway_pg_tls_key Required if using client certificate authentication. Path to the PostgreSQL SSL/TLS key file for platform gateway. automationgateway_redis_host gateway_redis_host The Redis hostname used by platform gateway. automationgateway_redis_port gateway_redis_port The Redis platform gateway port. Default = 6379 automationgateway_ssl_cert gateway_tls_cert Optional Path to the SSL/TLS certificate file for platform gateway. automationgateway_ssl_key gateway_tls_key Optional Path to the SSL/TLS key file for platform gateway. gateway_nginx_client_max_body_size NGINX maximum body size. Default = 5m gateway_nginx_hsts_max_age NGINX HSTS maximum age. Default = 63072000 gateway_nginx_http_port NGINX HTTP port. gateway_nginx_https_port NGINX HTTPS port. gateway_nginx_https_protocols NGINX HTTPS protocols. Default = [TLSv1.2, TLSv1.3] gateway_nginx_user_headers Custom NGINX headers. gateway_redis_disable_tls Disable TLS Redis. Default = false gateway_redis_password Redis platform gateway password. gateway_redis_tls_cert Optional Path to the platform gateway Redis certificate file. gateway_redis_tls_key Optional Path to the platform gateway Redis key file. gateway_redis_username Redis platform gateway username. Default = gateway gateway_secret_key The secret key value used by platform gateway to sign and encrypt data, ensuring secure communication and data integrity between services. gateway_tls_remote Platform gateway TLS remote files. Default = false gateway_uwsgi_listen_queue_size Platform gateway uWSGI listen queue size. Default = 4096 A.9. Receptor variables RPM variable name Container variable name Description receptor_disable_signing Disable receptor signing. Default = false receptor_disable_tls Disable receptor TLS. Default = false receptor_log_level Receptor logging level. Default = info receptor_mintls13 Receptor TLS 1.3 minimal. Default = false See peers for the RPM equivalent variable receptor_peers Used to indicate which nodes a specific host connects to. Wherever this variable is defined, an outbound connection to the specific host is established. This variable can be a comma-separated list of hosts only and not groups from the inventory. This is resolved into a set of hosts that is used to construct the receptor.conf file. For example usage, see Adding execution nodes . Default = [] receptor_datadir This variable configures the receptor data directory. By default, it is set to /tmp/receptor . To change the default location, run the installation script with \"-e receptor_datadir=\" and specify the target directory that you want. NOTES * The target directory must be accessible to awx users. * If the target directory is a temporary file system tmpfs , ensure it is remounted correctly after a reboot. Failure to do so results in the receptor no longer having a working directory. receptor_listener_port receptor_port Receptor port number. Default = 27199 receptor_listener_protocol receptor_protocol Receptor protocol. Default = tcp receptor_signing_private_key Receptor signing private key. receptor_signing_public_key Receptor signing public key. receptor_signing_remote Receptor signing remote files. Default = false receptor_tls_cert Path to the SSL/TLS certificate file for receptor. receptor_tls_key Path to the SSL/TLS key file for receptor. receptor_tls_remote Receptor TLS remote files. Default = false See node_type for the RPM equivalent variable. receptor_type For the [automationcontroller] group the two options are: receptor_type=control - The node only runs project and inventory updates, but not regular jobs. receptor_type=hybrid - The node runs everything. Default for this group = hybrid . For the [execution_nodes] group the two options are: receptor_type=hop - The node forwards jobs to an execution node. receptor_type=execution - The node can run jobs. Default for this group = execution Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/rpm_installation/appendix-inventory-files-vars"}
{"title": "Monitoring", "content": "Monitoring  Configuring and using the monitoring stack in OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/monitoring/index"}
{"title": "Chapter 14. Porting containers to systemd using Podman", "content": "Chapter 14. Porting containers to systemd using Podman Podman (Pod Manager) is a simple daemonless tool fully featured container engine. Podman provides a Docker-CLI comparable command line that makes the transition from other container engines easier and enables the management of pods, containers, and images. Originally, Podman was not designed to provide an entire Linux system or manage services, such as start-up order, dependency checking, and failed service recovery. systemd was responsible for a complete system initialization. Due to Red Hat integrating containers with systemd , you can manage OCI and Docker-formatted containers built by Podman in the same way as other services and features are managed in a Linux system. You can use the systemd initialization service to work with pods and containers. With systemd unit files, you can: Set up a container or pod to start as a systemd service. Define the order in which the containerized service runs and check for dependencies (for example making sure another service is running, a file is available or a resource is mounted). Control the state of the systemd system using the systemctl command. You can generate portable descriptions of containers and pods by using systemd unit files. 14.1. Auto-generating a systemd unit file using Quadlets With Quadlet, you describe how to run a container in a format that is very similar to regular systemd unit files. The container descriptions focus on the relevant container details and hide technical details of running containers under systemd . Create the <CTRNAME> .container unit file in one of the following directories: For root users: /usr/share/containers/systemd/ or /etc/containers/systemd/ For rootless users: $HOME/.config/containers/systemd/ , $XDG_CONFIG_HOME/containers/systemd/, /etc/containers/systemd/users/$(UID) , or /etc/containers/systemd/users/ Note Quadlet is available beginning with Podman v4.6. Prerequisites The container-tools meta-package is installed. Procedure Create the mysleep.container unit file: $ cat $HOME/.config/containers/systemd/mysleep.container [Unit] Description=The sleep container After=local-fs.target [Container] Image=registry.access.redhat.com/ubi9-minimal:latest Exec=sleep 1000 [Install] # Start by default on boot WantedBy=multi-user.target default.target In the [Container] section you must specify: Image - container mage you want to tun Exec - the command you want to run inside the container This enables you to use all other fields specified in a systemd unit file. Create the mysleep.service based on the mysleep.container file: $ systemctl --user daemon-reload Optional: Check the status of the mysleep.service : $ systemctl --user status mysleep.service ○ mysleep.service - The sleep container Loaded: loaded (/home/ username /.config/containers/systemd/mysleep.container; generated) Active: inactive (dead) Start the mysleep.service : $ systemctl --user start mysleep.service Verification Check the status of the mysleep.service : $ systemctl --user status mysleep.service ● mysleep.service - The sleep container Loaded: loaded (/home/ username /.config/containers/systemd/mysleep.container; generated) Active: active (running) since Thu 2023-02-09 18:07:23 EST; 2s ago Main PID: 265651 (conmon) Tasks: 3 (limit: 76815) Memory: 1.6M CPU: 94ms CGroup: ... List all containers: $ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 421c8293fc1b registry.access.redhat.com/ubi9-minimal:latest sleep 1000 30 seconds ago Up 10 seconds ago systemd-mysleep Note that the name of the created container consists of the following elements: a systemd- prefix a name of the systemd unit, that is systemd-mysleep This naming helps to distinguish common containers from containers running in systemd units. It also helps to determine which unit a container runs in. If you want to change the name of the container, use the ContainerName field in the [Container] section. Additional resources Make systemd better for Podman with Quadlet Quadlet upstream documentation 14.2. Enabling systemd services When enabling the service, you have different options. Procedure Enable the service: To enable a service at system start, no matter if user is logged in or not, enter: # systemctl enable <service> You have to copy the systemd unit files to the /etc/systemd/system directory. To start a service at user login and stop it at user logout, enter: $ systemctl --user enable <service> You have to copy the systemd unit files to the $HOME/.config/systemd/user directory. To enable users to start a service at system start and persist over logouts, enter: # loginctl enable-linger <username> Additional resources systemctl and loginctl man pages on your system Enabling a system service to start at boot 14.3. Auto-starting containers using systemd You can control the state of the systemd system and service manager using the systemctl command. You can enable, start, stop the service as a non-root user. To install the service as a root user, omit the --user option. Prerequisites The container-tools meta-package is installed. Procedure Reload systemd manager configuration: # systemctl --user daemon-reload Enable the service container.service and start it at boot time: # systemctl --user enable container.service Start the service immediately: # systemctl --user start container.service Check the status of the service: $ systemctl --user status container.service ● container.service - Podman container.service Loaded: loaded (/home/user/.config/systemd/user/container.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2020-09-16 11:56:57 CEST; 8s ago Docs: man:podman-generate-systemd(1) Process: 80602 ExecStart=/usr/bin/podman run --conmon-pidfile //run/user/1000/container.service-pid --cidfile //run/user/1000/container.service-cid -d ubi9-minimal:> Process: 80601 ExecStartPre=/usr/bin/rm -f //run/user/1000/container.service-pid //run/user/1000/container.service-cid (code=exited, status=0/SUCCESS) Main PID: 80617 (conmon) CGroup: /user.slice/user-1000.slice/user@1000.service/container.service ├─ 2870 /usr/bin/podman ├─80612 /usr/bin/slirp4netns --disable-host-loopback --mtu 65520 --enable-sandbox --enable-seccomp -c -e 3 -r 4 --netns-type=path /run/user/1000/netns/cni-> ├─80614 /usr/bin/fuse-overlayfs -o lowerdir=/home/user/.local/share/containers/storage/overlay/l/YJSPGXM2OCDZPLMLXJOW3NRF6Q:/home/user/.local/share/contain> ├─80617 /usr/bin/conmon --api-version 1 -c cbc75d6031508dfd3d78a74a03e4ace1732b51223e72a2ce4aa3bfe10a78e4fa -u cbc75d6031508dfd3d78a74a03e4ace1732b51223e72> └─cbc75d6031508dfd3d78a74a03e4ace1732b51223e72a2ce4aa3bfe10a78e4fa └─80626 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1d You can check if the service is enabled using the systemctl is-enabled container.service command. Verification List containers that are running or have exited: # podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f20988d59920 registry.access.redhat.com/ubi9-minimal:latest top 12 seconds ago Up 11 seconds ago funny_zhukovsky Note To stop container.service , enter: # systemctl --user stop container.service Additional resources systemctl man page on your system Running containers with Podman and shareable systemd services Enabling a system service to start at boot 14.4. Advantages of using Quadlets over the podman generate systemd command You can use the Quadlets tool, which describes how to run a container in a format similar to regular systemd unit files. Note Quadlet is available beginning with Podman v4.6. Quadlets have many advantages over generating unit files using the podman generate systemd command, such as: Easy to maintain : The container descriptions focus on the relevant container details and hide technical details of running containers under systemd . Automatically updated : Quadlets do not require manually regenerating unit files after an update. If a newer version of Podman is released, your service is automatically updated when the systemclt daemon-reload command is executed, for example, at boot time. Simplified workflow : Thanks to the simplified syntax, you can create Quadlet files from scratch and deploy them anywhere. Support standard systemd options : Quadlet extends the existing systemd-unit syntax with new tables, for example, a table to configure a container. Note Quadlet supports a subset of Kubernetes YAML capabilities. For more information, see the support matrix of supported YAML fields . You can generate the YAML files by using one of the following tools: Podman: podman generate kube command OpenShift: oc generate command with the --dry-run option Kubernetes: kubectl create command with the --dry-run option Quadlet supports these unit file types: Container units : Used to manage containers by running the podman run command. File extension: .container Section name: [Container] Required fields: Image describing the container image the service runs Kube units : Used to manage containers defined in Kubernetes YAML files by running the podman kube play command. File extension: .kube Section name: [Kube] Required fields: Yaml defining the path to the Kubernetes YAML file Network units : Used to create Podman networks that may be referenced in .container or .kube files. File extension: .network Section name: [Network] Required fields: None Volume units : Used to create Podman volumes that may be referenced in .container files. File extension: .volume Section name: [Volume] Required fields: None Additional resources Quadlet upstream documentation 14.5. Generating a systemd unit file using Podman Podman allows systemd to control and manage container processes. You can generate a systemd unit file for the existing containers and pods using podman generate systemd command. It is recommended to use podman generate systemd because the generated units files change frequently (via updates to Podman) and the podman generate systemd ensures that you get the latest version of unit files. Note Starting with Podman v4.6, you can use the Quadlets that describe how to run a container in a format similar to regular systemd unit files and hides the complexity of running containers under systemd . Prerequisites The container-tools meta-package is installed. Procedure Create a container (for example myubi ): $ podman create --name myubi registry.access.redhat.com/ubi9:latest sleep infinity 0280afe98bb75a5c5e713b28de4b7c5cb49f156f1cce4a208f13fee2f75cb453 Use the container name or ID to generate the systemd unit file and direct it into the ~/.config/systemd/user/container-myubi.service file: $ podman generate systemd --name myubi > ~/.config/systemd/user/container-myubi.service Verification Display the content of generated systemd unit file: $ cat ~/.config/systemd/user/container-myubi.service # container-myubi.service # autogenerated by Podman 3.3.1 # Wed Sep 8 20:34:46 CEST 2021 [Unit] Description=Podman container-myubi.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=/run/user/1000/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman start myubi ExecStop=/usr/bin/podman stop -t 10 myubi ExecStopPost=/usr/bin/podman stop -t 10 myubi PIDFile=/run/user/1000/containers/overlay-containers/9683103f58a32192c84801f0be93446cb33c1ee7d9cdda225b78049d7c5deea4/userdata/conmon.pid Type=forking [Install] WantedBy=multi-user.target default.target The Restart=on-failure line sets the restart policy and instructs systemd to restart when the service cannot be started or stopped cleanly, or when the process exits non-zero. The ExecStart line describes how we start the container. The ExecStop line describes how we stop and remove the container. Additional resources Running containers with Podman and shareable systemd services 14.6. Automatically generating a systemd unit file using Podman By default, Podman generates a unit file for existing containers or pods. You can generate more portable systemd unit files using the podman generate systemd --new . The --new flag instructs Podman to generate unit files that create, start and remove containers. Note Starting with Podman v4.6, you can use the Quadlets that describe how to run a container in a format similar to regular systemd unit files and hides the complexity of running containers under systemd . Prerequisites The container-tools meta-package is installed. Procedure Pull the image you want to use on your system. For example, to pull the httpd-24 image: # podman pull registry.access.redhat.com/ubi9/httpd-24 Optional: List all images available on your system: # podman images REPOSITORY TAG IMAGE ID CREATED SIZE registry.access.redhat.com/ubi9/httpd-24 latest 8594be0a0b57 2 weeks ago 462 MB Create the httpd container: # podman create --name httpd -p 8080:8080 registry.access.redhat.com/ubi9/httpd-24 cdb9f981cf143021b1679599d860026b13a77187f75e46cc0eac85293710a4b1 Optional: Verify the container has been created: # podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cdb9f981cf14 registry.access.redhat.com/ubi9/httpd-24:latest /usr/bin/run-http... 5 minutes ago Created 0.0.0.0:8080->8080/tcp httpd Generate a systemd unit file for the httpd container: # podman generate systemd --new --files --name httpd /root/container-httpd.service Display the content of the generated container-httpd.service systemd unit file: # cat /root/container-httpd.service # container-httpd.service # autogenerated by Podman 3.3.1 # Wed Sep 8 20:41:44 CEST 2021 [Unit] Description=Podman container-httpd.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStartPre=/bin/rm -f %t/%n.ctr-id ExecStart=/usr/bin/podman run --cidfile=%t/%n.ctr-id --sdnotify=conmon --cgroups=no-conmon --rm -d --replace --name httpd -p 8080:8080 registry.access.redhat.com/ubi9/httpd-24 ExecStop=/usr/bin/podman stop --ignore --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm -f --ignore --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=multi-user.target default.target Note Unit files generated using the --new option do not expect containers and pods to exist. Therefore, they perform the podman run command when starting the service (see the ExecStart line) instead of the podman start command. For example, see section Generating a systemd unit file using Podman . The podman run command uses the following command-line options: The --conmon-pidfile option points to a path to store the process ID for the conmon process running on the host. The conmon process terminates with the same exit status as the container, which allows systemd to report the correct service status and restart the container if needed. The --cidfile option points to the path that stores the container ID. The %t is the path to the run time directory root, for example /run/user/$UserID . The %n is the full name of the service. Copy unit files to /etc/systemd/system for installing them as a root user: # cp -Z container-httpd.service /etc/systemd/system Enable and start the container-httpd.service : # systemctl daemon-reload # systemctl enable --now container-httpd.service Created symlink /etc/systemd/system/multi-user.target.wants/container-httpd.service /etc/systemd/system/container-httpd.service. Created symlink /etc/systemd/system/default.target.wants/container-httpd.service /etc/systemd/system/container-httpd.service. Verification Check the status of the container-httpd.service : # systemctl status container-httpd.service ● container-httpd.service - Podman container-httpd.service Loaded: loaded (/etc/systemd/system/container-httpd.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2021-08-24 09:53:40 EDT; 1min 5s ago Docs: man:podman-generate-systemd(1) Process: 493317 ExecStart=/usr/bin/podman run --conmon-pidfile /run/container-httpd.pid --cidfile /run/container-httpd.ctr-id --cgroups=no-conmon -d --repla> Process: 493315 ExecStartPre=/bin/rm -f /run/container-httpd.pid /run/container-httpd.ctr-id (code=exited, status=0/SUCCESS) Main PID: 493435 (conmon) ... Additional resources Improved Systemd Integration with Podman 2.0 Enabling a system service to start at boot 14.7. Automatically starting pods using systemd You can start multiple containers as systemd services. Note that the systemctl command should only be used on the pod and you should not start or stop containers individually via systemctl , as they are managed by the pod service along with the internal infra-container. Note Starting with Podman v4.6, you can use the Quadlets that describe how to run a container in a format similar to regular systemd unit files and hides the complexity of running containers under systemd . Prerequisites The container-tools meta-package is installed. Procedure Create an empty pod, for example named systemd-pod : $ podman pod create --name systemd-pod 11d4646ba41b1fffa51c108cbdf97cfab3213f7bd9b3e1ca52fe81b90fed5577 Optional: List all pods: $ podman pod ps POD ID NAME STATUS CREATED # OF CONTAINERS INFRA ID 11d4646ba41b systemd-pod Created 40 seconds ago 1 8a428b257111 11d4646ba41b1fffa51c108cbdf97cfab3213f7bd9b3e1ca52fe81b90fed5577 Create two containers in the empty pod. For example, to create container0 and container1 in systemd-pod : $ podman create --pod systemd-pod --name container0 registry.access.redhat.com/ubi 9 top $ podman create --pod systemd-pod --name container1 registry.access.redhat.com/ubi 9 top Optional: List all pods and containers associated with them: $ podman ps -a --pod CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES POD ID PODNAME 24666f47d9b2 registry.access.redhat.com/ubi9:latest top 3 minutes ago Created container0 3130f724e229 systemd-pod 56eb1bf0cdfe k8s.gcr.io/pause:3.2 4 minutes ago Created 3130f724e229-infra 3130f724e229 systemd-pod 62118d170e43 registry.access.redhat.com/ubi9:latest top 3 seconds ago Created container1 3130f724e229 systemd-pod Generate the systemd unit file for the new pod: $ podman generate systemd --files --name systemd-pod /home/user1/pod-systemd-pod.service /home/user1/container-container0.service /home/user1/container-container1.service Note that three systemd unit files are generated, one for the systemd-pod pod and two for the containers container0 and container1 . Display pod-systemd-pod.service unit file: $ cat pod-systemd-pod.service # pod-systemd-pod.service # autogenerated by Podman 3.3.1 # Wed Sep 8 20:49:17 CEST 2021 [Unit] Description=Podman pod-systemd-pod.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor= Requires=container-container0.service container-container1.service Before=container-container0.service container-container1.service [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman start bcb128965b8e-infra ExecStop=/usr/bin/podman stop -t 10 bcb128965b8e-infra ExecStopPost=/usr/bin/podman stop -t 10 bcb128965b8e-infra PIDFile=/run/user/1000/containers/overlay-containers/1dfdcf20e35043939ea3f80f002c65c00d560e47223685dbc3230e26fe001b29/userdata/conmon.pid Type=forking [Install] WantedBy=multi-user.target default.target The Requires line in the [Unit] section defines dependencies on container-container0.service and container-container1.service unit files. Both unit files will be activated. The ExecStart and ExecStop lines in the [Service] section start and stop the infra-container, respectively. Display container-container0.service unit file: $ cat container-container0.service # container-container0.service # autogenerated by Podman 3.3.1 # Wed Sep 8 20:49:17 CEST 2021 [Unit] Description=Podman container-container0.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=/run/user/1000/containers BindsTo=pod-systemd-pod.service After=pod-systemd-pod.service [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman start container0 ExecStop=/usr/bin/podman stop -t 10 container0 ExecStopPost=/usr/bin/podman stop -t 10 container0 PIDFile=/run/user/1000/containers/overlay-containers/4bccd7c8616ae5909b05317df4066fa90a64a067375af5996fdef9152f6d51f5/userdata/conmon.pid Type=forking [Install] WantedBy=multi-user.target default.target The BindsTo line line in the [Unit] section defines the dependency on the pod-systemd-pod.service unit file The ExecStart and ExecStop lines in the [Service] section start and stop the container0 respectively. Display container-container1.service unit file: $ cat container-container1.service Copy all the generated files to $HOME/.config/systemd/user for installing as a non-root user: $ cp pod-systemd-pod.service container-container0.service container-container1.service $HOME/.config/systemd/user Enable the service and start at user login: $ systemctl enable --user pod-systemd-pod.service Created symlink /home/user1/.config/systemd/user/multi-user.target.wants/pod-systemd-pod.service /home/user1/.config/systemd/user/pod-systemd-pod.service. Created symlink /home/user1/.config/systemd/user/default.target.wants/pod-systemd-pod.service /home/user1/.config/systemd/user/pod-systemd-pod.service. Note that the service stops at user logout. Verification Check if the service is enabled: $ systemctl is-enabled pod-systemd-pod.service enabled Additional resources podman-create , podman-generate-systemd , and systemctl man pages on your system Running containers with Podman and shareable systemd services Enabling a system service to start at boot 14.8. Automatically updating containers using Podman The podman auto-update command allows you to automatically update containers according to their auto-update policy. The podman auto-update command updates services when the container image is updated on the registry. To use auto-updates, containers must be created with the --label \"io.containers.autoupdate=image\" label and run in a systemd unit generated by podman generate systemd --new command. Podman searches for running containers with the \"io.containers.autoupdate\" label set to \"image\" and communicates to the container registry. If the image has changed, Podman restarts the corresponding systemd unit to stop the old container and create a new one with the new image. As a result, the container, its environment, and all dependencies, are restarted. Note Starting with Podman v4.6, you can use the Quadlets that describe how to run a container in a format similar to regular systemd unit files and hides the complexity of running containers under systemd . Prerequisites The container-tools meta-package is installed. Procedure Start a myubi container based on the registry.access.redhat.com/ubi9/ubi-init image: # podman run --label \"io.containers.autoupdate=image\" \\ --name myubi -dt registry.access.redhat.com/ubi9/ubi-init top bc219740a210455fa27deacc96d50a9e20516492f1417507c13ce1533dbdcd9d Optional: List containers that are running or have exited: # podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 76465a5e2933 registry.access.redhat.com/9/ubi-init:latest top 24 seconds ago Up 23 seconds ago myubi Generate a systemd unit file for the myubi container: # podman generate systemd --new --files --name myubi /root/container-myubi.service Copy unit files to /usr/lib/systemd/system for installing it as a root user: # cp -Z ~/container-myubi.service /usr/lib/systemd/system Reload systemd manager configuration: # systemctl daemon-reload Start and check the status of a container: # systemctl start container-myubi.service # systemctl status container-myubi.service Auto-update the container: # podman auto-update Additional resources Improved Systemd Integration with Podman 2.0 Running containers with Podman and shareable systemd services Enabling a system service to start at boot 14.9. Automatically updating containers using systemd As mentioned in section Automatically updating containers using Podman , you can update the container using the podman auto-update command. It integrates into custom scripts and can be invoked when needed. Another way to auto update the containers is to use the pre-installed podman-auto-update.timer and podman-auto-update.service systemd service. The podman-auto-update.timer can be configured to trigger auto updates at a specific date or time. The podman-auto-update.service can further be started by the systemctl command or be used as a dependency by other systemd services. As a result, auto updates based on time and events can be triggered in various ways to meet individual needs and use cases. Note Starting with Podman v4.6, you can use the Quadlets that describe how to run a container in a format similar to regular systemd unit files and hides the complexity of running containers under systemd . Prerequisites The container-tools meta-package is installed. Procedure Display the podman-auto-update.service unit file: # cat /usr/lib/systemd/system/podman-auto-update.service [Unit] Description=Podman auto-update service Documentation=man:podman-auto-update(1) Wants=network.target After=network-online.target [Service] Type=oneshot ExecStart=/usr/bin/podman auto-update [Install] WantedBy=multi-user.target default.target Display the podman-auto-update.timer unit file: # cat /usr/lib/systemd/system/podman-auto-update.timer [Unit] Description=Podman auto-update timer [Timer] OnCalendar=daily Persistent=true [Install] WantedBy=timers.target In this example, the podman auto-update command is launched daily at midnight. Enable the podman-auto-update.timer service at system start: # systemctl enable podman-auto-update.timer Start the systemd service: # systemctl start podman-auto-update.timer Optional: List all timers: # systemctl list-timers --all NEXT LEFT LAST PASSED UNIT ACTIVATES Wed 2020-12-09 00:00:00 CET 9h left n/a n/a podman-auto-update.timer podman-auto-update.service You can see that podman-auto-update.timer activates the podman-auto-update.service . Additional resources Improved Systemd Integration with Podman 2.0 Running containers with Podman and shareable systemd services Enabling a system service to start at boot Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/building_running_and_managing_containers/assembly_porting-containers-to-systemd-using-podman_building-running-and-managing-containers"}
{"title": "Chapter 3. Understanding persistent storage", "content": "Chapter 3. Understanding persistent storage 3.1. Persistent storage overview Managing storage is a distinct problem from managing compute resources. OpenShift Container Platform uses the Kubernetes persistent volume (PV) framework to allow cluster administrators to provision persistent storage for a cluster. Developers can use persistent volume claims (PVCs) to request PV resources without having specific knowledge of the underlying storage infrastructure. PVCs are specific to a project, and are created and used by developers as a means to use a PV. PV resources on their own are not scoped to any single project; they can be shared across the entire OpenShift Container Platform cluster and claimed from any project. After a PV is bound to a PVC, that PV can not then be bound to additional PVCs. This has the effect of scoping a bound PV to a single namespace, that of the binding project. PVs are defined by a PersistentVolume API object, which represents a piece of existing storage in the cluster that was either statically provisioned by the cluster administrator or dynamically provisioned using a StorageClass object. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes but have a lifecycle that is independent of any individual pod that uses the PV. PV objects capture the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system. Important High availability of storage in the infrastructure is left to the underlying storage provider. PVCs are defined by a PersistentVolumeClaim API object, which represents a request for storage by a developer. It is similar to a pod in that pods consume node resources and PVCs consume PV resources. For example, pods can request specific levels of resources, such as CPU and memory, while PVCs can request specific storage capacity and access modes. For example, they can be mounted once read-write or many times read-only. 3.2. Lifecycle of a volume and claim PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs have the following lifecycle. 3.2.1. Provision storage In response to requests from a developer defined in a PVC, a cluster administrator configures one or more dynamic provisioners that provision storage and a matching PV. Alternatively, a cluster administrator can create a number of PVs in advance that carry the details of the real storage that is available for use. PVs exist in the API and are available for use. 3.2.2. Bind claims When you create a PVC, you request a specific amount of storage, specify the required access mode, and create a storage class to describe and classify the storage. The control loop in the master watches for new PVCs and binds the new PVC to an appropriate PV. If an appropriate PV does not exist, a provisioner for the storage class creates one. The size of all PVs might exceed your PVC size. This is especially true with manually provisioned PVs. To minimize the excess, OpenShift Container Platform binds to the smallest PV that matches all other criteria. Claims remain unbound indefinitely if a matching volume does not exist or can not be created with any available provisioner servicing a storage class. Claims are bound as matching volumes become available. For example, a cluster with many manually provisioned 50Gi volumes would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster. 3.2.3. Use pods and claimed PVs Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a pod. For those volumes that support multiple access modes, you must specify which mode applies when you use the claim as a volume in a pod. Once you have a claim and that claim is bound, the bound PV belongs to you for as long as you need it. You can schedule pods and access claimed PVs by including persistentVolumeClaim in the pod’s volumes block. Note If you attach persistent volumes that have high file counts to pods, those pods can fail or can take a long time to start. For more information, see When using Persistent Volumes with high file counts in OpenShift, why do pods fail to start or take an excessive amount of time to achieve \"Ready\" state? . 3.2.4. Storage Object in Use Protection The Storage Object in Use Protection feature ensures that PVCs in active use by a pod and PVs that are bound to PVCs are not removed from the system, as this can result in data loss. Storage Object in Use Protection is enabled by default. Note A PVC is in active use by a pod when a Pod object exists that uses the PVC. If a user deletes a PVC that is in active use by a pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any pods. Also, if a cluster admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC. 3.2.5. Release a persistent volume When you are finished with a volume, you can delete the PVC object from the API, which allows reclamation of the resource. The volume is considered released when the claim is deleted, but it is not yet available for another claim. The previous claimant’s data remains on the volume and must be handled according to policy. 3.2.6. Reclaim policy for persistent volumes The reclaim policy of a persistent volume tells the cluster what to do with the volume after it is released. A volume’s reclaim policy can be Retain , Recycle , or Delete . Retain reclaim policy allows manual reclamation of the resource for those volume plugins that support it. Recycle reclaim policy recycles the volume back into the pool of unbound persistent volumes once it is released from its claim. Important The Recycle reclaim policy is deprecated in OpenShift Container Platform 4. Dynamic provisioning is recommended for equivalent and better functionality. Delete reclaim policy deletes both the PersistentVolume object from OpenShift Container Platform and the associated storage asset in external infrastructure, such as AWS EBS or VMware vSphere. Note Dynamically provisioned volumes are always deleted. 3.2.7. Reclaiming a persistent volume manually When a persistent volume claim (PVC) is deleted, the persistent volume (PV) still exists and is considered \"released\". However, the PV is not yet available for another claim because the data of the previous claimant remains on the volume. Procedure To manually reclaim the PV as a cluster administrator: Delete the PV. $ oc delete pv <pv-name> The associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume, still exists after the PV is deleted. Clean up the data on the associated storage asset. Delete the associated storage asset. Alternately, to reuse the same storage asset, create a new PV with the storage asset definition. The reclaimed PV is now available for use by another PVC. 3.2.8. Changing the reclaim policy of a persistent volume To change the reclaim policy of a persistent volume: List the persistent volumes in your cluster: $ oc get pv Example output NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 manual 10s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 manual 6s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim3 manual 3s Choose one of your persistent volumes and change its reclaim policy: $ oc patch pv <your-pv-name> -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}' Verify that your chosen persistent volume has the right policy: $ oc get pv Example output NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 manual 10s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 manual 6s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Retain Bound default/claim3 manual 3s In the preceding output, the volume bound to claim default/claim3 now has a Retain reclaim policy. The volume will not be automatically deleted when a user deletes claim default/claim3 . 3.3. Persistent volumes Each PV contains a spec and status , which is the specification and status of the volume, for example: PersistentVolume object definition example apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 1 spec: capacity: storage: 5Gi 2 accessModes: - ReadWriteOnce 3 persistentVolumeReclaimPolicy: Retain 4 ... status: ... 1 Name of the persistent volume. 2 The amount of storage available to the volume. 3 The access mode, defining the read-write and mount permissions. 4 The reclaim policy, indicating how the resource should be handled once it is released. 3.3.1. Types of PVs OpenShift Container Platform supports the following persistent volume plugins: AliCloud Disk AWS Elastic Block Store (EBS) AWS Elastic File Store (EFS) Azure Disk Azure File Cinder Fibre Channel GCE Persistent Disk IBM VPC Block HostPath iSCSI Local volume NFS OpenStack Manila Red Hat OpenShift Data Foundation VMware vSphere 3.3.2. Capacity Generally, a persistent volume (PV) has a specific storage capacity. This is set by using the capacity attribute of the PV. Currently, storage capacity is the only resource that can be set or requested. Future attributes may include IOPS, throughput, and so on. 3.3.3. Access modes A persistent volume can be mounted on a host in any way supported by the resource provider. Providers have different capabilities and each PV’s access modes are set to the specific modes supported by that particular volume. For example, NFS can support multiple read-write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV’s capabilities. Claims are matched to volumes with similar access modes. The only two matching criteria are access modes and size. A claim’s access modes represent a request. Therefore, you might be granted more, but never less. For example, if a claim requests RWO, but the only volume available is an NFS PV (RWO+ROX+RWX), the claim would then match NFS because it supports RWO. Direct matches are always attempted first. The volume’s modes must match or contain more modes than you requested. The size must be greater than or equal to what is expected. If two types of volumes, such as NFS and iSCSI, have the same set of access modes, either of them can match a claim with those modes. There is no ordering between types of volumes and no way to choose one type over another. All volumes with the same modes are grouped, and then sorted by size, smallest to largest. The binder gets the group with matching modes and iterates over each, in size order, until one size matches. The following table lists the access modes: Table 3.1. Access modes Access Mode CLI abbreviation Description ReadWriteOnce RWO The volume can be mounted as read-write by a single node. ReadOnlyMany ROX The volume can be mounted as read-only by many nodes. ReadWriteMany RWX The volume can be mounted as read-write by many nodes. Important Volume access modes are descriptors of volume capabilities. They are not enforced constraints. The storage provider is responsible for runtime errors resulting from invalid use of the resource. For example, NFS offers ReadWriteOnce access mode. You must mark the claims as read-only if you want to use the volume’s ROX capability. Errors in the provider show up at runtime as mount errors. iSCSI and Fibre Channel volumes do not currently have any fencing mechanisms. You must ensure the volumes are only used by one node at a time. In certain situations, such as draining a node, the volumes can be used simultaneously by two nodes. Before draining the node, first ensure the pods that use these volumes are deleted. Table 3.2. Supported access modes for PVs Volume plugin ReadWriteOnce [1] ReadOnlyMany ReadWriteMany AliCloud Disk ✅ - - AWS EBS [2] ✅ - - AWS EFS ✅ ✅ ✅ Azure File ✅ ✅ ✅ Azure Disk ✅ - - Cinder ✅ - - Fibre Channel ✅ ✅ ✅ [3] GCE Persistent Disk ✅ - - HostPath ✅ - - IBM VPC Disk ✅ - - iSCSI ✅ ✅ ✅ [3] Local volume ✅ - - NFS ✅ ✅ ✅ OpenStack Manila - - ✅ Red Hat OpenShift Data Foundation ✅ - ✅ VMware vSphere ✅ - ✅ [4] ReadWriteOnce (RWO) volumes cannot be mounted on multiple nodes. If a node fails, the system does not allow the attached RWO volume to be mounted on a new node because it is already assigned to the failed node. If you encounter a multi-attach error message as a result, force delete the pod on a shutdown or crashed node to avoid data loss in critical workloads, such as when dynamic persistent volumes are attached. Use a recreate deployment strategy for pods that rely on AWS EBS. Only raw block volumes support the ReadWriteMany (RWX) access mode for Fibre Channel and iSCSI. For more information, see \"Block volume support\". If the underlying vSphere environment supports the vSAN file service, then the vSphere Container Storage Interface (CSI) Driver Operator installed by OpenShift Container Platform supports provisioning of ReadWriteMany (RWX) volumes. If you do not have vSAN file service configured, and you request RWX, the volume fails to get created and an error is logged. For more information, see \"Using Container Storage Interface\" \"VMware vSphere CSI Driver Operator\". 3.3.4. Phase Volumes can be found in one of the following phases: Table 3.3. Volume phases Phase Description Available A free resource not yet bound to a claim. Bound The volume is bound to a claim. Released The claim was deleted, but the resource is not yet reclaimed by the cluster. Failed The volume has failed its automatic reclamation. You can view the name of the PVC bound to the PV by running: $ oc get pv <pv-claim> 3.3.4.1. Mount options You can specify mount options while mounting a PV by using the attribute mountOptions . For example: Mount options example apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce mountOptions: 1 - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 persistentVolumeReclaimPolicy: Retain claimRef: name: claim1 namespace: default 1 Specified mount options are used while mounting the PV to the disk. The following PV types support mount options: AWS Elastic Block Store (EBS) Azure Disk Azure File Cinder GCE Persistent Disk iSCSI Local volume NFS Red Hat OpenShift Data Foundation (Ceph RBD only) VMware vSphere Note Fibre Channel and HostPath PVs do not support mount options. Additional resources ReadWriteMany vSphere volume support 3.4. Persistent volume claims Each PersistentVolumeClaim object contains a spec and status , which is the specification and status of the persistent volume claim (PVC), for example: PersistentVolumeClaim object definition example kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim 1 spec: accessModes: - ReadWriteOnce 2 resources: requests: storage: 8Gi 3 storageClassName: gold 4 status: ... 1 Name of the PVC 2 The access mode, defining the read-write and mount permissions 3 The amount of storage available to the PVC 4 Name of the StorageClass required by the claim 3.4.1. Storage classes Claims can optionally request a specific storage class by specifying the storage class’s name in the storageClassName attribute. Only PVs of the requested class, ones with the same storageClassName as the PVC, can be bound to the PVC. The cluster administrator can configure dynamic provisioners to service one or more storage classes. The cluster administrator can create a PV on demand that matches the specifications in the PVC. Important The Cluster Storage Operator might install a default storage class depending on the platform in use. This storage class is owned and controlled by the operator. It cannot be deleted or modified beyond defining annotations and labels. If different behavior is desired, you must define a custom storage class. The cluster administrator can also set a default storage class for all PVCs. When a default storage class is configured, the PVC must explicitly ask for StorageClass or storageClassName annotations set to \"\" to be bound to a PV without a storage class. Note If more than one storage class is marked as default, a PVC can only be created if the storageClassName is explicitly specified. Therefore, only one storage class should be set as the default. 3.4.2. Access modes Claims use the same conventions as volumes when requesting storage with specific access modes. 3.4.3. Resources Claims, such as pods, can request specific quantities of a resource. In this case, the request is for storage. The same resource model applies to volumes and claims. 3.4.4. Claims as volumes Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the pod using the claim. The cluster finds the claim in the pod’s namespace and uses it to get the PersistentVolume backing the claim. The volume is mounted to the host and into the pod, for example: Mount volume to the host and into the pod example kind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \"/var/www/html\" 1 name: mypd 2 volumes: - name: mypd persistentVolumeClaim: claimName: myclaim 3 1 Path to mount the volume inside the pod. 2 Name of the volume to mount. Do not mount to the container root, / , or any path that is the same in the host and the container. This can corrupt your host system if the container is sufficiently privileged, such as the host /dev/pts files. It is safe to mount the host by using /host . 3 Name of the PVC, that exists in the same namespace, to use. 3.5. Block volume support OpenShift Container Platform can statically provision raw block volumes. These volumes do not have a file system, and can provide performance benefits for applications that either write to the disk directly or implement their own storage service. Raw block volumes are provisioned by specifying volumeMode: Block in the PV and PVC specification. Important Pods using raw block volumes must be configured to allow privileged containers. The following table displays which volume plugins support block volumes. Table 3.4. Block volume support Volume Plugin Manually provisioned Dynamically provisioned Fully supported AliCloud Disk ✅ ✅ ✅ AWS EBS ✅ ✅ ✅ AWS EFS Azure Disk ✅ ✅ ✅ Azure File Cinder ✅ ✅ ✅ Fibre Channel ✅ ✅ GCP ✅ ✅ ✅ HostPath IBM VPC Disk ✅ ✅ ✅ iSCSI ✅ ✅ Local volume ✅ ✅ NFS Red Hat OpenShift Data Foundation ✅ ✅ ✅ VMware vSphere ✅ ✅ ✅ Important Using any of the block volumes that can be provisioned manually, but are not provided as fully supported, is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope . 3.5.1. Block volume examples PV example apiVersion: v1 kind: PersistentVolume metadata: name: block-pv spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce volumeMode: Block 1 persistentVolumeReclaimPolicy: Retain fc: targetWWNs: [\"50060e801049cfd1\"] lun: 0 readOnly: false 1 volumeMode must be set to Block to indicate that this PV is a raw block volume. PVC example apiVersion: v1 kind: PersistentVolumeClaim metadata: name: block-pvc spec: accessModes: - ReadWriteOnce volumeMode: Block 1 resources: requests: storage: 10Gi 1 volumeMode must be set to Block to indicate that a raw block PVC is requested. Pod specification example apiVersion: v1 kind: Pod metadata: name: pod-with-block-volume spec: containers: - name: fc-container image: fedora:26 command: [\"/bin/sh\", \"-c\"] args: [ \"tail -f /dev/null\" ] volumeDevices: 1 - name: data devicePath: /dev/xvda 2 volumes: - name: data persistentVolumeClaim: claimName: block-pvc 3 1 volumeDevices , instead of volumeMounts , is used for block devices. Only PersistentVolumeClaim sources can be used with raw block volumes. 2 devicePath , instead of mountPath , represents the path to the physical device where the raw block is mapped to the system. 3 The volume source must be of type persistentVolumeClaim and must match the name of the PVC as expected. Table 3.5. Accepted values for volumeMode Value Default Filesystem Yes Block No Table 3.6. Binding scenarios for block volumes PV volumeMode PVC volumeMode Binding result Filesystem Filesystem Bind Unspecified Unspecified Bind Filesystem Unspecified Bind Unspecified Filesystem Bind Block Block Bind Unspecified Block No Bind Block Unspecified No Bind Filesystem Block No Bind Block Filesystem No Bind Important Unspecified values result in the default value of Filesystem . 3.6. Using fsGroup to reduce pod timeouts If a storage volume contains many files (~1,000,000 or greater), you may experience pod timeouts. This can occur because, by default, OpenShift Container Platform recursively changes ownership and permissions for the contents of each volume to match the fsGroup specified in a pod’s securityContext when that volume is mounted. For large volumes, checking and changing ownership and permissions can be time consuming, slowing pod startup. You can use the fsGroupChangePolicy field inside a securityContext to control the way that OpenShift Container Platform checks and manages ownership and permissions for a volume. fsGroupChangePolicy defines behavior for changing ownership and permission of the volume before being exposed inside a pod. This field only applies to volume types that support fsGroup -controlled ownership and permissions. This field has two possible values: OnRootMismatch : Only change permissions and ownership if permission and ownership of root directory does not match with expected permissions of the volume. This can help shorten the time it takes to change ownership and permission of a volume to reduce pod timeouts. Always : Always change permission and ownership of the volume when a volume is mounted. fsGroupChangePolicy example securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 fsGroupChangePolicy: \"OnRootMismatch\" 1 ... 1 OnRootMismatch specifies skipping recursive permission change, thus helping to avoid pod timeout problems. Note The fsGroupChangePolicyfield has no effect on ephemeral volume types, such as secret, configMap, and emptydir. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/storage/understanding-persistent-storage"}
{"title": "Integrating Oracle Cloud data into cost management", "content": "Integrating Oracle Cloud data into cost management Cost Management Service 1-latest Learn how to add and configure your Oracle Cloud integration Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/integrating_oracle_cloud_data_into_cost_management/index"}
{"title": "Package manifest", "content": "Package manifest Red Hat Enterprise Linux 8 Package listing for Red Hat Enterprise Linux 8 Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/package_manifest/index"}
{"title": "Chapter 2. Understanding Operators", "content": "Chapter 2. Understanding Operators 2.1. What are Operators? Conceptually, Operators take human operational knowledge and encode it into software that is more easily shared with consumers. Operators are pieces of software that ease the operational complexity of running another piece of software. They act like an extension of the software vendor’s engineering team, monitoring a Kubernetes environment (such as OpenShift Container Platform) and using its current state to make decisions in real time. Advanced Operators are designed to handle upgrades seamlessly, react to failures automatically, and not take shortcuts, like skipping a software backup process to save time. More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application. A Kubernetes application is an app that is both deployed on Kubernetes and managed using the Kubernetes APIs and kubectl or oc tooling. To be able to make the most of Kubernetes, you require a set of cohesive APIs to extend in order to service and manage your apps that run on Kubernetes. Think of Operators as the runtime that manages this type of app on Kubernetes. 2.1.1. Why use Operators? Operators provide: Repeatability of installation and upgrade. Constant health checks of every system component. Over-the-air (OTA) updates for OpenShift components and ISV content. A place to encapsulate knowledge from field engineers and spread it to all users, not just one or two. Why deploy on Kubernetes? Kubernetes (and by extension, OpenShift Container Platform) contains all of the primitives needed to build complex distributed systems – secret handling, load balancing, service discovery, autoscaling – that work across on-premises and cloud providers. Why manage your app with Kubernetes APIs and kubectl tooling? These APIs are feature rich, have clients for all platforms and plug into the cluster’s access control/auditing. An Operator uses the Kubernetes extension mechanism, custom resource definitions (CRDs), so your custom object, for example MongoDB , looks and acts just like the built-in, native Kubernetes objects. How do Operators compare with service brokers? A service broker is a step towards programmatic discovery and deployment of an app. However, because it is not a long running process, it cannot execute Day 2 operations like upgrade, failover, or scaling. Customizations and parameterization of tunables are provided at install time, versus an Operator that is constantly watching the current state of your cluster. Off-cluster services are a good match for a service broker, although Operators exist for these as well. 2.1.2. Operator Framework The Operator Framework is a family of tools and capabilities to deliver on the customer experience described above. It is not just about writing code; testing, delivering, and updating Operators is just as important. The Operator Framework components consist of open source tools to tackle these problems: Operator SDK The Operator SDK assists Operator authors in bootstrapping, building, testing, and packaging their own Operator based on their expertise without requiring knowledge of Kubernetes API complexities. Operator Lifecycle Manager Operator Lifecycle Manager (OLM) controls the installation, upgrade, and role-based access control (RBAC) of Operators in a cluster. Deployed by default in . Operator Registry The Operator Registry stores cluster service versions (CSVs) and custom resource definitions (CRDs) for creation in a cluster and stores Operator metadata about packages and channels. It runs in a Kubernetes or OpenShift cluster to provide this Operator catalog data to OLM. OperatorHub OperatorHub is a web console for cluster administrators to discover and select Operators to install on their cluster. It is deployed by default in OpenShift Container Platform. These tools are designed to be composable, so you can use any that are useful to you. 2.1.3. Operator maturity model The level of sophistication of the management logic encapsulated within an Operator can vary. This logic is also in general highly dependent on the type of the service represented by the Operator. One can however generalize the scale of the maturity of the encapsulated operations of an Operator for certain set of capabilities that most Operators can include. To this end, the following Operator maturity model defines five phases of maturity for generic day two operations of an Operator: Figure 2.1. Operator maturity model The above model also shows how these capabilities can best be developed through the Helm, Go, and Ansible capabilities of the Operator SDK. 2.2. Operator Framework packaging format This guide outlines the packaging format for Operators supported by Operator Lifecycle Manager (OLM) in OpenShift Container Platform. Note Support for the legacy package manifest format for Operators is removed in  and later. Existing Operator projects in the package manifest format can be migrated to the bundle format by using the Operator SDK pkgman-to-bundle command. See Migrating package manifest projects to bundle format for more details. 2.2.1. Bundle format The bundle format for Operators is a packaging format introduced by the Operator Framework. To improve scalability and to better enable upstream users hosting their own catalogs, the bundle format specification simplifies the distribution of Operator metadata. An Operator bundle represents a single version of an Operator. On-disk bundle manifests are containerized and shipped as a bundle image , which is a non-runnable container image that stores the Kubernetes manifests and Operator metadata. Storage and distribution of the bundle image is then managed using existing container tools like podman and docker and container registries such as Quay. Operator metadata can include: Information that identifies the Operator, for example its name and version. Additional information that drives the UI, for example its icon and some example custom resources (CRs). Required and provided APIs. Related images. When loading manifests into the Operator Registry database, the following requirements are validated: The bundle must have at least one channel defined in the annotations. Every bundle has exactly one cluster service version (CSV). If a CSV owns a custom resource definition (CRD), that CRD must exist in the bundle. 2.2.1.1. Manifests Bundle manifests refer to a set of Kubernetes manifests that define the deployment and RBAC model of the Operator. A bundle includes one CSV per directory and typically the CRDs that define the owned APIs of the CSV in its /manifests directory. Example bundle format layout etcd ├── manifests │ ├── etcdcluster.crd.yaml │ └── etcdoperator.clusterserviceversion.yaml │ └── secret.yaml │ └── configmap.yaml └── metadata └── annotations.yaml └── dependencies.yaml Additionally supported objects The following object types can also be optionally included in the /manifests directory of a bundle: Supported optional object types ClusterRole ClusterRoleBinding ConfigMap ConsoleCLIDownload ConsoleLink ConsoleQuickStart ConsoleYamlSample PodDisruptionBudget PriorityClass PrometheusRule Role RoleBinding Secret Service ServiceAccount ServiceMonitor VerticalPodAutoscaler When these optional objects are included in a bundle, Operator Lifecycle Manager (OLM) can create them from the bundle and manage their lifecycle along with the CSV: Lifecycle for optional objects When the CSV is deleted, OLM deletes the optional object. When the CSV is upgraded: If the name of the optional object is the same, OLM updates it in place. If the name of the optional object has changed between versions, OLM deletes and recreates it. 2.2.1.2. Annotations A bundle also includes an annotations.yaml file in its /metadata directory. This file defines higher level aggregate data that helps describe the format and package information about how the bundle should be added into an index of bundles: Example annotations.yaml annotations: operators.operatorframework.io.bundle.mediatype.v1: \"registry+v1\" 1 operators.operatorframework.io.bundle.manifests.v1: \"manifests/\" 2 operators.operatorframework.io.bundle.metadata.v1: \"metadata/\" 3 operators.operatorframework.io.bundle.package.v1: \"test-operator\" 4 operators.operatorframework.io.bundle.channels.v1: \"beta,stable\" 5 operators.operatorframework.io.bundle.channel.default.v1: \"stable\" 6 1 The media type or format of the Operator bundle. The registry+v1 format means it contains a CSV and its associated Kubernetes objects. 2 The path in the image to the directory that contains the Operator manifests. This label is reserved for future use and currently defaults to manifests/ . The value manifests.v1 implies that the bundle contains Operator manifests. 3 The path in the image to the directory that contains metadata files about the bundle. This label is reserved for future use and currently defaults to metadata/ . The value metadata.v1 implies that this bundle has Operator metadata. 4 The package name of the bundle. 5 The list of channels the bundle is subscribing to when added into an Operator Registry. 6 The default channel an Operator should be subscribed to when installed from a registry. Note In case of a mismatch, the annotations.yaml file is authoritative because the on-cluster Operator Registry that relies on these annotations only has access to this file. 2.2.1.3. Dependencies The dependencies of an Operator are listed in a dependencies.yaml file in the metadata/ folder of a bundle. This file is optional and currently only used to specify explicit Operator-version dependencies. The dependency list contains a type field for each item to specify what kind of dependency this is. The following types of Operator dependencies are supported: olm.package This type indicates a dependency for a specific Operator version. The dependency information must include the package name and the version of the package in semver format. For example, you can specify an exact version such as 0.5.2 or a range of versions such as >0.5.1 . olm.gvk With this type, the author can specify a dependency with group/version/kind (GVK) information, similar to existing CRD and API-based usage in a CSV. This is a path to enable Operator authors to consolidate all dependencies, API or explicit versions, to be in the same place. olm.constraint This type declares generic constraints on arbitrary Operator properties. In the following example, dependencies are specified for a Prometheus Operator and etcd CRDs: Example dependencies.yaml file dependencies: - type: olm.package value: packageName: prometheus version: \">0.27.0\" - type: olm.gvk value: group: etcd.database.coreos.com kind: EtcdCluster version: v1beta2 Additional resources Operator Lifecycle Manager dependency resolution 2.2.1.4. About the opm CLI The opm CLI tool is provided by the Operator Framework for use with the Operator bundle format. This tool allows you to create and maintain catalogs of Operators from a list of Operator bundles that are similar to software repositories. The result is a container image which can be stored in a container registry and then installed on a cluster. A catalog contains a database of pointers to Operator manifest content that can be queried through an included API that is served when the container image is run. On OpenShift Container Platform, Operator Lifecycle Manager (OLM) can reference the image in a catalog source, defined by a CatalogSource object, which polls the image at regular intervals to enable frequent updates to installed Operators on the cluster. See CLI tools for steps on installing the opm CLI. 2.2.2. File-based catalogs File-based catalogs are the latest iteration of the catalog format in Operator Lifecycle Manager (OLM). It is a plain text-based (JSON or YAML) and declarative config evolution of the earlier SQLite database format, and it is fully backwards compatible. The goal of this format is to enable Operator catalog editing, composability, and extensibility. Editing With file-based catalogs, users interacting with the contents of a catalog are able to make direct changes to the format and verify that their changes are valid. Because this format is plain text JSON or YAML, catalog maintainers can easily manipulate catalog metadata by hand or with widely known and supported JSON or YAML tooling, such as the jq CLI. This editability enables the following features and user-defined extensions: Promoting an existing bundle to a new channel Changing the default channel of a package Custom algorithms for adding, updating, and removing upgrade edges Composability File-based catalogs are stored in an arbitrary directory hierarchy, which enables catalog composition. For example, consider two separate file-based catalog directories: catalogA and catalogB . A catalog maintainer can create a new combined catalog by making a new directory catalogC and copying catalogA and catalogB into it. This composability enables decentralized catalogs. The format permits Operator authors to maintain Operator-specific catalogs, and it permits maintainers to trivially build a catalog composed of individual Operator catalogs. File-based catalogs can be composed by combining multiple other catalogs, by extracting subsets of one catalog, or a combination of both of these. Note Duplicate packages and duplicate bundles within a package are not permitted. The opm validate command returns an error if any duplicates are found. Because Operator authors are most familiar with their Operator, its dependencies, and its upgrade compatibility, they are able to maintain their own Operator-specific catalog and have direct control over its contents. With file-based catalogs, Operator authors own the task of building and maintaining their packages in a catalog. Composite catalog maintainers, however, only own the task of curating the packages in their catalog and publishing the catalog to users. Extensibility The file-based catalog specification is a low-level representation of a catalog. While it can be maintained directly in its low-level form, catalog maintainers can build interesting extensions on top that can be used by their own custom tooling to make any number of mutations. For example, a tool could translate a high-level API, such as (mode=semver) , down to the low-level, file-based catalog format for upgrade edges. Or a catalog maintainer might need to customize all of the bundle metadata by adding a new property to bundles that meet a certain criteria. While this extensibility allows for additional official tooling to be developed on top of the low-level APIs for future OpenShift Container Platform releases, the major benefit is that catalog maintainers have this capability as well. Important As of , the default Red Hat-provided Operator catalog releases in the file-based catalog format. The default Red Hat-provided Operator catalogs for  through 4.10 released in the deprecated SQLite database format. The opm subcommands, flags, and functionality related to the SQLite database format are also deprecated and will be removed in a future release. The features are still supported and must be used for catalogs that use the deprecated SQLite database format. Many of the opm subcommands and flags for working with the SQLite database format, such as opm index prune , do not work with the file-based catalog format. For more information about working with file-based catalogs, see Managing custom catalogs and Mirroring images for a disconnected installation using the oc-mirror plugin . 2.2.2.1. Directory structure File-based catalogs can be stored and loaded from directory-based file systems. The opm CLI loads the catalog by walking the root directory and recursing into subdirectories. The CLI attempts to load every file it finds and fails if any errors occur. Non-catalog files can be ignored using .indexignore files, which have the same rules for patterns and precedence as .gitignore files. Example .indexignore file # Ignore everything except non-object .json and .yaml files **/* !*.json !*.yaml **/objects/*.json **/objects/*.yaml Catalog maintainers have the flexibility to choose their desired layout, but it is recommended to store each package’s file-based catalog blobs in separate subdirectories. Each individual file can be either JSON or YAML; it is not necessary for every file in a catalog to use the same format. Basic recommended structure catalog ├── packageA │ └── index.yaml ├── packageB │ ├── .indexignore │ ├── index.yaml │ └── objects │ └── packageB.v0.1.0.clusterserviceversion.yaml └── packageC └── index.json This recommended structure has the property that each subdirectory in the directory hierarchy is a self-contained catalog, which makes catalog composition, discovery, and navigation trivial file system operations. The catalog could also be included in a parent catalog by copying it into the parent catalog’s root directory. 2.2.2.2. Schemas File-based catalogs use a format, based on the CUE language specification , that can be extended with arbitrary schemas. The following _Meta CUE schema defines the format that all file-based catalog blobs must adhere to: _Meta schema _Meta: { // schema is required and must be a non-empty string schema: string & !=\"\" // package is optional, but if it's defined, it must be a non-empty string package?: string & !=\"\" // properties is optional, but if it's defined, it must be a list of 0 or more properties properties?: [... #Property] } #Property: { // type is required type: string & !=\"\" // value is required, and it must not be null value: !=null } Note No CUE schemas listed in this specification should be considered exhaustive. The opm validate command has additional validations that are difficult or impossible to express concisely in CUE. An Operator Lifecycle Manager (OLM) catalog currently uses three schemas ( olm.package , olm.channel , and olm.bundle ), which correspond to OLM’s existing package and bundle concepts. Each Operator package in a catalog requires exactly one olm.package blob, at least one olm.channel blob, and one or more olm.bundle blobs. Note All olm.* schemas are reserved for OLM-defined schemas. Custom schemas must use a unique prefix, such as a domain that you own. 2.2.2.2.1. olm.package schema The olm.package schema defines package-level metadata for an Operator. This includes its name, description, default channel, and icon. Example 2.1. olm.package schema #Package: { schema: \"olm.package\" // Package name name: string & !=\"\" // A description of the package description?: string // The package's default channel defaultChannel: string & !=\"\" // An optional icon icon?: { base64data: string mediatype: string } } 2.2.2.2.2. olm.channel schema The olm.channel schema defines a channel within a package, the bundle entries that are members of the channel, and the upgrade edges for those bundles. A bundle can included as an entry in multiple olm.channel blobs, but it can have only one entry per channel. It is valid for an entry’s replaces value to reference another bundle name that cannot be found in this catalog or another catalog. However, all other channel invariants must hold true, such as a channel not having multiple heads. Example 2.2. olm.channel schema #Channel: { schema: \"olm.channel\" package: string & !=\"\" name: string & !=\"\" entries: [...#ChannelEntry] } #ChannelEntry: { // name is required. It is the name of an `olm.bundle` that // is present in the channel. name: string & !=\"\" // replaces is optional. It is the name of bundle that is replaced // by this entry. It does not have to be present in the entry list. replaces?: string & !=\"\" // skips is optional. It is a list of bundle names that are skipped by // this entry. The skipped bundles do not have to be present in the // entry list. skips?: [...string & !=\"\"] // skipRange is optional. It is the semver range of bundle versions // that are skipped by this entry. skipRange?: string & !=\"\" } 2.2.2.2.3. olm.bundle schema Example 2.3. olm.bundle schema #Bundle: { schema: \"olm.bundle\" package: string & !=\"\" name: string & !=\"\" image: string & !=\"\" properties: [...#Property] relatedImages?: [...#RelatedImage] } #Property: { // type is required type: string & !=\"\" // value is required, and it must not be null value: !=null } #RelatedImage: { // image is the image reference image: string & !=\"\" // name is an optional descriptive name for an image that // helps identify its purpose in the context of the bundle name?: string & !=\"\" } 2.2.2.3. Properties Properties are arbitrary pieces of metadata that can be attached to file-based catalog schemas. The type field is a string that effectively specifies the semantic and syntactic meaning of the value field. The value can be any arbitrary JSON or YAML. OLM defines a handful of property types, again using the reserved olm.* prefix. 2.2.2.3.1. olm.package property The olm.package property defines the package name and version. This is a required property on bundles, and there must be exactly one of these properties. The packageName field must match the bundle’s first-class package field, and the version field must be a valid semantic version. Example 2.4. olm.package property #PropertyPackage: { type: \"olm.package\" value: { packageName: string & !=\"\" version: string & !=\"\" } } 2.2.2.3.2. olm.gvk property The olm.gvk property defines the group/version/kind (GVK) of a Kubernetes API that is provided by this bundle. This property is used by OLM to resolve a bundle with this property as a dependency for other bundles that list the same GVK as a required API. The GVK must adhere to Kubernetes GVK validations. Example 2.5. olm.gvk property #PropertyGVK: { type: \"olm.gvk\" value: { group: string & !=\"\" version: string & !=\"\" kind: string & !=\"\" } } 2.2.2.3.3. olm.package.required The olm.package.required property defines the package name and version range of another package that this bundle requires. For every required package property a bundle lists, OLM ensures there is an Operator installed on the cluster for the listed package and in the required version range. The versionRange field must be a valid semantic version (semver) range. Example 2.6. olm.package.required property #PropertyPackageRequired: { type: \"olm.package.required\" value: { packageName: string & !=\"\" versionRange: string & !=\"\" } } 2.2.2.3.4. olm.gvk.required The olm.gvk.required property defines the group/version/kind (GVK) of a Kubernetes API that this bundle requires. For every required GVK property a bundle lists, OLM ensures there is an Operator installed on the cluster that provides it. The GVK must adhere to Kubernetes GVK validations. Example 2.7. olm.gvk.required property #PropertyGVKRequired: { type: \"olm.gvk.required\" value: { group: string & !=\"\" version: string & !=\"\" kind: string & !=\"\" } } 2.2.2.4. Example catalog With file-based catalogs, catalog maintainers can focus on Operator curation and compatibility. Because Operator authors have already produced Operator-specific catalogs for their Operators, catalog maintainers can build their catalog by rendering each Operator catalog into a subdirectory of the catalog’s root directory. There are many possible ways to build a file-based catalog; the following steps outline a simple approach: Maintain a single configuration file for the catalog, containing image references for each Operator in the catalog: Example catalog configuration file name: community-operators repo: quay.io/community-operators/catalog tag: latest references: - name: etcd-operator image: quay.io/etcd-operator/index@sha256:5891b5b522d5df086d0ff0b110fbd9d21bb4fc7163af34d08286a2e846f6be03 - name: prometheus-operator image: quay.io/prometheus-operator/index@sha256:e258d248fda94c63753607f7c4494ee0fcbe92f1a76bfdac795c9d84101eb317 Run a script that parses the configuration file and creates a new catalog from its references: Example script name=$(yq eval '.name' catalog.yaml) mkdir \"$name\" yq eval '.name + \"/\" + .references[].name' catalog.yaml | xargs mkdir for l in $(yq e '.name as $catalog | .references[] | .image + \"|\" + $catalog + \"/\" + .name + \"/index.yaml\"' catalog.yaml); do image=$(echo $l | cut -d'|' -f1) file=$(echo $l | cut -d'|' -f2) opm render \"$image\" > \"$file\" done opm alpha generate dockerfile \"$name\" indexImage=$(yq eval '.repo + \":\" + .tag' catalog.yaml) docker build -t \"$indexImage\" -f \"$name.Dockerfile\" . docker push \"$indexImage\" 2.2.2.5. Guidelines Consider the following guidelines when maintaining file-based catalogs. 2.2.2.5.1. Immutable bundles The general advice with Operator Lifecycle Manager (OLM) is that bundle images and their metadata should be treated as immutable. If a broken bundle has been pushed to a catalog, you must assume that at least one of your users has upgraded to that bundle. Based on that assumption, you must release another bundle with an upgrade edge from the broken bundle to ensure users with the broken bundle installed receive an upgrade. OLM will not reinstall an installed bundle if the contents of that bundle are updated in the catalog. However, there are some cases where a change in the catalog metadata is preferred: Channel promotion: If you already released a bundle and later decide that you would like to add it to another channel, you can add an entry for your bundle in another olm.channel blob. New upgrade edges: If you release a new 1.2.z bundle version, for example 1.2.4 , but 1.3.0 is already released, you can update the catalog metadata for 1.3.0 to skip 1.2.4 . 2.2.2.5.2. Source control Catalog metadata should be stored in source control and treated as the source of truth. Updates to catalog images should include the following steps: Update the source-controlled catalog directory with a new commit. Build and push the catalog image. Use a consistent tagging taxonomy, such as :latest or :<target_cluster_version> , so that users can receive updates to a catalog as they become available. 2.2.2.6. CLI usage For instructions about creating file-based catalogs by using the opm CLI, see Managing custom catalogs . For reference documentation about the opm CLI commands related to managing file-based catalogs, see CLI tools . 2.2.2.7. Automation Operator authors and catalog maintainers are encouraged to automate their catalog maintenance with CI/CD workflows. Catalog maintainers can further improve on this by building GitOps automation to accomplish the following tasks: Check that pull request (PR) authors are permitted to make the requested changes, for example by updating their package’s image reference. Check that the catalog updates pass the opm validate command. Check that the updated bundle or catalog image references exist, the catalog images run successfully in a cluster, and Operators from that package can be successfully installed. Automatically merge PRs that pass the previous checks. Automatically rebuild and republish the catalog image. 2.3. Operator Framework glossary of common terms This topic provides a glossary of common terms related to the Operator Framework, including Operator Lifecycle Manager (OLM) and the Operator SDK. 2.3.1. Common Operator Framework terms 2.3.1.1. Bundle In the bundle format, a bundle is a collection of an Operator CSV, manifests, and metadata. Together, they form a unique version of an Operator that can be installed onto the cluster. 2.3.1.2. Bundle image In the bundle format, a bundle image is a container image that is built from Operator manifests and that contains one bundle. Bundle images are stored and distributed by Open Container Initiative (OCI) spec container registries, such as Quay.io or DockerHub. 2.3.1.3. Catalog source A catalog source represents a store of metadata that OLM can query to discover and install Operators and their dependencies. 2.3.1.4. Channel A channel defines a stream of updates for an Operator and is used to roll out updates for subscribers. The head points to the latest version of that channel. For example, a stable channel would have all stable versions of an Operator arranged from the earliest to the latest. An Operator can have several channels, and a subscription binding to a certain channel would only look for updates in that channel. 2.3.1.5. Channel head A channel head refers to the latest known update in a particular channel. 2.3.1.6. Cluster service version A cluster service version (CSV) is a YAML manifest created from Operator metadata that assists OLM in running the Operator in a cluster. It is the metadata that accompanies an Operator container image, used to populate user interfaces with information such as its logo, description, and version. It is also a source of technical information that is required to run the Operator, like the RBAC rules it requires and which custom resources (CRs) it manages or depends on. 2.3.1.7. Dependency An Operator may have a dependency on another Operator being present in the cluster. For example, the Vault Operator has a dependency on the etcd Operator for its data persistence layer. OLM resolves dependencies by ensuring that all specified versions of Operators and CRDs are installed on the cluster during the installation phase. This dependency is resolved by finding and installing an Operator in a catalog that satisfies the required CRD API, and is not related to packages or bundles. 2.3.1.8. Index image In the bundle format, an index image refers to an image of a database (a database snapshot) that contains information about Operator bundles including CSVs and CRDs of all versions. This index can host a history of Operators on a cluster and be maintained by adding or removing Operators using the opm CLI tool. 2.3.1.9. Install plan An install plan is a calculated list of resources to be created to automatically install or upgrade a CSV. 2.3.1.10. Multitenancy A tenant in OpenShift Container Platform is a user or group of users that share common access and privileges for a set of deployed workloads, typically represented by a namespace or project. You can use tenants to provide a level of isolation between different groups or teams. When a cluster is shared by multiple users or groups, it is considered a multitenant cluster. 2.3.1.11. Operator group An Operator group configures all Operators deployed in the same namespace as the OperatorGroup object to watch for their CR in a list of namespaces or cluster-wide. 2.3.1.12. Package In the bundle format, a package is a directory that encloses all released history of an Operator with each version. A released version of an Operator is described in a CSV manifest alongside the CRDs. 2.3.1.13. Registry A registry is a database that stores bundle images of Operators, each with all of its latest and historical versions in all channels. 2.3.1.14. Subscription A subscription keeps CSVs up to date by tracking a channel in a package. 2.3.1.15. Update graph An update graph links versions of CSVs together, similar to the update graph of any other packaged software. Operators can be installed sequentially, or certain versions can be skipped. The update graph is expected to grow only at the head with newer versions being added. 2.4. Operator Lifecycle Manager (OLM) 2.4.1. Operator Lifecycle Manager concepts and resources This guide provides an overview of the concepts that drive Operator Lifecycle Manager (OLM) in OpenShift Container Platform. 2.4.1.1. What is Operator Lifecycle Manager? Operator Lifecycle Manager (OLM) helps users install, update, and manage the lifecycle of Kubernetes native applications (Operators) and their associated services running across their OpenShift Container Platform clusters. It is part of the Operator Framework , an open source toolkit designed to manage Operators in an effective, automated, and scalable way. Figure 2.2. Operator Lifecycle Manager workflow OLM runs by default in , which aids cluster administrators in installing, upgrading, and granting access to Operators running on their cluster. The OpenShift Container Platform web console provides management screens for cluster administrators to install Operators, as well as grant specific projects access to use the catalog of Operators available on the cluster. For developers, a self-service experience allows provisioning and configuring instances of databases, monitoring, and big data services without having to be subject matter experts, because the Operator has that knowledge baked into it. 2.4.1.2. OLM resources The following custom resource definitions (CRDs) are defined and managed by Operator Lifecycle Manager (OLM): Table 2.1. CRDs managed by OLM and Catalog Operators Resource Short name Description ClusterServiceVersion (CSV) csv Application metadata. For example: name, version, icon, required resources. CatalogSource catsrc A repository of CSVs, CRDs, and packages that define an application. Subscription sub Keeps CSVs up to date by tracking a channel in a package. InstallPlan ip Calculated list of resources to be created to automatically install or upgrade a CSV. OperatorGroup og Configures all Operators deployed in the same namespace as the OperatorGroup object to watch for their custom resource (CR) in a list of namespaces or cluster-wide. OperatorConditions - Creates a communication channel between OLM and an Operator it manages. Operators can write to the Status.Conditions array to communicate complex states to OLM. 2.4.1.2.1. Cluster service version A cluster service version (CSV) represents a specific version of a running Operator on an OpenShift Container Platform cluster. It is a YAML manifest created from Operator metadata that assists Operator Lifecycle Manager (OLM) in running the Operator in the cluster. OLM requires this metadata about an Operator to ensure that it can be kept running safely on a cluster, and to provide information about how updates should be applied as new versions of the Operator are published. This is similar to packaging software for a traditional operating system; think of the packaging step for OLM as the stage at which you make your rpm , deb , or apk bundle. A CSV includes the metadata that accompanies an Operator container image, used to populate user interfaces with information such as its name, version, description, labels, repository link, and logo. A CSV is also a source of technical information required to run the Operator, such as which custom resources (CRs) it manages or depends on, RBAC rules, cluster requirements, and install strategies. This information tells OLM how to create required resources and set up the Operator as a deployment. 2.4.1.2.2. Catalog source A catalog source represents a store of metadata, typically by referencing an index image stored in a container registry. Operator Lifecycle Manager (OLM) queries catalog sources to discover and install Operators and their dependencies. OperatorHub in the OpenShift Container Platform web console also displays the Operators provided by catalog sources. Tip Cluster administrators can view the full list of Operators provided by an enabled catalog source on a cluster by using the Administration Cluster Settings Configuration OperatorHub page in the web console. The spec of a CatalogSource object indicates how to construct a pod or how to communicate with a service that serves the Operator Registry gRPC API. Example 2.8. Example CatalogSource object ﻿apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: generation: 1 name: example-catalog 1 namespace: openshift-marketplace 2 annotations: olm.catalogImageTemplate: 3 \"quay.io/example-org/example-catalog:v{kube_major_version}.{kube_minor_version}.{kube_patch_version}\" spec: displayName: Example Catalog 4 image: quay.io/example-org/example-catalog:v1 5 priority: -400 6 publisher: Example Org sourceType: grpc 7 grpcPodConfig: nodeSelector: 8 custom_label: <label> priorityClassName: system-cluster-critical 9 tolerations: 10 - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" updateStrategy: registryPoll: 11 interval: 30m0s status: connectionState: address: example-catalog.openshift-marketplace.svc:50051 lastConnect: 2021-08-26T18:14:31Z lastObservedState: READY 12 latestImageRegistryPoll: 2021-08-26T18:46:25Z 13 registryService: 14 createdAt: 2021-08-26T16:16:37Z port: 50051 protocol: grpc serviceName: example-catalog serviceNamespace: openshift-marketplace 1 Name for the CatalogSource object. This value is also used as part of the name for the related pod that is created in the requested namespace. 2 Namespace to create the catalog in. To make the catalog available cluster-wide in all namespaces, set this value to openshift-marketplace . The default Red Hat-provided catalog sources also use the openshift-marketplace namespace. Otherwise, set the value to a specific namespace to make the Operator only available in that namespace. 3 Optional: To avoid cluster upgrades potentially leaving Operator installations in an unsupported state or without a continued update path, you can enable automatically changing your Operator catalog’s index image version as part of cluster upgrades. Set the olm.catalogImageTemplate annotation to your index image name and use one or more of the Kubernetes cluster version variables as shown when constructing the template for the image tag. The annotation overwrites the spec.image field at run time. See the \"Image template for custom catalog sources\" section for more details. 4 Display name for the catalog in the web console and CLI. 5 Index image for the catalog. Optionally, can be omitted when using the olm.catalogImageTemplate annotation, which sets the pull spec at run time. 6 Weight for the catalog source. OLM uses the weight for prioritization during dependency resolution. A higher weight indicates the catalog is preferred over lower-weighted catalogs. 7 Source types include the following: grpc with an image reference: OLM pulls the image and runs the pod, which is expected to serve a compliant API. grpc with an address field: OLM attempts to contact the gRPC API at the given address. This should not be used in most cases. configmap : OLM parses config map data and runs a pod that can serve the gRPC API over it. 8 Optional: For grpc type catalog sources, overrides the default node selector for the pod serving the content in spec.image , if defined. 9 Optional: For grpc type catalog sources, overrides the default priority class name for the pod serving the content in spec.image , if defined. Kubernetes provides system-cluster-critical and system-node-critical priority classes by default. Setting the field to empty ( \"\" ) assigns the pod the default priority. Other priority classes can be defined manually. 10 Optional: For grpc type catalog sources, overrides the default tolerations for the pod serving the content in spec.image , if defined. 11 Automatically check for new versions at a given interval to stay up-to-date. 12 Last observed state of the catalog connection. For example: READY : A connection is successfully established. CONNECTING : A connection is attempting to establish. TRANSIENT_FAILURE : A temporary problem has occurred while attempting to establish a connection, such as a timeout. The state will eventually switch back to CONNECTING and try again. See States of Connectivity in the gRPC documentation for more details. 13 Latest time the container registry storing the catalog image was polled to ensure the image is up-to-date. 14 Status information for the catalog’s Operator Registry service. Referencing the name of a CatalogSource object in a subscription instructs OLM where to search to find a requested Operator: Example 2.9. Example Subscription object referencing a catalog source apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: example-operator namespace: example-namespace spec: channel: stable name: example-operator source: example-catalog sourceNamespace: openshift-marketplace Additional resources Understanding OperatorHub Red Hat-provided Operator catalogs Adding a catalog source to a cluster Catalog priority Viewing Operator catalog source status by using the CLI Catalog source pod scheduling 2.4.1.2.2.1. Image template for custom catalog sources Operator compatibility with the underlying cluster can be expressed by a catalog source in various ways. One way, which is used for the default Red Hat-provided catalog sources, is to identify image tags for index images that are specifically created for a particular platform release, for example . During a cluster upgrade, the index image tag for the default Red Hat-provided catalog sources are updated automatically by the Cluster Version Operator (CVO) so that Operator Lifecycle Manager (OLM) pulls the updated version of the catalog. For example during an upgrade from  to 4.11, the spec.image field in the CatalogSource object for the redhat-operators catalog is updated from: registry.redhat.io/redhat/redhat-operator-index:v4.10 to: registry.redhat.io/redhat/redhat-operator-index:v4.11 However, the CVO does not automatically update image tags for custom catalogs. To ensure users are left with a compatible and supported Operator installation after a cluster upgrade, custom catalogs should also be kept updated to reference an updated index image. Starting in , cluster administrators can add the olm.catalogImageTemplate annotation in the CatalogSource object for custom catalogs to an image reference that includes a template. The following Kubernetes version variables are supported for use in the template: kube_major_version kube_minor_version kube_patch_version Note You must specify the Kubernetes cluster version and not an OpenShift Container Platform cluster version, as the latter is not currently available for templating. Provided that you have created and pushed an index image with a tag specifying the updated Kubernetes version, setting this annotation enables the index image versions in custom catalogs to be automatically changed after a cluster upgrade. The annotation value is used to set or update the image reference in the spec.image field of the CatalogSource object. This helps avoid cluster upgrades leaving Operator installations in unsupported states or without a continued update path. Important You must ensure that the index image with the updated tag, in whichever registry it is stored in, is accessible by the cluster at the time of the cluster upgrade. Example 2.10. Example catalog source with an image template apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: generation: 1 name: example-catalog namespace: openshift-marketplace annotations: olm.catalogImageTemplate: \"quay.io/example-org/example-catalog:v{kube_major_version}.{kube_minor_version}\" spec: displayName: Example Catalog image: quay.io/example-org/example-catalog:v1.24 priority: -400 publisher: Example Org Note If the spec.image field and the olm.catalogImageTemplate annotation are both set, the spec.image field is overwritten by the resolved value from the annotation. If the annotation does not resolve to a usable pull spec, the catalog source falls back to the set spec.image value. If the spec.image field is not set and the annotation does not resolve to a usable pull spec, OLM stops reconciliation of the catalog source and sets it into a human-readable error condition. For an  cluster, which uses Kubernetes 1.24, the olm.catalogImageTemplate annotation in the preceding example resolves to the following image reference: quay.io/example-org/example-catalog:v1.24 For future releases of OpenShift Container Platform, you can create updated index images for your custom catalogs that target the later Kubernetes version that is used by the later OpenShift Container Platform version. With the olm.catalogImageTemplate annotation set before the upgrade, upgrading the cluster to the later OpenShift Container Platform version would then automatically update the catalog’s index image as well. 2.4.1.2.2.2. Catalog health requirements Operator catalogs on a cluster are interchangeable from the perspective of installation resolution; a Subscription object might reference a specific catalog, but dependencies are resolved using all catalogs on the cluster. For example, if Catalog A is unhealthy, a subscription referencing Catalog A could resolve a dependency in Catalog B, which the cluster administrator might not have been expecting, because B normally had a lower catalog priority than A. As a result, OLM requires that all catalogs with a given global namespace (for example, the default openshift-marketplace namespace or a custom global namespace) are healthy. When a catalog is unhealthy, all Operator installation or update operations within its shared global namespace will fail with a CatalogSourcesUnhealthy condition. If these operations were permitted in an unhealthy state, OLM might make resolution and installation decisions that were unexpected to the cluster administrator. As a cluster administrator, if you observe an unhealthy catalog and want to consider the catalog as invalid and resume Operator installations, see the \"Removing custom catalogs\" or \"Disabling the default OperatorHub catalog sources\" sections for information about removing the unhealthy catalog. Additional resources Removing custom catalogs Disabling the default OperatorHub catalog sources 2.4.1.2.3. Subscription A subscription , defined by a Subscription object, represents an intention to install an Operator. It is the custom resource that relates an Operator to a catalog source. Subscriptions describe which channel of an Operator package to subscribe to, and whether to perform updates automatically or manually. If set to automatic, the subscription ensures Operator Lifecycle Manager (OLM) manages and upgrades the Operator to ensure that the latest version is always running in the cluster. Example Subscription object apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: example-operator namespace: example-namespace spec: channel: stable name: example-operator source: example-catalog sourceNamespace: openshift-marketplace This Subscription object defines the name and namespace of the Operator, as well as the catalog from which the Operator data can be found. The channel, such as alpha , beta , or stable , helps determine which Operator stream should be installed from the catalog source. The names of channels in a subscription can differ between Operators, but the naming scheme should follow a common convention within a given Operator. For example, channel names might follow a minor release update stream for the application provided by the Operator ( 1.2 , 1.3 ) or a release frequency ( stable , fast ). In addition to being easily visible from the OpenShift Container Platform web console, it is possible to identify when there is a newer version of an Operator available by inspecting the status of the related subscription. The value associated with the currentCSV field is the newest version that is known to OLM, and installedCSV is the version that is installed on the cluster. Additional resources Multitenancy and Operator colocation Viewing Operator subscription status by using the CLI 2.4.1.2.4. Install plan An install plan , defined by an InstallPlan object, describes a set of resources that Operator Lifecycle Manager (OLM) creates to install or upgrade to a specific version of an Operator. The version is defined by a cluster service version (CSV). To install an Operator, a cluster administrator, or a user who has been granted Operator installation permissions, must first create a Subscription object. A subscription represents the intent to subscribe to a stream of available versions of an Operator from a catalog source. The subscription then creates an InstallPlan object to facilitate the installation of the resources for the Operator. The install plan must then be approved according to one of the following approval strategies: If the subscription’s spec.installPlanApproval field is set to Automatic , the install plan is approved automatically. If the subscription’s spec.installPlanApproval field is set to Manual , the install plan must be manually approved by a cluster administrator or user with proper permissions. After the install plan is approved, OLM creates the specified resources and installs the Operator in the namespace that is specified by the subscription. Example 2.11. Example InstallPlan object apiVersion: operators.coreos.com/v1alpha1 kind: InstallPlan metadata: name: install-abcde namespace: operators spec: approval: Automatic approved: true clusterServiceVersionNames: - my-operator.v1.0.1 generation: 1 status: ... catalogSources: [] conditions: - lastTransitionTime: '2021-01-01T20:17:27Z' lastUpdateTime: '2021-01-01T20:17:27Z' status: 'True' type: Installed phase: Complete plan: - resolving: my-operator.v1.0.1 resource: group: operators.coreos.com kind: ClusterServiceVersion manifest: >- ... name: my-operator.v1.0.1 sourceName: redhat-operators sourceNamespace: openshift-marketplace version: v1alpha1 status: Created - resolving: my-operator.v1.0.1 resource: group: apiextensions.k8s.io kind: CustomResourceDefinition manifest: >- ... name: webservers.web.servers.org sourceName: redhat-operators sourceNamespace: openshift-marketplace version: v1beta1 status: Created - resolving: my-operator.v1.0.1 resource: group: '' kind: ServiceAccount manifest: >- ... name: my-operator sourceName: redhat-operators sourceNamespace: openshift-marketplace version: v1 status: Created - resolving: my-operator.v1.0.1 resource: group: rbac.authorization.k8s.io kind: Role manifest: >- ... name: my-operator.v1.0.1-my-operator-6d7cbc6f57 sourceName: redhat-operators sourceNamespace: openshift-marketplace version: v1 status: Created - resolving: my-operator.v1.0.1 resource: group: rbac.authorization.k8s.io kind: RoleBinding manifest: >- ... name: my-operator.v1.0.1-my-operator-6d7cbc6f57 sourceName: redhat-operators sourceNamespace: openshift-marketplace version: v1 status: Created ... Additional resources Multitenancy and Operator colocation Allowing non-cluster administrators to install Operators 2.4.1.2.5. Operator groups An Operator group , defined by the OperatorGroup resource, provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators. The set of target namespaces is provided by a comma-delimited string stored in the olm.targetNamespaces annotation of a cluster service version (CSV). This annotation is applied to the CSV instances of member Operators and is projected into their deployments. Additional resources Operator groups 2.4.1.2.6. Operator conditions As part of its role in managing the lifecycle of an Operator, Operator Lifecycle Manager (OLM) infers the state of an Operator from the state of Kubernetes resources that define the Operator. While this approach provides some level of assurance that an Operator is in a given state, there are many instances where an Operator might need to communicate information to OLM that could not be inferred otherwise. This information can then be used by OLM to better manage the lifecycle of the Operator. OLM provides a custom resource definition (CRD) called OperatorCondition that allows Operators to communicate conditions to OLM. There are a set of supported conditions that influence management of the Operator by OLM when present in the Spec.Conditions array of an OperatorCondition resource. Note By default, the Spec.Conditions array is not present in an OperatorCondition object until it is either added by a user or as a result of custom Operator logic. Additional resources Operator conditions 2.4.2. Operator Lifecycle Manager architecture This guide outlines the component architecture of Operator Lifecycle Manager (OLM) in OpenShift Container Platform. 2.4.2.1. Component responsibilities Operator Lifecycle Manager (OLM) is composed of two Operators: the OLM Operator and the Catalog Operator. Each of these Operators is responsible for managing the custom resource definitions (CRDs) that are the basis for the OLM framework: Table 2.2. CRDs managed by OLM and Catalog Operators Resource Short name Owner Description ClusterServiceVersion (CSV) csv OLM Application metadata: name, version, icon, required resources, installation, and so on. InstallPlan ip Catalog Calculated list of resources to be created to automatically install or upgrade a CSV. CatalogSource catsrc Catalog A repository of CSVs, CRDs, and packages that define an application. Subscription sub Catalog Used to keep CSVs up to date by tracking a channel in a package. OperatorGroup og OLM Configures all Operators deployed in the same namespace as the OperatorGroup object to watch for their custom resource (CR) in a list of namespaces or cluster-wide. Each of these Operators is also responsible for creating the following resources: Table 2.3. Resources created by OLM and Catalog Operators Resource Owner Deployments OLM ServiceAccounts (Cluster)Roles (Cluster)RoleBindings CustomResourceDefinitions (CRDs) Catalog ClusterServiceVersions 2.4.2.2. OLM Operator The OLM Operator is responsible for deploying applications defined by CSV resources after the required resources specified in the CSV are present in the cluster. The OLM Operator is not concerned with the creation of the required resources; you can choose to manually create these resources using the CLI or using the Catalog Operator. This separation of concern allows users incremental buy-in in terms of how much of the OLM framework they choose to leverage for their application. The OLM Operator uses the following workflow: Watch for cluster service versions (CSVs) in a namespace and check that requirements are met. If requirements are met, run the install strategy for the CSV. Note A CSV must be an active member of an Operator group for the install strategy to run. 2.4.2.3. Catalog Operator The Catalog Operator is responsible for resolving and installing cluster service versions (CSVs) and the required resources they specify. It is also responsible for watching catalog sources for updates to packages in channels and upgrading them, automatically if desired, to the latest available versions. To track a package in a channel, you can create a Subscription object configuring the desired package, channel, and the CatalogSource object you want to use for pulling updates. When updates are found, an appropriate InstallPlan object is written into the namespace on behalf of the user. The Catalog Operator uses the following workflow: Connect to each catalog source in the cluster. Watch for unresolved install plans created by a user, and if found: Find the CSV matching the name requested and add the CSV as a resolved resource. For each managed or required CRD, add the CRD as a resolved resource. For each required CRD, find the CSV that manages it. Watch for resolved install plans and create all of the discovered resources for it, if approved by a user or automatically. Watch for catalog sources and subscriptions and create install plans based on them. 2.4.2.4. Catalog Registry The Catalog Registry stores CSVs and CRDs for creation in a cluster and stores metadata about packages and channels. A package manifest is an entry in the Catalog Registry that associates a package identity with sets of CSVs. Within a package, channels point to a particular CSV. Because CSVs explicitly reference the CSV that they replace, a package manifest provides the Catalog Operator with all of the information that is required to update a CSV to the latest version in a channel, stepping through each intermediate version. 2.4.3. Operator Lifecycle Manager workflow This guide outlines the workflow of Operator Lifecycle Manager (OLM) in OpenShift Container Platform. 2.4.3.1. Operator installation and upgrade workflow in OLM In the Operator Lifecycle Manager (OLM) ecosystem, the following resources are used to resolve Operator installations and upgrades: ClusterServiceVersion (CSV) CatalogSource Subscription Operator metadata, defined in CSVs, can be stored in a collection called a catalog source. OLM uses catalog sources, which use the Operator Registry API , to query for available Operators as well as upgrades for installed Operators. Figure 2.3. Catalog source overview Within a catalog source, Operators are organized into packages and streams of updates called channels , which should be a familiar update pattern from OpenShift Container Platform or other software on a continuous release cycle like web browsers. Figure 2.4. Packages and channels in a Catalog source A user indicates a particular package and channel in a particular catalog source in a subscription , for example an etcd package and its alpha channel. If a subscription is made to a package that has not yet been installed in the namespace, the latest Operator for that package is installed. Note OLM deliberately avoids version comparisons, so the \"latest\" or \"newest\" Operator available from a given catalog channel package path does not necessarily need to be the highest version number. It should be thought of more as the head reference of a channel, similar to a Git repository. Each CSV has a replaces parameter that indicates which Operator it replaces. This builds a graph of CSVs that can be queried by OLM, and updates can be shared between channels. Channels can be thought of as entry points into the graph of updates: Figure 2.5. OLM graph of available channel updates Example channels in a package packageName: example channels: - name: alpha currentCSV: example.v0.1.2 - name: beta currentCSV: example.v0.1.3 defaultChannel: alpha For OLM to successfully query for updates, given a catalog source, package, channel, and CSV, a catalog must be able to return, unambiguously and deterministically, a single CSV that replaces the input CSV. 2.4.3.1.1. Example upgrade path For an example upgrade scenario, consider an installed Operator corresponding to CSV version 0.1.1 . OLM queries the catalog source and detects an upgrade in the subscribed channel with new CSV version 0.1.3 that replaces an older but not-installed CSV version 0.1.2 , which in turn replaces the older and installed CSV version 0.1.1 . OLM walks back from the channel head to previous versions via the replaces field specified in the CSVs to determine the upgrade path 0.1.3 0.1.2 0.1.1 ; the direction of the arrow indicates that the former replaces the latter. OLM upgrades the Operator one version at the time until it reaches the channel head. For this given scenario, OLM installs Operator version 0.1.2 to replace the existing Operator version 0.1.1 . Then, it installs Operator version 0.1.3 to replace the previously installed Operator version 0.1.2 . At this point, the installed operator version 0.1.3 matches the channel head and the upgrade is completed. 2.4.3.1.2. Skipping upgrades The basic path for upgrades in OLM is: A catalog source is updated with one or more updates to an Operator. OLM traverses every version of the Operator until reaching the latest version the catalog source contains. However, sometimes this is not a safe operation to perform. There will be cases where a published version of an Operator should never be installed on a cluster if it has not already, for example because a version introduces a serious vulnerability. In those cases, OLM must consider two cluster states and provide an update graph that supports both: The \"bad\" intermediate Operator has been seen by the cluster and installed. The \"bad\" intermediate Operator has not yet been installed onto the cluster. By shipping a new catalog and adding a skipped release, OLM is ensured that it can always get a single unique update regardless of the cluster state and whether it has seen the bad update yet. Example CSV with skipped release apiVersion: operators.coreos.com/v1alpha1 kind: ClusterServiceVersion metadata: name: etcdoperator.v0.9.2 namespace: placeholder annotations: spec: displayName: etcd description: Etcd Operator replaces: etcdoperator.v0.9.0 skips: - etcdoperator.v0.9.1 Consider the following example of Old CatalogSource and New CatalogSource . Figure 2.6. Skipping updates This graph maintains that: Any Operator found in Old CatalogSource has a single replacement in New CatalogSource . Any Operator found in New CatalogSource has a single replacement in New CatalogSource . If the bad update has not yet been installed, it will never be. 2.4.3.1.3. Replacing multiple Operators Creating New CatalogSource as described requires publishing CSVs that replace one Operator, but can skip several. This can be accomplished using the skipRange annotation: olm.skipRange: <semver_range> where <semver_range> has the version range format supported by the semver library . When searching catalogs for updates, if the head of a channel has a skipRange annotation and the currently installed Operator has a version field that falls in the range, OLM updates to the latest entry in the channel. The order of precedence is: Channel head in the source specified by sourceName on the subscription, if the other criteria for skipping are met. The next Operator that replaces the current one, in the source specified by sourceName . Channel head in another source that is visible to the subscription, if the other criteria for skipping are met. The next Operator that replaces the current one in any source visible to the subscription. Example CSV with skipRange apiVersion: operators.coreos.com/v1alpha1 kind: ClusterServiceVersion metadata: name: elasticsearch-operator.v4.1.2 namespace: <namespace> annotations: olm.skipRange: '>=4.1.0 <4.1.2' 2.4.3.1.4. Z-stream support A z-stream , or patch release, must replace all previous z-stream releases for the same minor version. OLM does not consider major, minor, or patch versions, it just needs to build the correct graph in a catalog. In other words, OLM must be able to take a graph as in Old CatalogSource and, similar to before, generate a graph as in New CatalogSource : Figure 2.7. Replacing several Operators This graph maintains that: Any Operator found in Old CatalogSource has a single replacement in New CatalogSource . Any Operator found in New CatalogSource has a single replacement in New CatalogSource . Any z-stream release in Old CatalogSource will update to the latest z-stream release in New CatalogSource . Unavailable releases can be considered \"virtual\" graph nodes; their content does not need to exist, the registry just needs to respond as if the graph looks like this. 2.4.4. Operator Lifecycle Manager dependency resolution This guide outlines dependency resolution and custom resource definition (CRD) upgrade lifecycles with Operator Lifecycle Manager (OLM) in OpenShift Container Platform. 2.4.4.1. About dependency resolution Operator Lifecycle Manager (OLM) manages the dependency resolution and upgrade lifecycle of running Operators. In many ways, the problems OLM faces are similar to other system or language package managers, such as yum and rpm . However, there is one constraint that similar systems do not generally have that OLM does: because Operators are always running, OLM attempts to ensure that you are never left with a set of Operators that do not work with each other. As a result, OLM must never create the following scenarios: Install a set of Operators that require APIs that cannot be provided Update an Operator in a way that breaks another that depends upon it This is made possible with two types of data: Properties Typed metadata about the Operator that constitutes the public interface for it in the dependency resolver. Examples include the group/version/kind (GVK) of the APIs provided by the Operator and the semantic version (semver) of the Operator. Constraints or dependencies An Operator’s requirements that should be satisfied by other Operators that might or might not have already been installed on the target cluster. These act as queries or filters over all available Operators and constrain the selection during dependency resolution and installation. Examples include requiring a specific API to be available on the cluster or expecting a particular Operator with a particular version to be installed. OLM converts these properties and constraints into a system of Boolean formulas and passes them to a SAT solver, a program that establishes Boolean satisfiability, which does the work of determining what Operators should be installed. 2.4.4.2. Operator properties All Operators in a catalog have the following properties: olm.package Includes the name of the package and the version of the Operator olm.gvk A single property for each provided API from the cluster service version (CSV) Additional properties can also be directly declared by an Operator author by including a properties.yaml file in the metadata/ directory of the Operator bundle. Example arbitrary property properties: - type: olm.kubeversion value: version: \"1.16.0\" 2.4.4.2.1. Arbitrary properties Operator authors can declare arbitrary properties in a properties.yaml file in the metadata/ directory of the Operator bundle. These properties are translated into a map data structure that is used as an input to the Operator Lifecycle Manager (OLM) resolver at runtime. These properties are opaque to the resolver as it does not understand the properties, but it can evaluate the generic constraints against those properties to determine if the constraints can be satisfied given the properties list. Example arbitrary properties properties: - property: type: color value: red - property: type: shape value: square - property: type: olm.gvk value: group: olm.coreos.io version: v1alpha1 kind: myresource This structure can be used to construct a Common Expression Language (CEL) expression for generic constraints. Additional resources Common Expression Language (CEL) constraints 2.4.4.3. Operator dependencies The dependencies of an Operator are listed in a dependencies.yaml file in the metadata/ folder of a bundle. This file is optional and currently only used to specify explicit Operator-version dependencies. The dependency list contains a type field for each item to specify what kind of dependency this is. The following types of Operator dependencies are supported: olm.package This type indicates a dependency for a specific Operator version. The dependency information must include the package name and the version of the package in semver format. For example, you can specify an exact version such as 0.5.2 or a range of versions such as >0.5.1 . olm.gvk With this type, the author can specify a dependency with group/version/kind (GVK) information, similar to existing CRD and API-based usage in a CSV. This is a path to enable Operator authors to consolidate all dependencies, API or explicit versions, to be in the same place. olm.constraint This type declares generic constraints on arbitrary Operator properties. In the following example, dependencies are specified for a Prometheus Operator and etcd CRDs: Example dependencies.yaml file dependencies: - type: olm.package value: packageName: prometheus version: \">0.27.0\" - type: olm.gvk value: group: etcd.database.coreos.com kind: EtcdCluster version: v1beta2 2.4.4.4. Generic constraints An olm.constraint property declares a dependency constraint of a particular type, differentiating non-constraint and constraint properties. Its value field is an object containing a failureMessage field holding a string-representation of the constraint message. This message is surfaced as an informative comment to users if the constraint is not satisfiable at runtime. The following keys denote the available constraint types: gvk Type whose value and interpretation is identical to the olm.gvk type package Type whose value and interpretation is identical to the olm.package type cel A Common Expression Language (CEL) expression evaluated at runtime by the Operator Lifecycle Manager (OLM) resolver over arbitrary bundle properties and cluster information all , any , not Conjunction, disjunction, and negation constraints, respectively, containing one or more concrete constraints, such as gvk or a nested compound constraint 2.4.4.4.1. Common Expression Language (CEL) constraints The cel constraint type supports Common Expression Language (CEL) as the expression language. The cel struct has a rule field which contains the CEL expression string that is evaluated against Operator properties at runtime to determine if the Operator satisfies the constraint. Example cel constraint type: olm.constraint value: failureMessage: 'require to have \"certified\"' cel: rule: 'properties.exists(p, p.type == \"certified\")' The CEL syntax supports a wide range of logical operators, such as AND and OR . As a result, a single CEL expression can have multiple rules for multiple conditions that are linked together by these logical operators. These rules are evaluated against a dataset of multiple different properties from a bundle or any given source, and the output is solved into a single bundle or Operator that satisfies all of those rules within a single constraint. Example cel constraint with multiple rules type: olm.constraint value: failureMessage: 'require to have \"certified\" and \"stable\" properties' cel: rule: 'properties.exists(p, p.type == \"certified\") && properties.exists(p, p.type == \"stable\")' 2.4.4.4.2. Compound constraints (all, any, not) Compound constraint types are evaluated following their logical definitions. The following is an example of a conjunctive constraint ( all ) of two packages and one GVK. That is, they must all be satisfied by installed bundles: Example all constraint schema: olm.bundle name: red.v1.0.0 properties: - type: olm.constraint value: failureMessage: All are required for Red because... all: constraints: - failureMessage: Package blue is needed for... package: name: blue versionRange: '>=1.0.0' - failureMessage: GVK Green/v1 is needed for... gvk: group: greens.example.com version: v1 kind: Green The following is an example of a disjunctive constraint ( any ) of three versions of the same GVK. That is, at least one must be satisfied by installed bundles: Example any constraint schema: olm.bundle name: red.v1.0.0 properties: - type: olm.constraint value: failureMessage: Any are required for Red because... any: constraints: - gvk: group: blues.example.com version: v1beta1 kind: Blue - gvk: group: blues.example.com version: v1beta2 kind: Blue - gvk: group: blues.example.com version: v1 kind: Blue The following is an example of a negation constraint ( not ) of one version of a GVK. That is, this GVK cannot be provided by any bundle in the result set: Example not constraint schema: olm.bundle name: red.v1.0.0 properties: - type: olm.constraint value: all: constraints: - failureMessage: Package blue is needed for... package: name: blue versionRange: '>=1.0.0' - failureMessage: Cannot be required for Red because... not: constraints: - gvk: group: greens.example.com version: v1alpha1 kind: greens The negation semantics might appear unclear in the not constraint context. To clarify, the negation is really instructing the resolver to remove any possible solution that includes a particular GVK, package at a version, or satisfies some child compound constraint from the result set. As a corollary, the not compound constraint should only be used within all or any constraints, because negating without first selecting a possible set of dependencies does not make sense. 2.4.4.4.3. Nested compound constraints A nested compound constraint, one that contains at least one child compound constraint along with zero or more simple constraints, is evaluated from the bottom up following the procedures for each previously described constraint type. The following is an example of a disjunction of conjunctions, where one, the other, or both can satisfy the constraint: Example nested compound constraint schema: olm.bundle name: red.v1.0.0 properties: - type: olm.constraint value: failureMessage: Required for Red because... any: constraints: - all: constraints: - package: name: blue versionRange: '>=1.0.0' - gvk: group: blues.example.com version: v1 kind: Blue - all: constraints: - package: name: blue versionRange: '<1.0.0' - gvk: group: blues.example.com version: v1beta1 kind: Blue Note The maximum raw size of an olm.constraint type is 64KB to limit resource exhaustion attacks. 2.4.4.5. Dependency preferences There can be many options that equally satisfy a dependency of an Operator. The dependency resolver in Operator Lifecycle Manager (OLM) determines which option best fits the requirements of the requested Operator. As an Operator author or user, it can be important to understand how these choices are made so that dependency resolution is clear. 2.4.4.5.1. Catalog priority On OpenShift Container Platform cluster, OLM reads catalog sources to know which Operators are available for installation. Example CatalogSource object apiVersion: \"operators.coreos.com/v1alpha1\" kind: \"CatalogSource\" metadata: name: \"my-operators\" namespace: \"operators\" spec: sourceType: grpc image: example.com/my/operator-index:v1 displayName: \"My Operators\" priority: 100 A CatalogSource object has a priority field, which is used by the resolver to know how to prefer options for a dependency. There are two rules that govern catalog preference: Options in higher-priority catalogs are preferred to options in lower-priority catalogs. Options in the same catalog as the dependent are preferred to any other catalogs. 2.4.4.5.2. Channel ordering An Operator package in a catalog is a collection of update channels that a user can subscribe to in an OpenShift Container Platform cluster. Channels can be used to provide a particular stream of updates for a minor release ( 1.2 , 1.3 ) or a release frequency ( stable , fast ). It is likely that a dependency might be satisfied by Operators in the same package, but different channels. For example, version 1.2 of an Operator might exist in both the stable and fast channels. Each package has a default channel, which is always preferred to non-default channels. If no option in the default channel can satisfy a dependency, options are considered from the remaining channels in lexicographic order of the channel name. 2.4.4.5.3. Order within a channel There are almost always multiple options to satisfy a dependency within a single channel. For example, Operators in one package and channel provide the same set of APIs. When a user creates a subscription, they indicate which channel to receive updates from. This immediately reduces the search to just that one channel. But within the channel, it is likely that many Operators satisfy a dependency. Within a channel, newer Operators that are higher up in the update graph are preferred. If the head of a channel satisfies a dependency, it will be tried first. 2.4.4.5.4. Other constraints In addition to the constraints supplied by package dependencies, OLM includes additional constraints to represent the desired user state and enforce resolution invariants. 2.4.4.5.4.1. Subscription constraint A subscription constraint filters the set of Operators that can satisfy a subscription. Subscriptions are user-supplied constraints for the dependency resolver. They declare the intent to either install a new Operator if it is not already on the cluster, or to keep an existing Operator updated. 2.4.4.5.4.2. Package constraint Within a namespace, no two Operators may come from the same package. 2.4.4.5.5. Additional resources Catalog health requirements 2.4.4.6. CRD upgrades OLM upgrades a custom resource definition (CRD) immediately if it is owned by a singular cluster service version (CSV). If a CRD is owned by multiple CSVs, then the CRD is upgraded when it has satisfied all of the following backward compatible conditions: All existing serving versions in the current CRD are present in the new CRD. All existing instances, or custom resources, that are associated with the serving versions of the CRD are valid when validated against the validation schema of the new CRD. Additional resources Adding a new CRD version Deprecating or removing a CRD version 2.4.4.7. Dependency best practices When specifying dependencies, there are best practices you should consider. Depend on APIs or a specific version range of Operators Operators can add or remove APIs at any time; always specify an olm.gvk dependency on any APIs your Operators requires. The exception to this is if you are specifying olm.package constraints instead. Set a minimum version The Kubernetes documentation on API changes describes what changes are allowed for Kubernetes-style Operators. These versioning conventions allow an Operator to update an API without bumping the API version, as long as the API is backwards-compatible. For Operator dependencies, this means that knowing the API version of a dependency might not be enough to ensure the dependent Operator works as intended. For example: TestOperator v1.0.0 provides v1alpha1 API version of the MyObject resource. TestOperator v1.0.1 adds a new field spec.newfield to MyObject , but still at v1alpha1. Your Operator might require the ability to write spec.newfield into the MyObject resource. An olm.gvk constraint alone is not enough for OLM to determine that you need TestOperator v1.0.1 and not TestOperator v1.0.0. Whenever possible, if a specific Operator that provides an API is known ahead of time, specify an additional olm.package constraint to set a minimum. Omit a maximum version or allow a very wide range Because Operators provide cluster-scoped resources such as API services and CRDs, an Operator that specifies a small window for a dependency might unnecessarily constrain updates for other consumers of that dependency. Whenever possible, do not set a maximum version. Alternatively, set a very wide semantic range to prevent conflicts with other Operators. For example, >1.0.0 <2.0.0 . Unlike with conventional package managers, Operator authors explicitly encode that updates are safe through channels in OLM. If an update is available for an existing subscription, it is assumed that the Operator author is indicating that it can update from the previous version. Setting a maximum version for a dependency overrides the update stream of the author by unnecessarily truncating it at a particular upper bound. Note Cluster administrators cannot override dependencies set by an Operator author. However, maximum versions can and should be set if there are known incompatibilities that must be avoided. Specific versions can be omitted with the version range syntax, for example > 1.0.0 !1.2.1 . Additional resources Kubernetes documentation: Changing the API 2.4.4.8. Dependency caveats When specifying dependencies, there are caveats you should consider. No compound constraints (AND) There is currently no method for specifying an AND relationship between constraints. In other words, there is no way to specify that one Operator depends on another Operator that both provides a given API and has version >1.1.0 . This means that when specifying a dependency such as: dependencies: - type: olm.package value: packageName: etcd version: \">3.1.0\" - type: olm.gvk value: group: etcd.database.coreos.com kind: EtcdCluster version: v1beta2 It would be possible for OLM to satisfy this with two Operators: one that provides EtcdCluster and one that has version >3.1.0 . Whether that happens, or whether an Operator is selected that satisfies both constraints, depends on the ordering that potential options are visited. Dependency preferences and ordering options are well-defined and can be reasoned about, but to exercise caution, Operators should stick to one mechanism or the other. Cross-namespace compatibility OLM performs dependency resolution at the namespace scope. It is possible to get into an update deadlock if updating an Operator in one namespace would be an issue for an Operator in another namespace, and vice-versa. 2.4.4.9. Example dependency resolution scenarios In the following examples, a provider is an Operator which \"owns\" a CRD or API service. Example: Deprecating dependent APIs A and B are APIs (CRDs): The provider of A depends on B. The provider of B has a subscription. The provider of B updates to provide C but deprecates B. This results in: B no longer has a provider. A no longer works. This is a case OLM prevents with its upgrade strategy. Example: Version deadlock A and B are APIs: The provider of A requires B. The provider of B requires A. The provider of A updates to (provide A2, require B2) and deprecate A. The provider of B updates to (provide B2, require A2) and deprecate B. If OLM attempts to update A without simultaneously updating B, or vice-versa, it is unable to progress to new versions of the Operators, even though a new compatible set can be found. This is another case OLM prevents with its upgrade strategy. 2.4.5. Operator groups This guide outlines the use of Operator groups with Operator Lifecycle Manager (OLM) in OpenShift Container Platform. 2.4.5.1. About Operator groups An Operator group , defined by the OperatorGroup resource, provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators. The set of target namespaces is provided by a comma-delimited string stored in the olm.targetNamespaces annotation of a cluster service version (CSV). This annotation is applied to the CSV instances of member Operators and is projected into their deployments. 2.4.5.2. Operator group membership An Operator is considered a member of an Operator group if the following conditions are true: The CSV of the Operator exists in the same namespace as the Operator group. The install modes in the CSV of the Operator support the set of namespaces targeted by the Operator group. An install mode in a CSV consists of an InstallModeType field and a boolean Supported field. The spec of a CSV can contain a set of install modes of four distinct InstallModeTypes : Table 2.4. Install modes and supported Operator groups InstallModeType Description OwnNamespace The Operator can be a member of an Operator group that selects its own namespace. SingleNamespace The Operator can be a member of an Operator group that selects one namespace. MultiNamespace The Operator can be a member of an Operator group that selects more than one namespace. AllNamespaces The Operator can be a member of an Operator group that selects all namespaces (target namespace set is the empty string \"\" ). Note If the spec of a CSV omits an entry of InstallModeType , then that type is considered unsupported unless support can be inferred by an existing entry that implicitly supports it. 2.4.5.3. Target namespace selection You can explicitly name the target namespace for an Operator group using the spec.targetNamespaces parameter: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: my-group namespace: my-namespace spec: targetNamespaces: - my-namespace Warning Operator Lifecycle Manager (OLM) creates the following cluster roles for each Operator group: <operatorgroup_name>-admin <operatorgroup_name>-edit <operatorgroup_name>-view When you manually create an Operator group, you must specify a unique name that does not conflict with the existing cluster roles or other Operator groups on the cluster. You can alternatively specify a namespace using a label selector with the spec.selector parameter: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: my-group namespace: my-namespace spec: selector: cool.io/prod: \"true\" Important Listing multiple namespaces via spec.targetNamespaces or use of a label selector via spec.selector is not recommended, as the support for more than one target namespace in an Operator group will likely be removed in a future release. If both spec.targetNamespaces and spec.selector are defined, spec.selector is ignored. Alternatively, you can omit both spec.selector and spec.targetNamespaces to specify a global Operator group, which selects all namespaces: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: my-group namespace: my-namespace The resolved set of selected namespaces is shown in the status.namespaces parameter of an Opeator group. The status.namespace of a global Operator group contains the empty string ( \"\" ), which signals to a consuming Operator that it should watch all namespaces. 2.4.5.4. Operator group CSV annotations Member CSVs of an Operator group have the following annotations: Annotation Description olm.operatorGroup=<group_name> Contains the name of the Operator group. olm.operatorNamespace=<group_namespace> Contains the namespace of the Operator group. olm.targetNamespaces=<target_namespaces> Contains a comma-delimited string that lists the target namespace selection of the Operator group. Note All annotations except olm.targetNamespaces are included with copied CSVs. Omitting the olm.targetNamespaces annotation on copied CSVs prevents the duplication of target namespaces between tenants. 2.4.5.5. Provided APIs annotation A group/version/kind (GVK) is a unique identifier for a Kubernetes API. Information about what GVKs are provided by an Operator group are shown in an olm.providedAPIs annotation. The value of the annotation is a string consisting of <kind>.<version>.<group> delimited with commas. The GVKs of CRDs and API services provided by all active member CSVs of an Operator group are included. Review the following example of an OperatorGroup object with a single active member CSV that provides the PackageManifest resource: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: annotations: olm.providedAPIs: PackageManifest.v1alpha1.packages.apps.redhat.com name: olm-operators namespace: local ... spec: selector: {} serviceAccount: metadata: creationTimestamp: null targetNamespaces: - local status: lastUpdated: 2019-02-19T16:18:28Z namespaces: - local 2.4.5.6. Role-based access control When an Operator group is created, three cluster roles are generated. Each contains a single aggregation rule with a cluster role selector set to match a label, as shown below: Cluster role Label to match <operatorgroup_name>-admin olm.opgroup.permissions/aggregate-to-admin: <operatorgroup_name> <operatorgroup_name>-edit olm.opgroup.permissions/aggregate-to-edit: <operatorgroup_name> <operatorgroup_name>-view olm.opgroup.permissions/aggregate-to-view: <operatorgroup_name> Warning Operator Lifecycle Manager (OLM) creates the following cluster roles for each Operator group: <operatorgroup_name>-admin <operatorgroup_name>-edit <operatorgroup_name>-view When you manually create an Operator group, you must specify a unique name that does not conflict with the existing cluster roles or other Operator groups on the cluster. The following RBAC resources are generated when a CSV becomes an active member of an Operator group, as long as the CSV is watching all namespaces with the AllNamespaces install mode and is not in a failed state with reason InterOperatorGroupOwnerConflict : Cluster roles for each API resource from a CRD Cluster roles for each API resource from an API service Additional roles and role bindings Table 2.5. Cluster roles generated for each API resource from a CRD Cluster role Settings <kind>.<group>-<version>-admin Verbs on <kind> : * Aggregation labels: rbac.authorization.k8s.io/aggregate-to-admin: true olm.opgroup.permissions/aggregate-to-admin: <operatorgroup_name> <kind>.<group>-<version>-edit Verbs on <kind> : create update patch delete Aggregation labels: rbac.authorization.k8s.io/aggregate-to-edit: true olm.opgroup.permissions/aggregate-to-edit: <operatorgroup_name> <kind>.<group>-<version>-view Verbs on <kind> : get list watch Aggregation labels: rbac.authorization.k8s.io/aggregate-to-view: true olm.opgroup.permissions/aggregate-to-view: <operatorgroup_name> <kind>.<group>-<version>-view-crdview Verbs on apiextensions.k8s.io customresourcedefinitions <crd-name> : get Aggregation labels: rbac.authorization.k8s.io/aggregate-to-view: true olm.opgroup.permissions/aggregate-to-view: <operatorgroup_name> Table 2.6. Cluster roles generated for each API resource from an API service Cluster role Settings <kind>.<group>-<version>-admin Verbs on <kind> : * Aggregation labels: rbac.authorization.k8s.io/aggregate-to-admin: true olm.opgroup.permissions/aggregate-to-admin: <operatorgroup_name> <kind>.<group>-<version>-edit Verbs on <kind> : create update patch delete Aggregation labels: rbac.authorization.k8s.io/aggregate-to-edit: true olm.opgroup.permissions/aggregate-to-edit: <operatorgroup_name> <kind>.<group>-<version>-view Verbs on <kind> : get list watch Aggregation labels: rbac.authorization.k8s.io/aggregate-to-view: true olm.opgroup.permissions/aggregate-to-view: <operatorgroup_name> Additional roles and role bindings If the CSV defines exactly one target namespace that contains * , then a cluster role and corresponding cluster role binding are generated for each permission defined in the permissions field of the CSV. All resources generated are given the olm.owner: <csv_name> and olm.owner.namespace: <csv_namespace> labels. If the CSV does not define exactly one target namespace that contains * , then all roles and role bindings in the Operator namespace with the olm.owner: <csv_name> and olm.owner.namespace: <csv_namespace> labels are copied into the target namespace. 2.4.5.7. Copied CSVs OLM creates copies of all active member CSVs of an Operator group in each of the target namespaces of that Operator group. The purpose of a copied CSV is to tell users of a target namespace that a specific Operator is configured to watch resources created there. Copied CSVs have a status reason Copied and are updated to match the status of their source CSV. The olm.targetNamespaces annotation is stripped from copied CSVs before they are created on the cluster. Omitting the target namespace selection avoids the duplication of target namespaces between tenants. Copied CSVs are deleted when their source CSV no longer exists or the Operator group that their source CSV belongs to no longer targets the namespace of the copied CSV. Note By default, the disableCopiedCSVs field is disabled. After enabling a disableCopiedCSVs field, the OLM deletes existing copied CSVs on a cluster. When a disableCopiedCSVs field is disabled, the OLM adds copied CSVs again. Disable the disableCopiedCSVs field: $ cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OLMConfig metadata: name: cluster spec: features: disableCopiedCSVs: false EOF Enable the disableCopiedCSVs field: $ cat << EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OLMConfig metadata: name: cluster spec: features: disableCopiedCSVs: true EOF 2.4.5.8. Static Operator groups An Operator group is static if its spec.staticProvidedAPIs field is set to true . As a result, OLM does not modify the olm.providedAPIs annotation of an Operator group, which means that it can be set in advance. This is useful when a user wants to use an Operator group to prevent resource contention in a set of namespaces but does not have active member CSVs that provide the APIs for those resources. Below is an example of an Operator group that protects Prometheus resources in all namespaces with the something.cool.io/cluster-monitoring: \"true\" annotation: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-monitoring namespace: cluster-monitoring annotations: olm.providedAPIs: Alertmanager.v1.monitoring.coreos.com,Prometheus.v1.monitoring.coreos.com,PrometheusRule.v1.monitoring.coreos.com,ServiceMonitor.v1.monitoring.coreos.com spec: staticProvidedAPIs: true selector: matchLabels: something.cool.io/cluster-monitoring: \"true\" Warning Operator Lifecycle Manager (OLM) creates the following cluster roles for each Operator group: <operatorgroup_name>-admin <operatorgroup_name>-edit <operatorgroup_name>-view When you manually create an Operator group, you must specify a unique name that does not conflict with the existing cluster roles or other Operator groups on the cluster. 2.4.5.9. Operator group intersection Two Operator groups are said to have intersecting provided APIs if the intersection of their target namespace sets is not an empty set and the intersection of their provided API sets, defined by olm.providedAPIs annotations, is not an empty set. A potential issue is that Operator groups with intersecting provided APIs can compete for the same resources in the set of intersecting namespaces. Note When checking intersection rules, an Operator group namespace is always included as part of its selected target namespaces. Rules for intersection Each time an active member CSV synchronizes, OLM queries the cluster for the set of intersecting provided APIs between the Operator group of the CSV and all others. OLM then checks if that set is an empty set: If true and the CSV’s provided APIs are a subset of the Operator group’s: Continue transitioning. If true and the CSV’s provided APIs are not a subset of the Operator group’s: If the Operator group is static: Clean up any deployments that belong to the CSV. Transition the CSV to a failed state with status reason CannotModifyStaticOperatorGroupProvidedAPIs . If the Operator group is not static: Replace the Operator group’s olm.providedAPIs annotation with the union of itself and the CSV’s provided APIs. If false and the CSV’s provided APIs are not a subset of the Operator group’s: Clean up any deployments that belong to the CSV. Transition the CSV to a failed state with status reason InterOperatorGroupOwnerConflict . If false and the CSV’s provided APIs are a subset of the Operator group’s: If the Operator group is static: Clean up any deployments that belong to the CSV. Transition the CSV to a failed state with status reason CannotModifyStaticOperatorGroupProvidedAPIs . If the Operator group is not static: Replace the Operator group’s olm.providedAPIs annotation with the difference between itself and the CSV’s provided APIs. Note Failure states caused by Operator groups are non-terminal. The following actions are performed each time an Operator group synchronizes: The set of provided APIs from active member CSVs is calculated from the cluster. Note that copied CSVs are ignored. The cluster set is compared to olm.providedAPIs , and if olm.providedAPIs contains any extra APIs, then those APIs are pruned. All CSVs that provide the same APIs across all namespaces are requeued. This notifies conflicting CSVs in intersecting groups that their conflict has possibly been resolved, either through resizing or through deletion of the conflicting CSV. 2.4.5.10. Limitations for multitenant Operator management OpenShift Container Platform provides limited support for simultaneously installing different versions of an Operator on the same cluster. Operator Lifecycle Manager (OLM) installs Operators multiple times in different namespaces. One constraint of this is that the Operator’s API versions must be the same. Operators are control plane extensions due to their usage of CustomResourceDefinition objects (CRDs), which are global resources in Kubernetes. Different major versions of an Operator often have incompatible CRDs. This makes them incompatible to install simultaneously in different namespaces on a cluster. All tenants, or namespaces, share the same control plane of a cluster. Therefore, tenants in a multitenant cluster also share global CRDs, which limits the scenarios in which different instances of the same Operator can be used in parallel on the same cluster. The supported scenarios include the following: Operators of different versions that ship the exact same CRD definition (in case of versioned CRDs, the exact same set of versions) Operators of different versions that do not ship a CRD, and instead have their CRD available in a separate bundle on the OperatorHub All other scenarios are not supported, because the integrity of the cluster data cannot be guaranteed if there are multiple competing or overlapping CRDs from different Operator versions to be reconciled on the same cluster. Additional resources Operator Lifecycle Manager (OLM) Multitenancy and Operator colocation Operators in multitenant clusters Allowing non-cluster administrators to install Operators 2.4.5.11. Troubleshooting Operator groups Membership An install plan’s namespace must contain only one Operator group. When attempting to generate a cluster service version (CSV) in a namespace, an install plan considers an Operator group invalid in the following scenarios: No Operator groups exist in the install plan’s namespace. Multiple Operator groups exist in the install plan’s namespace. An incorrect or non-existent service account name is specified in the Operator group. If an install plan encounters an invalid Operator group, the CSV is not generated and the InstallPlan resource continues to install with a relevant message. For example, the following message is provided if more than one Operator group exists in the same namespace: attenuated service account query failed - more than one operator group(s) are managing this namespace count=2 where count= specifies the number of Operator groups in the namespace. If the install modes of a CSV do not support the target namespace selection of the Operator group in its namespace, the CSV transitions to a failure state with the reason UnsupportedOperatorGroup . CSVs in a failed state for this reason transition to pending after either the target namespace selection of the Operator group changes to a supported configuration, or the install modes of the CSV are modified to support the target namespace selection. 2.4.6. Multitenancy and Operator colocation This guide outlines multitenancy and Operator colocation in Operator Lifecycle Manager (OLM). 2.4.6.1. Colocation of Operators in a namespace Operator Lifecycle Manager (OLM) handles OLM-managed Operators that are installed in the same namespace, meaning their Subscription resources are colocated in the same namespace, as related Operators. Even if they are not actually related, OLM considers their states, such as their version and update policy, when any one of them is updated. This default behavior manifests in two ways: InstallPlan resources of pending updates include ClusterServiceVersion (CSV) resources of all other Operators that are in the same namespace. All Operators in the same namespace share the same update policy. For example, if one Operator is set to manual updates, all other Operators' update policies are also set to manual. These scenarios can lead to the following issues: It becomes hard to reason about install plans for Operator updates, because there are many more resources defined in them than just the updated Operator. It becomes impossible to have some Operators in a namespace update automatically while other are updated manually, which is a common desire for cluster administrators. These issues usually surface because, when installing Operators with the OpenShift Container Platform web console, the default behavior installs Operators that support the All namespaces install mode into the default openshift-operators global namespace. As a cluster administrator, you can bypass this default behavior manually by using the following workflow: Create a namespace for the installation of the Operator. Create a custom global Operator group , which is an Operator group that watches all namespaces. By associating this Operator group with the namespace you just created, it makes the installation namespace a global namespace, which makes Operators installed there available in all namespaces. Install the desired Operator in the installation namespace. If the Operator has dependencies, the dependencies are automatically installed in the pre-created namespace. As a result, it is then valid for the dependency Operators to have the same update policy and shared install plans. For a detailed procedure, see \"Installing global Operators in custom namespaces\". Additional resources Installing global Operators in custom namespaces Operators in multitenant clusters 2.4.7. Operator conditions This guide outlines how Operator Lifecycle Manager (OLM) uses Operator conditions. 2.4.7.1. About Operator conditions As part of its role in managing the lifecycle of an Operator, Operator Lifecycle Manager (OLM) infers the state of an Operator from the state of Kubernetes resources that define the Operator. While this approach provides some level of assurance that an Operator is in a given state, there are many instances where an Operator might need to communicate information to OLM that could not be inferred otherwise. This information can then be used by OLM to better manage the lifecycle of the Operator. OLM provides a custom resource definition (CRD) called OperatorCondition that allows Operators to communicate conditions to OLM. There are a set of supported conditions that influence management of the Operator by OLM when present in the Spec.Conditions array of an OperatorCondition resource. Note By default, the Spec.Conditions array is not present in an OperatorCondition object until it is either added by a user or as a result of custom Operator logic. 2.4.7.2. Supported conditions Operator Lifecycle Manager (OLM) supports the following Operator conditions. 2.4.7.2.1. Upgradeable condition The Upgradeable Operator condition prevents an existing cluster service version (CSV) from being replaced by a newer version of the CSV. This condition is useful when: An Operator is about to start a critical process and should not be upgraded until the process is completed. An Operator is performing a migration of custom resources (CRs) that must be completed before the Operator is ready to be upgraded. Important Setting the Upgradeable Operator condition to the False value does not avoid pod disruption. If you must ensure your pods are not disrupted, see \"Using pod disruption budgets to specify the number of pods that must be up\" and \"Graceful termination\" in the \"Additional resources\" section. Example Upgradeable Operator condition apiVersion: operators.coreos.com/v1 kind: OperatorCondition metadata: name: my-operator namespace: operators spec: conditions: - type: Upgradeable 1 status: \"False\" 2 reason: \"migration\" message: \"The Operator is performing a migration.\" lastTransitionTime: \"2020-08-24T23:15:55Z\" 1 Name of the condition. 2 A False value indicates the Operator is not ready to be upgraded. OLM prevents a CSV that replaces the existing CSV of the Operator from leaving the Pending phase. A False value does not block cluster upgrades. 2.4.7.3. Additional resources Managing Operator conditions Enabling Operator conditions Using pod disruption budgets to specify the number of pods that must be up Graceful termination 2.4.8. Operator Lifecycle Manager metrics 2.4.8.1. Exposed metrics Operator Lifecycle Manager (OLM) exposes certain OLM-specific resources for use by the Prometheus-based OpenShift Container Platform cluster monitoring stack. Table 2.7. Metrics exposed by OLM Name Description catalog_source_count Number of catalog sources. catalogsource_ready State of a catalog source. The value 1 indicates that the catalog source is in a READY state. The value of 0 indicates that the catalog source is not in a READY state. csv_abnormal When reconciling a cluster service version (CSV), present whenever a CSV version is in any state other than Succeeded , for example when it is not installed. Includes the name , namespace , phase , reason , and version labels. A Prometheus alert is created when this metric is present. csv_count Number of CSVs successfully registered. csv_succeeded When reconciling a CSV, represents whether a CSV version is in a Succeeded state (value 1 ) or not (value 0 ). Includes the name , namespace , and version labels. csv_upgrade_count Monotonic count of CSV upgrades. install_plan_count Number of install plans. installplan_warnings_total Monotonic count of warnings generated by resources, such as deprecated resources, included in an install plan. olm_resolution_duration_seconds The duration of a dependency resolution attempt. subscription_count Number of subscriptions. subscription_sync_total Monotonic count of subscription syncs. Includes the channel , installed CSV, and subscription name labels. 2.4.9. Webhook management in Operator Lifecycle Manager Webhooks allow Operator authors to intercept, modify, and accept or reject resources before they are saved to the object store and handled by the Operator controller. Operator Lifecycle Manager (OLM) can manage the lifecycle of these webhooks when they are shipped alongside your Operator. See Defining cluster service versions (CSVs) for details on how an Operator developer can define webhooks for their Operator, as well as considerations when running on OLM. 2.4.9.1. Additional resources Types of webhook admission plugins Kubernetes documentation: Validating admission webhooks Mutating admission webhooks Conversion webhooks 2.5. Understanding OperatorHub 2.5.1. About OperatorHub OperatorHub is the web console interface in OpenShift Container Platform that cluster administrators use to discover and install Operators. With one click, an Operator can be pulled from its off-cluster source, installed and subscribed on the cluster, and made ready for engineering teams to self-service manage the product across deployment environments using Operator Lifecycle Manager (OLM). Cluster administrators can choose from catalogs grouped into the following categories: Category Description Red Hat Operators Red Hat products packaged and shipped by Red Hat. Supported by Red Hat. Certified Operators Products from leading independent software vendors (ISVs). Red Hat partners with ISVs to package and ship. Supported by the ISV. Red Hat Marketplace Certified software that can be purchased from Red Hat Marketplace . Community Operators Optionally-visible software maintained by relevant representatives in the redhat-openshift-ecosystem/community-operators-prod/operators GitHub repository. No official support. Custom Operators Operators you add to the cluster yourself. If you have not added any custom Operators, the Custom category does not appear in the web console on your OperatorHub. Operators on OperatorHub are packaged to run on OLM. This includes a YAML file called a cluster service version (CSV) containing all of the CRDs, RBAC rules, deployments, and container images required to install and securely run the Operator. It also contains user-visible information like a description of its features and supported Kubernetes versions. The Operator SDK can be used to assist developers packaging their Operators for use on OLM and OperatorHub. If you have a commercial application that you want to make accessible to your customers, get it included using the certification workflow provided on the Red Hat Partner Connect portal at connect.redhat.com . 2.5.2. OperatorHub architecture The OperatorHub UI component is driven by the Marketplace Operator by default on OpenShift Container Platform in the openshift-marketplace namespace. 2.5.2.1. OperatorHub custom resource The Marketplace Operator manages an OperatorHub custom resource (CR) named cluster that manages the default CatalogSource objects provided with OperatorHub. You can modify this resource to enable or disable the default catalogs, which is useful when configuring OpenShift Container Platform in restricted network environments. Example OperatorHub custom resource apiVersion: config.openshift.io/v1 kind: OperatorHub metadata: name: cluster spec: disableAllDefaultSources: true 1 sources: [ 2 { name: \"community-operators\", disabled: false } ] 1 disableAllDefaultSources is an override that controls availability of all default catalogs that are configured by default during an OpenShift Container Platform installation. 2 Disable default catalogs individually by changing the disabled parameter value per source. 2.5.3. Additional resources Catalog source About the Operator SDK Defining cluster service versions (CSVs) Operator installation and upgrade workflow in OLM Red Hat Partner Connect Red Hat Marketplace 2.6. Red Hat-provided Operator catalogs Red Hat provides several Operator catalogs that are included with OpenShift Container Platform by default. Important As of , the default Red Hat-provided Operator catalog releases in the file-based catalog format. The default Red Hat-provided Operator catalogs for  through 4.10 released in the deprecated SQLite database format. The opm subcommands, flags, and functionality related to the SQLite database format are also deprecated and will be removed in a future release. The features are still supported and must be used for catalogs that use the deprecated SQLite database format. Many of the opm subcommands and flags for working with the SQLite database format, such as opm index prune , do not work with the file-based catalog format. For more information about working with file-based catalogs, see Managing custom catalogs , Operator Framework packaging format , and Mirroring images for a disconnected installation using the oc-mirror plugin . 2.6.1. About Operator catalogs An Operator catalog is a repository of metadata that Operator Lifecycle Manager (OLM) can query to discover and install Operators and their dependencies on a cluster. OLM always installs Operators from the latest version of a catalog. An index image, based on the Operator bundle format, is a containerized snapshot of a catalog. It is an immutable artifact that contains the database of pointers to a set of Operator manifest content. A catalog can reference an index image to source its content for OLM on the cluster. As catalogs are updated, the latest versions of Operators change, and older versions may be removed or altered. In addition, when OLM runs on an OpenShift Container Platform cluster in a restricted network environment, it is unable to access the catalogs directly from the internet to pull the latest content. As a cluster administrator, you can create your own custom index image, either based on a Red Hat-provided catalog or from scratch, which can be used to source the catalog content on the cluster. Creating and updating your own index image provides a method for customizing the set of Operators available on the cluster, while also avoiding the aforementioned restricted network environment issues. Important Kubernetes periodically deprecates certain APIs that are removed in subsequent releases. As a result, Operators are unable to use removed APIs starting with the version of OpenShift Container Platform that uses the Kubernetes version that removed the API. If your cluster is using custom catalogs, see Controlling Operator compatibility with OpenShift Container Platform versions for more details about how Operator authors can update their projects to help avoid workload issues and prevent incompatible upgrades. Note Support for the legacy package manifest format for Operators, including custom catalogs that were using the legacy format, is removed in  and later. When creating custom catalog images, previous versions of OpenShift Container Platform 4 required using the oc adm catalog build command, which was deprecated for several releases and is now removed. With the availability of Red Hat-provided index images starting in , catalog builders must use the opm index command to manage index images. Additional resources Managing custom catalogs Packaging format Using Operator Lifecycle Manager on restricted networks 2.6.2. About Red Hat-provided Operator catalogs The Red Hat-provided catalog sources are installed by default in the openshift-marketplace namespace, which makes the catalogs available cluster-wide in all namespaces. The following Operator catalogs are distributed by Red Hat: Catalog Index image Description redhat-operators registry.redhat.io/redhat/redhat-operator-index:v4.11 Red Hat products packaged and shipped by Red Hat. Supported by Red Hat. certified-operators registry.redhat.io/redhat/certified-operator-index:v4.11 Products from leading independent software vendors (ISVs). Red Hat partners with ISVs to package and ship. Supported by the ISV. redhat-marketplace registry.redhat.io/redhat/redhat-marketplace-index:v4.11 Certified software that can be purchased from Red Hat Marketplace . community-operators registry.redhat.io/redhat/community-operator-index:v4.11 Software maintained by relevant representatives in the redhat-openshift-ecosystem/community-operators-prod/operators GitHub repository. No official support. During a cluster upgrade, the index image tag for the default Red Hat-provided catalog sources are updated automatically by the Cluster Version Operator (CVO) so that Operator Lifecycle Manager (OLM) pulls the updated version of the catalog. For example during an upgrade from  to 4.9, the spec.image field in the CatalogSource object for the redhat-operators catalog is updated from: registry.redhat.io/redhat/redhat-operator-index:v4.8 to: registry.redhat.io/redhat/redhat-operator-index:v4.9 2.7. Operators in multitenant clusters The default behavior for Operator Lifecycle Manager (OLM) aims to provide simplicity during Operator installation. However, this behavior can lack flexibility, especially in multitenant clusters. In order for multiple tenants on a OpenShift Container Platform cluster to use an Operator, the default behavior of OLM requires that administrators install the Operator in All namespaces mode, which can be considered to violate the principle of least privilege. Consider the following scenarios to determine which Operator installation workflow works best for your environment and requirements. Additional resources Common terms: Multitenant Limitations for multitenant Operator management 2.7.1. Default Operator install modes and behavior When installing Operators with the web console as an administrator, you typically have two choices for the install mode, depending on the Operator’s capabilities: Single namespace Installs the Operator in the chosen single namespace, and makes all permissions that the Operator requests available in that namespace. All namespaces Installs the Operator in the default openshift-operators namespace to watch and be made available to all namespaces in the cluster. Makes all permissions that the Operator requests available in all namespaces. In some cases, an Operator author can define metadata to give the user a second option for that Operator’s suggested namespace. This choice also means that users in the affected namespaces get access to the Operators APIs, which can leverage the custom resources (CRs) they own, depending on their role in the namespace: The namespace-admin and namespace-edit roles can read/write to the Operator APIs, meaning they can use them. The namespace-view role can read CR objects of that Operator. For Single namespace mode, because the Operator itself installs in the chosen namespace, its pod and service account are also located there. For All namespaces mode, the Operator’s privileges are all automatically elevated to cluster roles, meaning the Operator has those permissions in all namespaces. Additional resources Adding Operators to a cluster Install modes types Setting a suggested namespace 2.7.2. Recommended solution for multitenant clusters While a Multinamespace install mode does exist, it is supported by very few Operators. As a middle ground solution between the standard All namespaces and Single namespace install modes, you can install multiple instances of the same Operator, one for each tenant, by using the following workflow: Create a namespace for the tenant Operator that is separate from the tenant’s namespace. Create an Operator group for the tenant Operator scoped only to the tenant’s namespace. Install the Operator in the tenant Operator namespace. As a result, the Operator resides in the tenant Operator namespace and watches the tenant namespace, but neither the Operator’s pod nor its service account are visible or usable by the tenant. This solution provides better tenant separation, least privilege principle at the cost of resource usage, and additional orchestration to ensure the constraints are met. For a detailed procedure, see \"Preparing for multiple instances of an Operator for multitenant clusters\". Limitations and considerations This solution only works when the following constraints are met: All instances of the same Operator must be the same version. The Operator cannot have dependencies on other Operators. The Operator cannot ship a CRD conversion webhook. Important You cannot use different versions of the same Operator on the same cluster. Eventually, the installation of another instance of the Operator would be blocked when it meets the following conditions: The instance is not the newest version of the Operator. The instance ships an older revision of the CRDs that lack information or versions that newer revisions have that are already in use on the cluster. Warning As an administrator, use caution when allowing non-cluster administrators to install Operators self-sufficiently, as explained in \"Allowing non-cluster administrators to install Operators\". These tenants should only have access to a curated catalog of Operators that are known to not have dependencies. These tenants must also be forced to use the same version line of an Operator, to ensure the CRDs do not change. This requires the use of namespace-scoped catalogs and likely disabling the global default catalogs. Additional resources Preparing for multiple instances of an Operator for multitenant clusters Allowing non-cluster administrators to install Operators Disabling the default OperatorHub catalog sources 2.7.3. Operator colocation and Operator groups Operator Lifecycle Manager (OLM) handles OLM-managed Operators that are installed in the same namespace, meaning their Subscription resources are colocated in the same namespace, as related Operators. Even if they are not actually related, OLM considers their states, such as their version and update policy, when any one of them is updated. For more information on Operator colocation and using Operator groups effectively, see Operator Lifecycle Manager (OLM) Multitenancy and Operator colocation . 2.8. CRDs 2.8.1. Extending the Kubernetes API with custom resource definitions Operators use the Kubernetes extension mechanism, custom resource definitions (CRDs), so that custom objects managed by the Operator look and act just like the built-in, native Kubernetes objects. This guide describes how cluster administrators can extend their OpenShift Container Platform cluster by creating and managing CRDs. 2.8.1.1. Custom resource definitions In the Kubernetes API, a resource is an endpoint that stores a collection of API objects of a certain kind. For example, the built-in Pods resource contains a collection of Pod objects. A custom resource definition (CRD) object defines a new, unique object type, called a kind , in the cluster and lets the Kubernetes API server handle its entire lifecycle. Custom resource (CR) objects are created from CRDs that have been added to the cluster by a cluster administrator, allowing all cluster users to add the new resource type into projects. When a cluster administrator adds a new CRD to the cluster, the Kubernetes API server reacts by creating a new RESTful resource path that can be accessed by the entire cluster or a single project (namespace) and begins serving the specified CR. Cluster administrators that want to grant access to the CRD to other users can use cluster role aggregation to grant access to users with the admin , edit , or view default cluster roles. Cluster role aggregation allows the insertion of custom policy rules into these cluster roles. This behavior integrates the new resource into the RBAC policy of the cluster as if it was a built-in resource. Operators in particular make use of CRDs by packaging them with any required RBAC policy and other software-specific logic. Cluster administrators can also add CRDs manually to the cluster outside of the lifecycle of an Operator, making them available to all users. Note While only cluster administrators can create CRDs, developers can create the CR from an existing CRD if they have read and write permission to it. 2.8.1.2. Creating a custom resource definition To create custom resource (CR) objects, cluster administrators must first create a custom resource definition (CRD). Prerequisites Access to an OpenShift Container Platform cluster with cluster-admin user privileges. Procedure To create a CRD: Create a YAML file that contains the following field types: Example YAML file for a CRD apiVersion: apiextensions.k8s.io/v1 1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com 2 spec: group: stable.example.com 3 versions: name: v1 4 scope: Namespaced 5 names: plural: crontabs 6 singular: crontab 7 kind: CronTab 8 shortNames: - ct 9 1 Use the apiextensions.k8s.io/v1 API. 2 Specify a name for the definition. This must be in the <plural-name>.<group> format using the values from the group and plural fields. 3 Specify a group name for the API. An API group is a collection of objects that are logically related. For example, all batch objects like Job or ScheduledJob could be in the batch API group (such as batch.api.example.com ). A good practice is to use a fully-qualified-domain name (FQDN) of your organization. 4 Specify a version name to be used in the URL. Each API group can exist in multiple versions, for example v1alpha , v1beta , v1 . 5 Specify whether the custom objects are available to a project ( Namespaced ) or all projects in the cluster ( Cluster ). 6 Specify the plural name to use in the URL. The plural field is the same as a resource in an API URL. 7 Specify a singular name to use as an alias on the CLI and for display. 8 Specify the kind of objects that can be created. The type can be in CamelCase. 9 Specify a shorter string to match your resource on the CLI. Note By default, a CRD is cluster-scoped and available to all projects. Create the CRD object: $ oc create -f <file_name>.yaml A new RESTful API endpoint is created at: /apis/<spec:group>/<spec:version>/<scope>/*/<names-plural>/... For example, using the example file, the following endpoint is created: /apis/stable.example.com/v1/namespaces/*/crontabs/... You can now use this endpoint URL to create and manage CRs. The object kind is based on the spec.kind field of the CRD object you created. 2.8.1.3. Creating cluster roles for custom resource definitions Cluster administrators can grant permissions to existing cluster-scoped custom resource definitions (CRDs). If you use the admin , edit , and view default cluster roles, you can take advantage of cluster role aggregation for their rules. Important You must explicitly assign permissions to each of these roles. The roles with more permissions do not inherit rules from roles with fewer permissions. If you assign a rule to a role, you must also assign that verb to roles that have more permissions. For example, if you grant the get crontabs permission to the view role, you must also grant it to the edit and admin roles. The admin or edit role is usually assigned to the user that created a project through the project template. Prerequisites Create a CRD. Procedure Create a cluster role definition file for the CRD. The cluster role definition is a YAML file that contains the rules that apply to each cluster role. An OpenShift Container Platform controller adds the rules that you specify to the default cluster roles. Example YAML file for a cluster role definition kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 1 metadata: name: aggregate-cron-tabs-admin-edit 2 labels: rbac.authorization.k8s.io/aggregate-to-admin: \"true\" 3 rbac.authorization.k8s.io/aggregate-to-edit: \"true\" 4 rules: - apiGroups: [\"stable.example.com\"] 5 resources: [\"crontabs\"] 6 verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\", \"deletecollection\"] 7 --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: aggregate-cron-tabs-view 8 labels: # Add these permissions to the \"view\" default role. rbac.authorization.k8s.io/aggregate-to-view: \"true\" 9 rbac.authorization.k8s.io/aggregate-to-cluster-reader: \"true\" 10 rules: - apiGroups: [\"stable.example.com\"] 11 resources: [\"crontabs\"] 12 verbs: [\"get\", \"list\", \"watch\"] 13 1 Use the rbac.authorization.k8s.io/v1 API. 2 8 Specify a name for the definition. 3 Specify this label to grant permissions to the admin default role. 4 Specify this label to grant permissions to the edit default role. 5 11 Specify the group name of the CRD. 6 12 Specify the plural name of the CRD that these rules apply to. 7 13 Specify the verbs that represent the permissions that are granted to the role. For example, apply read and write permissions to the admin and edit roles and only read permission to the view role. 9 Specify this label to grant permissions to the view default role. 10 Specify this label to grant permissions to the cluster-reader default role. Create the cluster role: $ oc create -f <file_name>.yaml 2.8.1.4. Creating custom resources from a file After a custom resource definitions (CRD) has been added to the cluster, custom resources (CRs) can be created with the CLI from a file using the CR specification. Prerequisites CRD added to the cluster by a cluster administrator. Procedure Create a YAML file for the CR. In the following example definition, the cronSpec and image custom fields are set in a CR of Kind: CronTab . The Kind comes from the spec.kind field of the CRD object: Example YAML file for a CR apiVersion: \"stable.example.com/v1\" 1 kind: CronTab 2 metadata: name: my-new-cron-object 3 finalizers: 4 - finalizer.stable.example.com spec: 5 cronSpec: \"* * * * /5\" image: my-awesome-cron-image 1 Specify the group name and API version (name/version) from the CRD. 2 Specify the type in the CRD. 3 Specify a name for the object. 4 Specify the finalizers for the object, if any. Finalizers allow controllers to implement conditions that must be completed before the object can be deleted. 5 Specify conditions specific to the type of object. After you create the file, create the object: $ oc create -f <file_name>.yaml 2.8.1.5. Inspecting custom resources You can inspect custom resource (CR) objects that exist in your cluster using the CLI. Prerequisites A CR object exists in a namespace to which you have access. Procedure To get information on a specific kind of a CR, run: $ oc get <kind> For example: $ oc get crontab Example output NAME KIND my-new-cron-object CronTab.v1.stable.example.com Resource names are not case-sensitive, and you can use either the singular or plural forms defined in the CRD, as well as any short name. For example: $ oc get crontabs $ oc get crontab $ oc get ct You can also view the raw YAML data for a CR: $ oc get <kind> -o yaml For example: $ oc get ct -o yaml Example output apiVersion: v1 items: - apiVersion: stable.example.com/v1 kind: CronTab metadata: clusterName: \"\" creationTimestamp: 2017-05-31T12:56:35Z deletionGracePeriodSeconds: null deletionTimestamp: null name: my-new-cron-object namespace: default resourceVersion: \"285\" selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object uid: 9423255b-4600-11e7-af6a-28d2447dc82b spec: cronSpec: '* * * * /5' 1 image: my-awesome-cron-image 2 1 2 Custom data from the YAML that you used to create the object displays. 2.8.2. Managing resources from custom resource definitions This guide describes how developers can manage custom resources (CRs) that come from custom resource definitions (CRDs). 2.8.2.1. Custom resource definitions In the Kubernetes API, a resource is an endpoint that stores a collection of API objects of a certain kind. For example, the built-in Pods resource contains a collection of Pod objects. A custom resource definition (CRD) object defines a new, unique object type, called a kind , in the cluster and lets the Kubernetes API server handle its entire lifecycle. Custom resource (CR) objects are created from CRDs that have been added to the cluster by a cluster administrator, allowing all cluster users to add the new resource type into projects. Operators in particular make use of CRDs by packaging them with any required RBAC policy and other software-specific logic. Cluster administrators can also add CRDs manually to the cluster outside of the lifecycle of an Operator, making them available to all users. Note While only cluster administrators can create CRDs, developers can create the CR from an existing CRD if they have read and write permission to it. 2.8.2.2. Creating custom resources from a file After a custom resource definitions (CRD) has been added to the cluster, custom resources (CRs) can be created with the CLI from a file using the CR specification. Prerequisites CRD added to the cluster by a cluster administrator. Procedure Create a YAML file for the CR. In the following example definition, the cronSpec and image custom fields are set in a CR of Kind: CronTab . The Kind comes from the spec.kind field of the CRD object: Example YAML file for a CR apiVersion: \"stable.example.com/v1\" 1 kind: CronTab 2 metadata: name: my-new-cron-object 3 finalizers: 4 - finalizer.stable.example.com spec: 5 cronSpec: \"* * * * /5\" image: my-awesome-cron-image 1 Specify the group name and API version (name/version) from the CRD. 2 Specify the type in the CRD. 3 Specify a name for the object. 4 Specify the finalizers for the object, if any. Finalizers allow controllers to implement conditions that must be completed before the object can be deleted. 5 Specify conditions specific to the type of object. After you create the file, create the object: $ oc create -f <file_name>.yaml 2.8.2.3. Inspecting custom resources You can inspect custom resource (CR) objects that exist in your cluster using the CLI. Prerequisites A CR object exists in a namespace to which you have access. Procedure To get information on a specific kind of a CR, run: $ oc get <kind> For example: $ oc get crontab Example output NAME KIND my-new-cron-object CronTab.v1.stable.example.com Resource names are not case-sensitive, and you can use either the singular or plural forms defined in the CRD, as well as any short name. For example: $ oc get crontabs $ oc get crontab $ oc get ct You can also view the raw YAML data for a CR: $ oc get <kind> -o yaml For example: $ oc get ct -o yaml Example output apiVersion: v1 items: - apiVersion: stable.example.com/v1 kind: CronTab metadata: clusterName: \"\" creationTimestamp: 2017-05-31T12:56:35Z deletionGracePeriodSeconds: null deletionTimestamp: null name: my-new-cron-object namespace: default resourceVersion: \"285\" selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object uid: 9423255b-4600-11e7-af6a-28d2447dc82b spec: cronSpec: '* * * * /5' 1 image: my-awesome-cron-image 2 1 2 Custom data from the YAML that you used to create the object displays. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/operators/understanding-operators"}
{"title": "Power Monitoring", "content": "Power Monitoring  Configuring and using power monitoring for OpenShift Container Platform Red Hat OpenShift Documentation Team", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/power_monitoring/index"}
{"title": "Tested deployment models", "content": "Tested deployment models Red Hat Ansible Automation Platform 2.5 Plan your deployment of Ansible Automation Platform Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/tested_deployment_models/index"}
{"title": "Chapter 1. Limiting access to cost management resources", "content": "Chapter 1. Limiting access to cost management resources You may not want users to have access to all cost data, but instead only data specific to their projects or organization. Using role-based access control, you can limit the visibility of resources involved in cost management reports. For example, you may want to restrict a user’s view to only AWS integrations, rather than the entire environment. Role-based access control works by organizing users into groups, which can be associated with one or more roles. A role defines a permission and a set of resource definitions. By default, a user who is not an administrator or viewer will not have access to data, but instead must be granted access to resources. Account administrators can view all data without any further role-based access control configuration. Note A Red Hat account user with Organization Administrator entitlements is required to configure account users on Red Hat Hybrid Cloud Console . This Red Hat login allows you to look up users, add them to groups, and to assign roles that control visibility to resources. For more information about Red Hat account roles, see User Access Configuration Guide For Role-Based Access Control (RBAC) in the Red Hat Hybrid Cloud Console documentation.. 1.1. Default user roles in cost management You can configure custom user access roles for cost management, or assign each user a predefined role within the Red Hat Hybrid Cloud Console . To use a default role, determine the required level of access to permit your users based on the following predefined cost management related roles: Administrator roles Organization Administrator : Can configure and manage user access and is the only user with access to cost management settings . User Access Administrator : Can configure and manage user access to services hosted on Red Hat Hybrid Cloud Console . Cloud Administrator : Can perform any available operation on any integration. Cost Administrator : Can read and write to all resources in cost management. Cost Price List Administrator : Can read and write on all cost models. Viewer roles Cost Cloud Viewer : Has read permissions on cost reports related to cloud integrations. Cost OpenShift Viewer : Has read permissions on cost reports related to OpenShift integrations. Cost Price List Viewer : Has read permissions on price list rates. In addition to using these predefined roles, you can create and manage custom User Access roles with granular permissions for one or more applications in Red Hat Hybrid Cloud Console . For more information, see Adding custom User Access roles in the Red Hat Hybrid Cloud Console documentation. 1.2. Adding a role to a group Once you have decided the correct roles for your organization, you must add your role to a group to manage and limit the scope of information that members in that group can see within cost management. The Member tab shows all users that you can add to the group. When you add users to a group, they become members of that group. A group member inherits the roles of all other groups they belong to. Prerequisites You must be an Organization Administrator. If you are not an Organization Administrator, you must be a member of a group that has the User Access Administrator role assigned to it. Note Only the Organization Administrator can assign the User Access Administrator role to a group. Procedure Log in to your Red Hat organization account at Red Hat Hybrid Cloud Console . Click Settings > Identity & Access Management to open the Red Hat Hybrid Cloud Console Settings page. In the Global navigation, click the User Access Groups . Click Create group . Follow the guided actions provided by the wizard to add a group name, roles, and members. To grant additional group access, edit the group and add additional roles. Your new group is listed in the Groups list on the User Access screen. Verification To verify your configuration, log out of the cost management application and log back in as a user added to the group. For more information about configuring Red Hat account roles and groups, see User Access Configuration Guide For Role-Based Access Control (RBAC) in the Red Hat Hybrid Cloud Console documentation. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/limiting_access_to_cost_management_resources/assembly-limiting-access-cost-resources-rbac"}
{"title": "Interactively installing RHEL from installation media", "content": "Interactively installing RHEL from installation media Red Hat Enterprise Linux 8 Installing RHEL on a local system using the graphical installer Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/interactively_installing_rhel_from_installation_media/index"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/installing_managing_and_removing_user-space_components/legal-notice"}
{"title": "Integrating Microsoft Azure data into cost management", "content": "Integrating Microsoft Azure data into cost management Cost Management Service 1-latest Learn how to add your Microsoft Azure integration and RHEL metering Red Hat Customer Content Services", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/integrating_microsoft_azure_data_into_cost_management/index"}
{"title": "Chapter 11. Managing versions of Application Stream content", "content": "Chapter 11. Managing versions of Application Stream content Content in the AppStream repository can be available in multiple versions, corresponding to module streams. For example, you can install a different version of a module that is already installed on your system. 11.1. Modular dependencies and stream changes Traditionally, packages providing content depend on further packages, and usually specify the desired dependency versions. For packages contained in modules, this mechanism applies as well, but the grouping of packages and their particular versions into modules and streams provides further constraints. Additionally, module streams can declare dependencies on streams of other modules, independent of the packages contained and provided by them. After any operations with packages or modules, the whole dependency tree of all underlying installed packages must satisfy all the conditions that the packages declare. Additionally, all module stream dependencies must be satisfied. For example, disabling a module stream can require disabling other module streams. No packages will be removed automatically. Note that the following actions can cause subsequent automatic operations: Enabling a module stream can result in enabling further module streams. Installing a module stream profile or installing packages from a stream can result in enabling further module streams and installing further packages. Removing a package can result in removing further packages. If these packages were provided by modules, the module streams remain enabled in preparation for further installation, even if no packages from these streams are installed any more. This mirrors the behavior of an unused YUM repository. Important You cannot enable a stream of a module when another stream of the same module is already enabled. To switch streams, complete the steps in Switching to a later stream . Alternatively, reset the module , and then enable the new stream . Important Removing all packages installed from a stream before switching to a different stream prevents the system from reaching states where packages could be installed with no repository or stream that provides them. 11.2. Interaction of modular and non-modular dependencies Modular dependencies are an additional layer on top of regular RPM dependencies. Modular dependencies behave similarly to hypothetical dependencies between repositories. This means that installing different packages requires resolution of both the RPM dependencies and the modular dependencies. The system will always retain the module and stream choices, unless explicitly instructed to change them. A modular package will receive updates contained in the currently enabled stream of the module that provides this package, but will not upgrade to a version contained in a different stream. 11.3. Resetting module streams Resetting a module is an action that returns this module to its initial state - neither enabled nor disabled. If the module has a configured default stream, this stream becomes active as a result of resetting the module. Resetting the module is useful, for example, if you want to only extract the RPM content from the module without keeping the module enabled. You can use the yum module reset command after enabling the module and extracting its contents to reset this module to its initial state. Procedure Reset the module state: # yum module reset <module_name> The module is returned to the initial state. Information about an enabled stream and installed profiles is erased but no installed content is removed. 11.4. Disabling all streams of a module Modules that have a default stream will always have one stream active. If you want to make the content from all module streams of the module inaccessible, you can disable the whole module. Prerequisites You understand the concept of an active module stream . Procedure Disable the module: # yum module disable <module_name> Replace module-name with the name of the module that you want to disable. The yum command asks for confirmation and then disables the module with all its streams. All of the module streams become inactive. No installed content is removed. 11.5. Switching to a later stream When you switch to a later module stream, all respective packages are replaced with their later versions. Important Back up your data and follow migration instructions specific to the component. Alternatively, you can remove all the module’s content installed from the current stream, reset the module , and install the new stream . Prerequisites The system is fully updated. No packages installed on the system are newer than the packages available in the repository. Procedure Determine if your system is prepared for switching to a later stream: # yum distro-sync Important This command must finish with the Nothing to do. Complete! . If it instead proposes changes and asks for confirmation, carefully review these changes and consider whether you want to proceed. Run the YUM distro-sync command repeatedly, if necessary. Alternatively, you can refuse the suggested changes and manually modify your system to a state where the command returns Nothing to do. Complete! . By checking the yum distro-sync result before switching the streams, you prevent making changes to the system that are unrelated to the stream switching because the same command is required as the last step of this procedure. Change the active stream to the later one: # yum module reset <module-name> # yum module enable <module-name> : <new-stream> Synchronize installed packages to perform the change between streams: # yum distro-sync If this action suggests changes to content outside the streams, review them carefully. Note If certain installed packages depend on the earlier stream, and there is no compatible version in the later stream, YUM reports a dependency conflict. In this case, use the --allowerasing option to remove such packages because they cannot be installed together with the later stream because of the missing dependencies. When switching Perl modules, you must always use the --allowerasing option because certain packages in the base RHEL 8 installation depend on Perl 5.26 . You need to reinstall binary extensions for interpreted languages, which are typically written in C or C++, after the new stream is enabled. It concerns, for example, packages installed by using the following commands: The gem command from the ruby module. For more information, see How to switch Ruby streams in RHEL 8 . The npm command from the nodejs module The cpan command from the perl module The pecl command from the php module 11.6. Defining custom default module streams and profiles By default, the YUM utility uses the default module streams defined in the repository that contains the modules. You can configure the default stream and default module profile in the /etc/dnf/modules.defaults.d/ directory. Important Always consider the module stream’s life cycle . Prerequisites You understand the concept of an active module stream . Procedure Display the available streams and their profiles: # yum module list <module_name> For example, to list the available streams and their profiles of the postgresql module, enter: # yum module list postgresql (…​) Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs) Name Stream Profiles Summary postgresql 9.6 client, server [d] PostgreSQL server and client module postgresql 10 [d] client, server [d] PostgreSQL server and client module postgresql 12 client, server [d] PostgreSQL server and client module postgresql 13 client, server [d] PostgreSQL server and client module postgresql 15 client, server [d] PostgreSQL server and client module …​ Hint: [d]efault, [e]nabled, [x]disabled, [i]nstalled Create a YAML configuration file in the /etc/dnf/modules.defaults.d/ drop-in directory. For example, create the /etc/dnf/modules.defaults.d/postgresql.yaml file with the following content to define 13 as the default stream and server as the default profile for the postgresql module: --- document: modulemd-defaults version: 1 data: module: postgresql stream: \"13\" profiles: 13: [server] Verification Verify the default stream and profile settings: # yum module list postgresql (…​) Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs) Name Stream Profiles Summary postgresql 9.6 client, server PostgreSQL server and client module postgresql 10 client, server PostgreSQL server and client module postgresql 12 client, server PostgreSQL server and client module postgresql 13 [d] client, server [d] PostgreSQL server and client module postgresql 15 client, server PostgreSQL server and client module …​ Hint: [d]efault, [e]nabled, [x]disabled, [i]nstalled Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/installing_managing_and_removing_user-space_components/managing-versions-of-appstream-content_using-appstream"}
{"title": "Chapter 10. Managing custom software repositories", "content": "Chapter 10. Managing custom software repositories You can configure a repository in the /etc/yum.conf file. or in a .repo file in the /etc/yum.repos.d/ directory. Note It is recommended to define your repositories in the new or existing .repo file in /etc/yum.repos.d/ because all files with the .repo file extension are read by YUM . The /etc/yum.conf file contains the [main] sections and can contain one or more repository sections ( [ <repository_ID> ] ) that you can use to set repository-specific options. The values you define in individual repository sections of the /etc/yum.conf file override values set in the [main] section. 10.1. YUM repository options The /etc/yum.conf configuration file contains the repository sections with a repository ID in brackets ( [ <repository_ID> ] ). You can use these sections to define individual YUM repositories. Important Repository IDs must be unique. For a complete list of available repository ID options, see the [ <repository_ID> ] OPTIONS section of the dnf.conf(5) man page on your system. 10.2. Adding a YUM repository You can add a YUM repository to your system by defining it in the .repo file in the /etc/yum.repos.d/ directory. Procedure Add a repository to your system: # yum-config-manager --add-repo <repository_URL> Note that repositories added by this command are enabled by default. Review and, optionally, update the repository settings that the previous command has created in the /etc/yum.repos.d/ <repository_URL> .repo file: # cat /etc/yum.repos.d/ <repository_URL> .repo Warning Obtaining and installing software packages from unverified or untrusted sources other than Red Hat certificate-based Content Delivery Network ( CDN ) is a potential security risk, and can lead to security, stability, compatibility, and maintainability issues. 10.3. Enabling a YUM repository Once you added a YUM repository to your system, enable it to ensure installation and updates. Procedure Enable a repository: # yum-config-manager --enable <repository_id> 10.4. Disabling a YUM repository To to prevent particular packages from installation or update, you can disable a YUM repository that contains these packages. Procedure Disable a repository: # yum-config-manager --disable <repository_id> Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/installing_managing_and_removing_user-space_components/managing-software-repositories_using-appstream"}
{"title": "Chapter 12. YUM commands list", "content": "Chapter 12. YUM commands list In the following sections, examine YUM commands for listing, installing, and removing content in Red Hat Enterprise Linux 8. 12.1. Commands for listing content in RHEL 8 The following are the commonly used YUM commands for finding content and its details in Red Hat Enterprise Linux 8: Command Description yum search term Search for a package by using term related to the package. yum repoquery package Search for enabled YUM repositories for a selected package and its version. yum list List information about all installed and available packages. yum list --installed yum repoquery --installed List all packages installed on your system. yum list --available yum repoquery List all packages in all enabled repositories that are available to install. yum repolist List all enabled repositories on your system. yum repolist --disabled List all disabled repositories on your system. yum repolist --all List both enabled and disabled repositories. yum repoinfo List additional information about the repositories. yum info package_name yum repoquery --info package_name Display details of an available package. yum repoquery --info --installed package_name Display details of a package installed on your system. yum module list List modules and their current status. yum module info module_name Display details of a module. yum module list module_name Display the current status of a module. yum module info --profile module_name Display packages associated with available profiles of a selected module. yum module info --profile module_name:stream Display packages associated with available profiles of a module by using a specified stream. yum module provides package Determine which modules, streams, and profiles provide a package. Note that if the package is available outside any modules, the output of this command is empty. yum group summary View the number of installed and available groups. yum group list List all installed and available groups. yum group info group_name List mandatory and optional packages included in a particular group. 12.2. Commands for installing content in RHEL 8 The following are the commonly used YUM commands for installing content in Red Hat Enterprise Linux 8: Command Description yum install package_name Install a package. If the package is provided by a module stream, yum resolves the required module stream and enables it automatically while installing this package. This also happens recursively for all package dependencies. If more module streams satisfy the requirement, the default ones are used. yum install package_name_1 package_name_2 Install multiple packages and their dependencies simultaneously. yum install package_name.arch Specify the architecture of the package by appending it to the package name when installing packages on a multilib system (AMD64, Intel 64 machine). yum install /usr/sbin/binary_file Install a binary by using the path to the binary as an argument. yum install /path/ Install a previously downloaded package from a local directory. yum install package_url Install a remote package by using a package URL. yum module enable module_name:stream Enable a module by using a specific stream. Note that running this command does not install any RPM packages. yum module install module_name:stream yum install @ module_name:stream Install a default profile from a specific module stream. Note that running this command also enables the specified stream. yum module install module_name:stream/profile yum install @ module_name:stream/profile Install a selected profile by using a specific stream. yum group install group_name Install a package group by a group name. yum group install group_ID Install a package group by the groupID. yum install-n <package_name> Install a package by using its exact name. yum install-na <package_name>.<architecture> Install a package by using its exact name and architecture. yum install-nevra <package_name>-<epoch>:<version>-<release>.<architecture> Install a package by using its exact name, epoch, version, release, and architecture. 12.3. Commands for removing content in RHEL 8 The following are the commonly used YUM commands for removing content in Red Hat Enterprise Linux 8: Command Description yum remove package_name Remove a particular package and all dependent packages. yum remove package_name_1 package_name_2 Remove multiple packages and their unused dependencies simultaneously. yum group remove group_name Remove a package group by the group name. yum group remove group_ID Remove a package group by the groupID. yum module remove --all module_name:stream Remove all packages from the specified stream. Note that running this command can remove critical packages from your system. yum module remove module_name:stream/profile Remove packages from an installed profile. yum module remove module_name:stream Remove packages from all installed profiles within the specified stream. yum module reset module_name Reset a module to the initial state. Note that running this command does not remove packages from the specified module. yum module disable module_name Disable a module and all its streams. Note that running this command does not remove packages from the specified module. yum remove-n <package_name> Remove a package by using its exact name. yum remove-na <package_name>.<architecture> Remove a package by using its exact name and architecture. yum remove-nevra <package_name>-<epoch>:<version>-<release>.<architecture> Remove a package by using its exact name, epoch, version, release, and architecture. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/installing_managing_and_removing_user-space_components/yum-commands-list_using-appstream"}
{"title": "Chapter 10. Disabling Windows container workloads", "content": "Chapter 10. Disabling Windows container workloads You can disable the capability to run Windows container workloads by uninstalling the Windows Machine Config Operator (WMCO) and deleting the namespace that was added by default when you installed the WMCO. 10.1. Uninstalling the Windows Machine Config Operator You can uninstall the Windows Machine Config Operator (WMCO) from your cluster. Prerequisites Delete the Windows Machine objects hosting your Windows workloads. Procedure From the Operators OperatorHub page, use the Filter by keyword box to search for Red Hat Windows Machine Config Operator . Click the Red Hat Windows Machine Config Operator tile. The Operator tile indicates it is installed. In the Windows Machine Config Operator descriptor page, click Uninstall . 10.2. Deleting the Windows Machine Config Operator namespace You can delete the namespace that was generated for the Windows Machine Config Operator (WMCO) by default. Prerequisites The WMCO is removed from your cluster. Procedure Remove all Windows workloads that were created in the openshift-windows-machine-config-operator namespace: $ oc delete --all pods --namespace=openshift-windows-machine-config-operator Verify that all pods in the openshift-windows-machine-config-operator namespace are deleted or are reporting a terminating state: $ oc get pods --namespace openshift-windows-machine-config-operator Delete the openshift-windows-machine-config-operator namespace: $ oc delete namespace openshift-windows-machine-config-operator Additional resources Deleting Operators from a cluster Removing Windows nodes Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/windows_container_support_for_openshift/disabling-windows-container-workloads"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/integrating_oracle_cloud_data_into_cost_management/legal-notice"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/storage/legal-notice"}
{"title": "Chapter 9. Removing Windows nodes", "content": "Chapter 9. Removing Windows nodes You can remove a Windows node by deleting its host Windows machine. 9.1. Deleting a specific machine You can delete a specific machine. Note You cannot delete a control plane machine. Prerequisites Install an OpenShift Container Platform cluster. Install the OpenShift CLI ( oc ). Log in to oc as a user with cluster-admin permission. Procedure View the machines that are in the cluster by running the following command: $ oc get machine -n openshift-machine-api The command output contains a list of machines in the <clusterid>-<role>-<cloud_region> format. Identify the machine that you want to delete. Delete the machine by running the following command: $ oc delete machine <machine> -n openshift-machine-api Important By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine. You can skip draining the node by annotating machine.openshift.io/exclude-node-draining in a specific machine. If the machine that you delete belongs to a machine set, a new machine is immediately created to satisfy the specified number of replicas. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/windows_container_support_for_openshift/removing-windows-nodes"}
{"title": "Providing feedback on Red Hat documentation", "content": "Providing feedback on Red Hat documentation We appreciate and prioritize your feedback regarding our documentation. Provide as much detail as possible, so that your request can be quickly addressed. Prerequisites You are logged in to the Red Hat Customer Portal. Procedure To provide feedback, perform the following steps: Click the following link: Create Issue . Describe the issue or enhancement in the Summary text box. Provide details about the issue or requested enhancement in the Description text box. Type your name in the Reporter text box. Click the Create button. This action creates a documentation ticket and routes it to the appropriate documentation team. Thank you for taking the time to provide feedback. Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/cost_management_service/1-latest/html/integrating_oracle_cloud_data_into_cost_management/proc-providing-feedback-on-redhat-documentation"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/package_manifest/legal-notice"}
{"title": "Legal Notice", "content": null, "commands": null, "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/html/operators/legal-notice"}
{"title": "Chapter 6. The Resilient Storage add-on", "content": "Chapter 6. The Resilient Storage add-on The Resilient Storage add-on enables a shared storage or clustered file system to access the same storage device over a network through a pool of data that is available to each server in the group. The add-on is available with a separate subscription. For details, see the Support Policies for RHEL Resilient Storage - Subscriptions, Support Services, and Software Access . The following table lists all the packages available with the Resilient Storage add-on along with their license. Package License awscli ASL 2.0 and MIT booth GPLv2+ booth-arbitrator GPLv2+ booth-core GPLv2+ booth-site GPLv2+ booth-test GPLv2+ clufter-bin GPLv2+ clufter-cli GPLv2+ clufter-common GPLv2+ clufter-lib-ccs GPLv2+ clufter-lib-general GPLv2+ clufter-lib-pcs GPLv2+ cmirror GPLv2 corosync BSD corosync-qdevice BSD corosync-qnetd BSD corosynclib-devel BSD dlm GPLv2 and GPLv2+ and LGPLv2+ fence-agents-aliyun GPLv2+ and LGPLv2+ and ASL 2.0 and BSD and MIT fence-agents-aws GPLv2+ and LGPLv2+ fence-agents-azure-arm GPLv2+ and LGPLv2+ fence-agents-gce GPLv2+ and LGPLv2+ and MIT fence-agents-openstack GPLv2+ and LGPLv2+ and ASL 2.0 and MIT and Python libknet1 LGPLv2+ libknet1-compress-bzip2-plugin LGPLv2+ libknet1-compress-lz4-plugin LGPLv2+ libknet1-compress-lzma-plugin LGPLv2+ libknet1-compress-lzo2-plugin LGPLv2+ libknet1-compress-plugins-all LGPLv2+ libknet1-compress-zlib-plugin LGPLv2+ libknet1-crypto-nss-plugin LGPLv2+ libknet1-crypto-openssl-plugin LGPLv2+ libknet1-crypto-plugins-all LGPLv2+ libknet1-plugins-all LGPLv2+ libnozzle1 LGPLv2+ pacemaker GPL-2.0-or-later AND LGPL-2.1-or-later pacemaker-cli GPL-2.0-or-later AND LGPL-2.1-or-later pacemaker-cts GPL-2.0-or-later AND LGPL-2.1-or-later pacemaker-doc CC-BY-SA-4.0 pacemaker-libs-devel GPL-2.0-or-later AND LGPL-2.1-or-later pacemaker-nagios-plugins-metadata GPLv3 pacemaker-remote GPL-2.0-or-later AND LGPL-2.1-or-later pcs GPL-2.0-only AND Apache-2.0 AND MIT AND BSD-3-Clause AND (Apache-2.0 OR BSD-3-Clause) AND (BSD-2-Clause OR Ruby) AND (BSD-2-Clause OR GPL-2.0-or-later) AND (GPL-2.0-only or Ruby) pcs-snmp GPL-2.0-only and BSD-2-Clause python3-azure-sdk MIT and ASL 2.0 and MPLv2.0 and BSD and Python python3-boto3 ASL 2.0 python3-botocore ASL 2.0 python3-clufter GPLv2+ and GFDL python3-fasteners ASL 2.0 python3-gflags BSD python3-google-api-client ASL 2.0 python3-httplib2 MIT python3-oauth2client ASL 2.0 python3-pacemaker LGPL-2.1-or-later python3-s3transfer ASL 2.0 python3-uritemplate BSD resource-agents GPLv2+ and LGPLv2+ resource-agents-aliyun GPLv2+ and LGPLv2+ and ASL 2.0 and BSD and MIT resource-agents-gcp GPLv2+ and LGPLv2+ and BSD and ASL 2.0 and MIT and Python resource-agents-paf PostgreSQL spausedd BSD Previous Next", "commands": null, "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/package_manifest/resilient-storage-addon"}
